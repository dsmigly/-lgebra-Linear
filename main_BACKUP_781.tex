\PassOptionsToPackage{dvipsnames}{xcolor}

\documentclass[11pt,twoside,a4paper]{book}

\usepackage{estilos} %Relacionado ao arquivo estilos.sty com os packages usados
\makeindex
\title{Álgebra Linear \\  Douglas Smigly}
\author{MAT5730}
\date{2º semestre de 2019}

\begin{document}

\maketitle

\tableofcontents

\newpage

\chapter{Espaços vetoriais}

Durante este capítulo, sempre adotaremos $K$ como sendo um corpo qualquer.

\section{Base e Dimensão}

\begin{definicao}\index{Espaço Vetorial!Base}
Seja $V \neq 0$ um espaço vetorial sobre um corpo $K$. Um subconjunto $B\subseteq V$ chama-se uma \textbf{base} de $V$ se:
\begin{itemize}
\item $B$ é linearmente independente.
\item $B$ gera $V$.
\end{itemize}
\end{definicao}
Lembramos aqui que $B$ é linearmente independente se todo subconjunto finito de $B$ é linearmente independente, ou seja
\[
\sum\limits_{\substack{J \subseteq I \\ \abs{J} < \aleph_0}} \alpha_j v_j = 0, \ \alpha_j \in K, v_j \in B \Rightarrow \alpha_j = 0 \ \forall j \in J 
\]
\begin{teorema}
Seja $V$ um espaço vetorial e sejam $I\subseteq V$ linearmente independente e $S\subseteq V$ gerador de $V$ tais que $I\subseteq S$. Então existe uma base $B$ de $V$ tal que \[I\subseteq B\subseteq S.\]
\end{teorema}
\begin{proof}
Consideremos o conjunto:
\[
\mathcal{M}=\{M\subseteq S\mid M\text{ é linearmente independente e }I\subseteq M\}
\]
Então $\langle \mathcal{M},\subseteq \rangle$ é um conjunto parcialmente ordenado indutivo (ou seja, todo subconjunto totalmente ordenado possui uma cota superior). De fato, $I\in\mathcal{M}$, o que nos mostra que $\mathcal{M} \neq \emptyset,$ e para subconjunto totalmente ordenado não vazio $\mathcal{C}\subseteq\mathcal{M}$ então $\bigcup\limits_{M \in \mathcal{C}} M \in\mathcal{M}.$ 

Logo, pelo Lema de Zorn, $\mathcal{M}$ possui um elemento maximal $B$. Vamos provar que esse elemento maximal é de fato uma base para $V.$
\begin{itemize}
    \item \textbf{$B$ é linearmente independente:} segue da definição de $\mathcal{M}.$
    \item \textbf{$B$ gera $V$:} Suponha por absurdo que $B$ não gera $V$. Então existe $v\in S$ que não é combinação linear de elementos de $B$, aí $B\cup\{v\}$ é linearmente independente e $I\subseteq B\cup\{v\}\subseteq S$. Então $B\cup\mathcal{v}\in\mathcal{M}$, uma contradição, pois $B$ já é um elemento maximal de $\mathcal{M}$ e obviamente $B \subseteq B \cup \{ v \}$. Logo $B$ gera $V$. Portanto, $B$ é uma base de $V$ e $I\subseteq B\subseteq S$.
    \end{itemize}
\end{proof}

O resultado acima mostra que todo espaço vetorial tem base, bastando para isso tomar $I = \{ v \}$ e $S = V.$
\begin{corolario}
Temos o seguinte:
\begin{itemize}
\item Todo espaço vetorial $V$ tem uma base.
\item Para todo $I\subseteq V$ linearmente independente, existe uma base $B$ de $V$ que contém $I$.
\item Para todo $S\subseteq V$ gerador de $V$, existe uma base $B$ de $V$ tal que $B\subseteq S$.
\end{itemize}
\end{corolario}


\begin{lema}
Sejam $\{v_i\}_{i \in \mathbb{N}_{\le n}}$ linearmente independente e $\{u_j\}_{j\in \mathbb{N}_{\le m}}$ um conjunto gerador de $V$. Então $n\leq m$.
\end{lema}

\begin{sublema}
Um conjunto $\{v_i\}_{i \in \mathbb{N}_{\le n}}$ é linearmente dependente se e somente se existem $i\in  \mathbb{N}_{\le n}$ e um $\alpha: i\rightarrow K$ tais que \[v_i=\sum\limits_{j <i}\alpha_jv_j\]
\begin{proof}
Se $\{v_i\}_{i\in n}$ é linearmente dependente, então existe $\alpha:n\rightarrow K$ tal que $\exists i\in n:\alpha_n$ e $\sum_{i\in n}\alpha_iv_i=0$. Seja $i$ o maior elemento de $n$ tal que $\alpha_i\neq 0$. Então 
\[
\alpha_1 v_1 + \ldots + \alpha_i v_i = 0 \Rightarrow \alpha_1v_1 + \ldots + \alpha_{i-1} v_{i-1} = - \alpha_i v_i \Rightarrow
\]
\[v_i= - \sum\limits_{j\in i}\frac{\alpha_j}{\alpha_i}v_j\]
\end{proof}
\end{sublema}

Vamos relembrar o que fizemos até aqui com um exemplo:
\begin{exemplo}
Considere $V = \mathbb{R}^4$ um $\mathbb{R}$-espaço vetorial. Sejam os vetores:
\[
\begin{array}{l}
v_1 = (1,0,0,0) \\
v_2 = (0,1,0,-1) \\
v_3 = (0,0,1,-1) \\
v_4 = (1,-1,0,0) \\
v_5 = (1,2,1,0) 
\end{array}
\]
Considere $I = \{ v_1, v_2 \}$ e $S  =\{ v_1,v_2,v_3,v_4,v_5 \}.$ Observe que $I$ é LI; de fato,
\[
\alpha_1v_1 + \alpha_2v_2 = 0 \Rightarrow \alpha_1(1,0,0,0) + \alpha_2 (0,1,0,-1) = 0 \Rightarrow \left\{ \begin{array}{l} \alpha_1 = 0 \\ \alpha_2 = 0 \\ - \alpha_2 = 0 \end{array} \right. \Rightarrow \alpha_1 = \alpha_2 = 0
\]

Ademais, tomando $v = (x,y,z,w) \in \mathbb{R}^4,$ temos que
\[
(x-z+w+y)v_1 + (z- w - \varepsilon)v_2 + (z - \varepsilon)v_3 + (z-w-y + \varepsilon)v_4 + \varepsilon_5 = v,
\]
para todo $\varepsilon \in \mathbb{R}.$ Logo, $S$ gera $V.$ 

Então, existe uma base $B$ de $\mathbb{R}^4$ tal que 
\[
\{ v_1, v_2 \} \subseteq B \subseteq \{v_1,v_2,v_3,v_4,v_5 \}
\]
De fato, esta base é $B \{v_1, v_2, v_3, v_4 \},$ pois percebe-se que
\[
v_5 = \frac{5}{2}v_1 + \frac{1}{2} v_2 - \frac{1}{2}v_3 - \frac{3}{2} v_4
\]
\end{exemplo}




Para trabalhar com a cardinalidade das bases, utilizaremos alguns fatos conhecidos, enunciados na
\begin{proposicao}
Se $\lambda$ e $\mu$ são cardinais, então:
\begin{itemize}
\item Se $\lambda\leq\mu$ e $\mu\leq\lambda$, então $\lambda=\mu$. (Teorema de Cantor-Bernstein)\index{Teorema de Cantor-Bernstein}
\item Se $\lambda$ e $\mu$ são infinitos, então \[\lambda+\mu=\lambda\mu=\max\{\lambda,\mu\}.\]
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço vetorial, então duas bases quaisquer têm o mesmo cardinal.
\end{teorema}
\begin{proof}
Sejam $B$ e $C$ bases de $V$. Para $u\in C$ existem um conjunto finito $I_u\subseteq B$ e uma função $\alpha_u:I_u\rightarrow K$ tais que $u=\sum_{i\in I_u}\alpha_{u,i}i$. Seja $I\subseteq\bigcup_{u\in C}\subseteq B$. Então $I$ gera $V$, assim $I=C$. Desse modo:
\[
\abs{B}=\abs{I}=\abs{\bigcup_{u\in C}I_u}\leq\sum_{u\in C}\abs{I_u}\leq\aleph_0\cdot\abs{C}=\abs{C},
\]
assim $\abs{B}\leq\abs{C}$. Analogamente $\abs{C}\leq\abs{B}$. Portanto $\abs{B}=\abs{C}$.
\end{proof}
\begin{definicao}\index{Espaço Vetorial!Dimensão}
Dizemos que a \textbf{dimensão} de um espaço vetorial é a cardinalidade de sua base.
\end{definicao}
\section{Subespaços}

\begin{proposicao}
Seja $V$ um espaço vetorial e seja $\mathcal{W}$ um conjunto de subespaços. Então $\bigcap\mathcal{M}$ é um subespaço de $V$.
\end{proposicao}

\begin{definicao}
Se $S$ é subconjunto de $V$, definimos:
\[
\langle S\rangle=\left\{\sum\limits_{v\in I}\alpha_vv\mid I\subseteq S\text{ e }I\text{ é finito e }\alpha\in K^I\right\}
\]
e chamamos de \textbf{subespaço gerado} por $S$.
\end{definicao}

\begin{proposicao}
Se $S$ é subconjunto de $V$, então:
\[
\langle S\rangle=\{W\mid W\text{ é subespaço de }V\text{ e }W\subseteq S\}.
\]
\end{proposicao}
A intersecção de subsespaços sempre é um subespaço, mas o mesmo não acontece com a união de subespaços.
\begin{proposicao}
Se $A$ e $B$ são subespaços de $V$ tais que $A\nsubseteq B$ e $B\nsubseteq A$, então $A\cup B$ não é subespaço de $V$.
\end{proposicao}
\begin{proof}
Nesse caso, existe $a\in A$ tal que $a\notin B$ e existe $b\in B$ tal que $b\notin A$. Seja $c=a+b$. Então:
\begin{itemize}
    \item Se $c \in A,$ $b = c - a \in A,$ o que é impossível.
    \item Se $c \in B,$ $a = c - b \in b,$ o que é impossível.
\end{itemize}
Logo, concluímos que $c \notin A \cup B,$ absurdo.

%Então $c-a=b\notin A$ e $a\in A$, de modo que $c\notin A$, e $c-b=a\notin B$ e $b\in B$, de modo que $c\notin B$, assim $c\notin A\cup B$.
\end{proof}

Na verdade, $A \cup B$ é um subespaço se e somente se $A \subseteq B$ ou $B \subseteq A.$

\begin{observacao}
Seja $K = F_2 = \{ 0, 1 \},$ e tome $V = K^2.$ Então,
\[
V = \langle (0,1) \rangle \cup \langle (1,0) \rangle \cup \langle (1,1) \rangle
\]
Na verdade, $V$ só pode ser escrito como união de seus subespaços se $K$ for um corpo finito.
\end{observacao}

Apesar de não podermos trabalhar com a união, podemos realizar a soma de subespaços, e esta sim é um subespaço:

\begin{definicao}
Sejam $W_i \subseteq V$, $i \in I,$ subespaços de $V.$ Definimos:
\[
\sum\limits_{i \in I} W_i = \{ w_{i_1} + \ldots + w_{i_k} | k \in \mathbb{N}, w_i \in W_i \}
\]
\end{definicao}
Pode-se mostrar que $\sum\limits_{i \in I} W_i $ é subespaço de $V$.
\begin{definicao}\index{Espaço Vetorial!Soma direta}
Uma soma $\sum\limits_{i \in I} W_i$ chama-se \textbf{soma direta} se para todo $i \in I$
\[
W_i \cap \left( \sum\limits_{j \neq i} W_j \right) = 0
\]
\end{definicao}
\begin{teorema}
Para subespaço $A$ de $V$, então existe subespaço $B\subseteq V$ tal que $V=A\oplus B$.
\end{teorema}

\begin{teorema}
\[
\dim(A+B)+\dim(A\cap B)=\dim(A)+\dim(B).
\]
\end{teorema}
\begin{proof}
Seja $E$ base de $A\cap B$. Então existe $F$ tal que $B\cap F=\emptyset$ e $E\cup F$ seja base de $A$ e existe $G$ tal que $A\cap G=\emptyset$ e $E\cup G$ seja base de $B$. Então $E\cup F\cup G$ é base de $A+B$. Daí:
\[
\textcolor{Green}{\dim(A+B)} + \textcolor{Blue}{\dim(A \cap B)} = \textcolor{Green}{\abs{E} + \abs{F} + \abs{G}} + \textcolor{Blue}{\abs{E}} = \textcolor{Red}{\abs{E} + \abs{F}} + \textcolor{Laranja}{\abs{E} + \abs{G}} = \textcolor{Red}{\dim(A)} + \textcolor{Laranja}{\dim(B)}
\]
\end{proof}

\begin{exemplo}
Considere novamente $V = \mathbb{R}^4$. Sejam
\[
W_1 = \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \}
\]
\[
W_2 = \{(x,y,z,t) \in \mathbb{R}^4 | x +y = 0 \mbox{ e } z - 2t = 0 \}
\]

$W_1$ e $W_2$ são subespaços de $V.$ Assim, $W_1 + W_2$ e $W_1 \cap W_2$ são subespaços de $V.$ Vamos encontrar bases para eles.
Note que 
\[
\begin{array}{lcl}
W_1 &=& \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \} \\
&=& \{(x,y,z,-y-z) \in \mathbb{R}^4 | x,y,z \in \mathbb{R} \} \\
&=& \{ (x,0,0,0) + (0,y,0-y) + (0,0,z,-z) : x,y,z \in \mathbb{R} \} \\
&=& \langle (1,0,0,0), (0,1,0-1), (0,0,1,-1) \rangle\\
\end{array}
\]
Verifica-se também que $(1,0,0,0), (0,1,0-1), (0,0,1,-1)$ são linearmente independentes. Logo, $B_1 = \{ (1,0,0,0), (0,1,0-1), (0,0,1,-1) \}$ é base para $W_1.$
Analogamente, mostra-se que $B_2 = \{ (1,-1,0,0), (0,0,2,1) \}$ é base para $W_2.$
Agora, para determinar uma base de $W_1 + W_2,$ podemos escalonar a matriz
\[
\left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & -1 \\
1 & -1 & 0 & 0\\
0 & 0 & 2 & 1

\end{array} \right) \rightarrow \cdots \rightarrow \left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0& 0& 0 & 1\\
0 & 0 & 0 & 0

\end{array} \right)
\]
Portanto, o conjunto
\[
\mathcal{B} = \{ (1,0,0,0), (0,1,0,-1),(0,0,1,-1),(1,-1,0,0) \}\]
é base de $W_1 + W_2.$

Para determinar uma base de $W_1 \cap W_2,$ basta resolver o sistema
\[
\left\{ \begin{array}{l}
y+z+t = 0 \\x+y = 0 \\z - 2t = 0
\end{array} \right.
\]
Assim, $W_1 \cap W_2 = \langle (3,-3,2,1) \rangle.$

Observe que
\[
\dim(W_1 \cap W_2) + \dim(W_1 + W_2) = 1 + 4 = 5 = 3 + 2 = \dim(W_1) + \dim(W_2)
\]

Como $\dim(W_1 + W_2) = 4,$ temos que $W_1 + W_2 = V = \mathbb{R}^4.$
Observe também que, como $\dim(W_1 \cap W_2) = 1,$ a soma $W_1 + W_2$ não é direta. 
\end{exemplo}
\section{Coordenadas}

\begin{definicao}
Seja $V$ um espaço vetorial de dimensão finita. Seja $B$ uma base de $V$. Então para $v\in V$ existe um único $\alpha:B\rightarrow K$ tal que \[v=\sum\limits_{b\in B}\alpha_bb,\] e chamamos esse $\alpha$ de $[v]_B$.
\end{definicao}

\chapter{Transformações Lineares}
\index{Espaço Vetorial!Transformações Lineares}

\section{Definições}

\begin{definicao}
Uma função $T:U\rightarrow V$ se chama uma \textbf{transformação linear} se para quaisquer $\alpha,\beta\in K$ e $u,v\in V$ tivermos $T(\alpha u+\beta v)=\alpha T(u)+\beta T(v)$.
\end{definicao}

\begin{definicao}
Para espaços vetoriais $U$ e $V$, denotamos o conjunto das transformações lineares de $U$ a $V$ por $\mathcal{L}(U,V).$
\end{definicao}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, seja $B$ uma base de $U$ e $f:B\rightarrow V$ uma função. Então existe uma única transformação linear $T\in\mathcal{L}(U,V)$ tal que $\forall b\in B:T(b)=f(b)$.
\end{teorema}

\begin{definicao}
Seja $T\in\mathcal{L}(U,V)$. Definimos $\mathrm{Ker}(T)=\{u\in U:T(u)=0\}$. Definimos $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$.
\end{definicao}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$. Então:
\begin{itemize}
\item $\mathrm{Ker}(T)$ é um subespaço de $U$.
\item $\mathrm{Im}(T)$ é um subespaço de $V$.
\item $T$ é injetora se e só se $\mathrm{Ker}(T)=0$.
\item Se $T$ é bijetora, então $T^{-1}\in\mathcal{L}(V,U)$.
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $\mathcal{L}(U,V)$, seja $B$ uma base de $\mathrm{Ker}(T)$, e seja $C$ um conjunto tal que $T[C]$ seja base de $\mathrm{Im}(T)$. Então $B\cup C$ é base $V$.
\end{teorema}
\begin{proof}
Para $v\in V$ então $T(v)\in\mathrm{Im}(T)$, então existem um conjunto finito $F\subseteq C$ e $\alpha:F\rightarrow K$ tais que $T(v)=\sum\limits_{w\in F}\alpha_wT(w)$, assim $T\left(v-\sum\limits_{w\in F}\alpha_ww\right)=0$, aí $v-\sum\limits_{w\in F}\alpha_ww\in\mathrm{Ker}(T)$, assim existem conjunto finito $E\subseteq B$ e função $\beta:B\rightarrow K$ tal que $v-\sum\limits_{w\in F}\alpha_ww=\sum\limits_{u\in E}\beta_uu$, aí $v=\sum\limits_{u\in E}\beta_uu+\sum\limits_{w\in F}\alpha_ww$.

\medskip
\noindent
Por outro lado, para subconjunto finito $E\subseteq B\cup C$ e função $\alpha:E\rightarrow K$ tal que $\sum_{e\in E}\alpha_ee=0$, então $\sum_{e\in E\cap C}\alpha_eT(e)=0$, aí $\forall E\cap C:\alpha_e=0$, aí $bla$. 
\end{proof}

\begin{teorema}[Teorema do Núcleo-Imagem] \index{Espaço Vetorial!Teorema do Núcleo-Imagem}
Seja $T \in \mathcal{L}(U,V).$ Então
\[
U = \mathrm{Ker}(T) \oplus \mathrm{Im}(T)
\]

\end{teorema}
\begin{corolario}
\[
\dim V=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T)).
\]
\end{corolario}

\begin{definicao}\index{Transformações Lineares!Isomorfismos}
Se $T\in\mathcal{L}(U,V)$ é bijetora, dizemos que $T$ é um \textbf{isomorfismo} de $U$ a $V$.
\end{definicao}
\begin{proposicao}
$T \in \mathcal{L}(U,V)$ é isomorfismo se e somente se $T^{-1}$ também o é.
\end{proposicao}

\begin{proposicao}
dois espaços vetoriais $U$ e $V$ são isomorfos se e somente se quaisquer duas bases $\mathcal{B}$ de $U$ e $\mathcal{C}$ de $V$ possuem a mesma cardinalidade.
\end{proposicao}
\begin{teorema}
Para espaços vetoriais $U$ e $V$, então $U$ é isomorfo a $V$ se e só se $\dim(U)=\dim(V)$.
\end{teorema}

\section{Espaço Dual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. Denotamos $V^*=\mathcal{L}(V,K)$. O espaço $V^*$ chama-se o \textbf{espaço dual} de $V$. Os elementos de $V$ chama-se \textbf{funcionais lineares}.
\end{definicao}

\medskip
\noindent
Se $\dim(V)=n$, então $\dim(V^*)=n\cdot 1=n$. Assim, $V$ e $V^*$ são isomorfos (no caso de $\dim(V)=n<\aleph_0$).

\begin{teorema}
Seja $V$ um espaço vetorial com $\dim(V)=n$ e $B=\{v_i\}_{i\in n}$ uma base de $V$. Então existe uma base $B^*=\{f_i\}_{i\in n}$ de $V^*$ tal que $f_i(v_j)=\delta_{i,j}$ para $i,j\in n$. Além disso, $\forall v\in V:v=\sum_{i\in n}f_i(v)v_i$ e $\forall f\in V^*:f=\sum_{i\in n}f(v_i)f_i$.
\end{teorema}
\begin{proof}
Para todo $i\in n$, existe uma única função linear $f_i:V\rightarrow K$ tal que:
\[
f_i(v_j)=\left\{\begin{array}{rl}0,&i\neq j\\1,&i=j\end{array}\right.
\]
Seja $\alpha:n\rightarrow K$ tal que:
\[
\sum_{i\in n}\alpha_if_i=0.
\]
Para $j\in n$, aplicando este funcional para o vetor $v_j\in B$, então:
\[
0=0(v_j)=\sum_{i\in n}\alpha_if_i(v_j)=\alpha_j,
\]
ou seja, $\alpha_j=0$. Portanto $B^*$ é linearmente independente.

\medskip
\noindent
Além disso, para $v\in V$ existe $\alpha:n\rightarrow K$ tal que $v=\sum_{i\in n}\alpha_iv_i$, aí para $i\in n$ temos $f_i(v)=\alpha_if_i(v_i)=\alpha_i$; logo $f(v)=\sum_{i\in n}\alpha_if(v_i)=\sum_{i\in n}f(v_i)f_i(v)$. 
\end{proof}

\begin{definicao}\index{Espaço Vetorial!Base dual}
A base $B^*$ chama-se a \textbf{base dual} da base $B$.
\end{definicao}

\section{Espaço Bidual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. O espaço $V^{**}=(V^*)^*$ chama-se o \textbf{espaço bidual} do espaço $V$.
\end{definicao}

\begin{definicao}
Para $v\in V$, definamos $\varphi_v:V^*\rightarrow K$ assim:
\[
\forall f\in V^*:\varphi_v(f)=f(v).
\]
Então $\varphi_v\in V^{**}$.
\end{definicao}

\begin{proposicao}
$\varphi\in\mathcal{L}(V,V^{**})$ e $\varphi$ é injetora.
\end{proposicao}
\begin{proof}
Para $v\in\mathrm{Ker}(\varphi)$, então $\varphi_v=0$, aí para todo $f\in V^*$ temos $f(v)=\varphi_v(f)=0$, aí para todo $i\in n$ temos $f_i(v)=0$, aí $v=\sum_{i\in n}f_i(v)v_i=0$, aí $v=0$.
\end{proof}

\noindent
Seja $B$ uma base de $V$, então para cada $a\in B$ definimos a transformação linear $f_a\in V^*$ por $f_a(b)=\delta_{a,b}$, então $(f_a)_{a\in B}$ é linearmente independente em $V^*$ e para todo $v\in V$ existem um conjunto finito $F\subseteq B$ tal que $v=\sum_{b\in F}f_b(v)b$.

\begin{corolario}
Se $\dim(V)=n<\aleph_0$ então $\varphi:V\rightarrow V^{**}$ é um isomorfismo.
\end{corolario}
\begin{proof}
\[
\dim(V)=\dim(V^*)=\dim(V^{**}).
\]
\end{proof}

\begin{observacao}
Nesse caso $\varphi$ é um isomorfismo natural, ou seja, não depende da escolha de uma base.
\end{observacao}

\begin{corolario}
Se $\dim(V)<\aleph_0$, então toda base de $V^*$ é a base dual para uma base de $V$.
\end{corolario}
\begin{proof}
Seja $C$ uma base de $V^*$. Consideremos a base dual $C^*$ de $V^{**}$. Mas $V^{**}\cong V$, então existe $v:C\rightarrow V$ tal que $\forall c\in C:f_c=\varphi_{v_c}$, assim:
\[
c(v_d)=\varphi_{v_d}(c)=f_d(v_c)=\delta_{d,c}=\delta_{c,d},
\]
logo $C$ é base dual da base $(v_c)_{c\in C}$ de $V$.
\end{proof}

\section{Anuladores}

\begin{definicao}
Seja $V$ um espaço vetorial e seja $S\subseteq V$ um subconjunto. Então definimos:
\[
S^0=\{f\in V^*\mid\forall s\in S:f(s)=0\}.
\]
O conjunto $S^0$ chama-se o \textbf{anulador} de $S$.
\end{definicao}

\begin{proposicao}
$S^0$ é um subespaço de $V$.
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço com $\dim(V)<\aleph_0$ e $W\subseteq V$ um subespaço. Então:
\[
\dim(V)=\dim(W)+\dim(V^0).
\]
\end{teorema}
\begin{proof}
Seja $\dim(V)=n$ e $\dim(W)=m$. Escolhemos uma base $(v_i)_{i\in m}$ de $W$ e completemo-la até uma base $(v_i)_{i\in n}$ de $V$. Consideremos a base dual $(f_{v_i})_{i\in n}$ de $V^*$. Mostraremos que $(f_{v_i})_{i\in n\setminus m}$ é uma base de $W^0$. É claro que $\forall i\in n\setminus m:f_{v_i}\in W^0$. Seja $f\in W^0$, então $f=\sum_{i\in n}f(v_i)f_{v_i}=\sum_{i\in n\setminus m}f(v_i)f_{v_i}$.
\end{proof}

\begin{teorema}
Se $\dim(V)<\aleph_0$ e $V=U\oplus W$, então $V^*=U^0\oplus W^0$ e $U^0\cong W^*$ e $W_0\cong U^*$.
\end{teorema}
\begin{proof}
Seja $B=B_U\cup B_W$ uma base de $V$, em que $B_U$ é base de $U$ e $B_W$ é base de $W$. Então na base dual temos $B^*=B_U^*\cup B_V^*$, e pelo teorema anterior temos $\langle B_U^*\rangle=W^0$ e $\langle B_V^*\rangle=U^0$.
\end{proof}

\section{Transpostas}

\begin{definicao}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e $T\in\mathcal{L}(U,V)$. Então definimos a \textbf{transposta} de $T$ como a função:
\[
\begin{array}{rcl}
T^t:V^t&\rightarrow&U^t\\f&\mapsto&T^t(f)=f\circ T
\end{array}
\]
\end{definicao}

\begin{proposicao}
Se $\dim(U)<\aleph_0$ e $T\in\mathcal{L}(U,V)$, então:
\begin{itemize}
\item[a)] $\mathrm{Ker}(T^t)=(\mathrm{Im}(T))^0$.
\item[b)] $\mathrm{Rank}(T^t)=\mathrm{Rank}(T)$.
\item[c)] $\mathrm{Im}(T^t)=(\mathrm{Ker}(T))^0$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[a)] Temos:
\[
\begin{array}{rcl}
\mathrm{Ker}(T^t)&=&\{f\in V^*\mid T^t(f)=0\}\\&=&\{f\in V^*\mid f\circ T=0\}\\&=&\{f\in V^*\mid \forall u\in U:f(T(u))=0\}\\&=&\{f\in V^*\mid f[\mathrm{Im}(T)]=0\}\\&=&(\mathrm{Im}(T))^0.
\end{array}
\]
\item[b)] Temos $\mathrm{Rank}(T^t)=\dim(\mathrm{Im}(T^t))$ e $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$. Além disso:
\[
\dim(V^*)=\dim(\mathrm{Im}(T^t))+\dim(\mathrm{Ker}(T^t))
\]
\[
\dim(V^*)=\dim(\mathrm{Im}(T))+\dim(\mathrm{Im}(T))^0
\]
mas $\dim(V^*)=\dim(V)$ e $\dim(\mathrm{Ker}(T^t))+\dim(\mathrm{Im}(T))^0$.
\item[c)] Temos $\mathrm{Im}(T^t)\subseteq(\mathrm{Ker}(T))^0$. Seja $\varphi\in\mathrm{Im}(T^t)$, então existe $g\in V^*$ tal que $\varphi=T^t(g)$, aí para todo $u\in U$ nós temos $\varphi(u)=T^t(g)(u)=g(T(u))$. Se $u\in\mathrm{Ker}(T)$ então $T(u)=0$, aí $\varphi(u)=0$; logo $\varphi\in(\mathrm{Ker}(T))^0$. Além disso:
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Ker}(T))^0
\]
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T))
\]
aí $\dim(\mathrm{Ker}(T))^0=\dim(\mathrm{Im}(T))$, aí $(\mathrm{Ker}(T))^0=\mathrm{Im}(T)$.
\end{itemize}
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais de dimensão finita com bases $B$ e $C$ e bases duais $B^*$ e $C^*$. Se $T\in\mathcal{L}(U,V)$, então:
\[
([T]_{B,C})^t=[T^t]_{C^*,B^*}
\]
\end{teorema}

\begin{corolario}
Se $A\in M_{m,n}(K)$, então:
\[
\mathrm{Row Rank}(A)=\mathrm{Column Rank}(A).
\]
\end{corolario}
\begin{proof}
Consideremos $T:K^n\rightarrow K^m$ dada por $T(v)=Av$. Sejam $B$ e $C$ as bases canônicas de $K^n$ e $K^m$, então $[T]_{B,C}=A$. Temos:
\[
\begin{array}{ccccc}
\mathrm{Rank}(T)&=&\mathrm{Column Rank}(A)&&\\
\mathrm{Rank}(T^t)&=&\mathrm{Column Rank}(A^t)&=&\mathrm{Row Rank}(A).
\end{array}
\]

\noindent
\end{proof}

\section{Espaços Quocientes}

\begin{definicao}
Seja $V$ um espaço, $W\subseteq V$ um subespaço. Para $u,v\in V$, digamos que $u\sim v$ se e só se $u-v\in W$. Então $\sim$ é uma relação de equivalência, ou seja:
\begin{itemize}
\item Reflexiva, ou seja, $v\sim v$ sempre.
\item Simétrica, ou seja, se $v\sim u$ então $u\sim v$.
\item Transitiva, ou seja, se $v\sim u$ e $u\sim w$, então $v\sim w$.
\end{itemize}
Seja $V/W$ o conjunto das classes de equivalência relativamente a $\sim$. Para $v\in V$ seja $\overline{v}$ a classe de equivalência de $v$.
\begin{itemize}
\item Definamos em $V/W$ uma estrutura de espaço vetorial. Para $\overline{v},\overline{w}\in V/W$ definamos $\overline{v}+\overline{w}=\overline{v+w}$.
\item Para $\alpha\in K$ e $\overline{v}\in V$ definamos $\alpha\cdot\overline{v}=\overline{\alpha v}$. Então $V/W$ é um espaço vetorial chamado \textbf{espaço quociente}.
\end{itemize}
\end{definicao}

\begin{observacao}
As operações estão ``bem definidas'' pois:
\begin{itemize}
\item Se $\overline{v}=\overline{v'}$ e $\overline{u}=\overline{u'}$, então $v\sim v'$ e $u\sim u'$, aí $v-v',u-u'\in W$, aí $(v+u)-(v'+u')=(v-v')+(u-u')\in W$, aí $\overline{v+u}=\overline{v'+u'}$, aí $\overline{v}+\overline{u}=\overline{v'}+\overline{u'}$.
\item Analogamente para a outra propriedade.
\end{itemize}
Também verificaremos algumas propriedades, deixando o resto ao leitor.
\begin{itemize}
\item Temos a comutatividade da adição, pois $\overline{u}+\overline{v}=\overline{v}+\overline{u}$ equivale a $\overline{u+v}=\overline{v+u}$, que é verdade pois $u+v=v+u$.
\item O que é o $\overline{0}$ de $V/W$? Temos $\overline{0}=W$, e também para todo $w\in W$ temos $w\sim 0$, aí $\overline{w}=\overline{0}=W$.
\end{itemize}
Também temos o seguinte:
\begin{itemize}
\item Se $W=V$, então $V/V=\{\overline{0}\}$.
\item Se $W=\{0\}$, então $V/\{0\}\cong V$.
\end{itemize}
\end{observacao}

\begin{proposicao}
Consideremos a aplicação:
\[
\pi:V\rightarrow V/W,\quad\quad v\mapsto\overline{v}.
\]
Então $\pi\in\mathcal{L}(V,V/W)$, com $\mathrm{Ker}(\pi)=W$.
\end{proposicao}

\begin{notacao}
$\pi$ chama-se a \textbf{projeção canônica} de $V$ para $V/W$.
\end{notacao}

\begin{proof}
Temos o seguinte:
\begin{itemize}
\item $\pi(v+u)=\overline{v+u}=\overline{v}+\overline{u}=\pi(v)+\pi(u)$.
\item $\pi(\alpha v)=\overline{\alpha v}=\alpha\overline{v}=\alpha\pi(v)$.
\end{itemize}
Além disso, se $w\in W$ então $\pi(w)=\overline{w}=W$
\end{proof}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$ e $W\subseteq U$ tal que $W\subseteq\mathrm{Ker}(T)$. Então existe um único $\overline{T}\in\mathcal{L}(U/W,V)$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
\end{proposicao}
\begin{proof}
Temos o seguinte:

\medskip
\noindent
1) Mostraremos que $\overline{T}$ está ``bem definida''. Se $\overline{u}=\overline{v}$, então $u-v\in W\in\mathrm{Ker}(T)$, aí $T(u-v)=0$, aí $T(u)=T(v)$.

\medskip
\noindent
2) Mostraremos que $\overline{T}$ é uma transformação linear.

\begin{itemize}
\item $\overline{T}(\overline{u}+\overline{v})=\overline{T}(\overline{u+v})=T(u+v)=T(u)+T(v)=\overline{T}(\overline{u})+\overline{T}(\overline{v})$.
\item 
\end{itemize}
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e seja $T\in\mathcal{L}(U,V)$. Então $U/\mathrm{Ker}(T)\cong\mathrm{Im}(T)$.
\end{teorema}
\begin{proof}
Pela proposição anterior, existe uma única $\overline{T}:U/\mathrm{Ker}(T)\rightarrow V$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
Observemos que $\mathrm{Im}(\overline{T})=\mathrm{Im}(T)=\{T(u)\mid u\in U\}$.

\medskip
\noindent
Além disso, para $\overline{u}\in\mathrm{Ker}(\overline{T})$, então $T(u)=\overline{T}(\overline{u})=0$, aí $u\in\mathrm{Ker}(T)$, aí $\overline{u}=\overline{0}$, de modo que $\overline{T}$ é injetora.
\end{proof}

\begin{teorema}
Seja $W$ subespaço de $V$. Então todos os complementos de $W$ em $V$ são isomorfos ao $V/W$.
\end{teorema}
\begin{proof}
Seja $V=W\oplus U$. Consideremos a projeção canônica:
\[
\pi:V\rightarrow V/W.
\]
Seja $\overline{\pi}=\pi\upharpoonright U$. Então $\mathrm{Ker}(\overline{\pi})=U\cap\mathrm{Ker}(\pi)=U\cap W=\{0\}$. Logo $\overline{\pi}$ é injetora.

\medskip
\noindent
Para $\overline{v}\in V/W$, seja $v=w+u$, com $w\in W$ e $u\in U$. Então $\pi(v)=\pi(w)+\pi(u)=\pi(u)=\overline{\pi}(u)$, aí $\overline{v}=\overline{\pi}(u)$, assim $\overline{\pi}$ é sobre $V/W$.
\end{proof}

\begin{corolario}
Seja $W\subseteq V$ um subespaço. Então $\dim V=\dim W+\dim V/W$.
\end{corolario}
\begin{proof}
Seja $V=W\oplus U$, então $\dim V=\dim W+\dim U$, mas $U\cong V/W$, aí $\dim U=\dim V/W$.
\end{proof}

\begin{observacao}
Existem espaços vetoriais $W$ e $U$ e $W'$ e $U'$ tais que $W\oplus U\cong W'\oplus U'$ e $W\cong W'$, mas $U\ncong U'$. De fato podemos tomar $W=\bigoplus_{i=0}^\infty Ke_{2i}$ e $U=\bigoplus_{i=0}^\infty Ke_{2i+1}$ e $W'=\bigoplus_{i=0}^\infty Ke_i$ e $U'=\{0\}$.
\end{observacao}

\chapter{Determinantes}

\section{Formas Multilineares}

\begin{definicao}
Seja $V$ um espaço vetorial e $V^r=V\times\dots\times V$. Uma \textbf{forma $r$-linear} sobre $V$ é uma função:
\[
F:V^r\rightarrow K,\quad\quad (v_i)_{i\in r}\mapsto F((v_i)_{i\in r})\in K
\]
que é linear em cada argumento, ou seja, para $i\in r$ temos:
\[
F(v_0,\dots,\alpha v_i+\beta v'_i,\dots,v_{r-1})=\alpha F(v_0,\dots,v_i,\dots,v_{r-1})+\beta F(v_0,\dots,v'_i,\dots,v_{r-1}).
\]
Denotamos por $L_r(V)$ o conjunto das formas $r$-lineares sobre $V$.
\end{definicao}

\begin{exemplo}
Seja $V=K^2$ e:
\[
F((x_0,y_0),(x_1,y_1),(x_2,y_2))=x_0y_1x_2-x_0x_1x_2.
\]
Então $F$ é uma forma 3-linear.
\end{exemplo}

\begin{definicao}
Uma forma $F\in L_r(V)$ chama-se \textbf{alternativa} se e só se para $v\in V^r$, se $v$ não é injetora, então $F(v)=0$. Denotamos por $A_r(V)$ o conjunto das formas $r$-lineares alternativas.
\end{definicao}

\begin{definicao}
Uma forma $F$ é chamada \textbf{antissimétrica} se para $v\in V^r$ e para $i,j\in r$ tais que $i\neq j$, então:
\[
F(v_0,\dots,v_i,\dots,v_j,\dots,v_{r-1})=-F(v_0,\dots,v_j,\dots,v_i,\dots,v_{r-1}).
\]
\end{definicao}

\begin{proposicao}
Toda forma alternativa é antissimétrica.
\end{proposicao}
\begin{proof}
Seja $F\in A_r(V)$. Sejam $v\in V^r$ e $i,j\in r$ tais que $i\neq j$. Então:
\[
\begin{array}{rcl}
0&=&F(v_0,\dots,v_i+v_j,\dots,v_i+v_j,\dots,v_{r-1})\\&=&F(v_0,\dots,v_i,\dots,v_i,\dots,v_{r-1})+F(v_0,\dots,v_i,\dots,v_j,\dots,v_{r-1})\\&&+F(v_0,\dots,v_j,\dots,v_i,\dots,v_{r-1})+F(v_0,\dots,v_j,\dots,v_j,\dots,v_{r-1})\\&=&F(v_0,\dots,v_i,\dots,v_j,\dots,v_{r-1})+F(v_0,\dots,v_j,\dots,v_i,\dots,v_{r-1})
\end{array}
\]
\end{proof}

\begin{proposicao}
Se a característica do corpo é $\neq 2$, então toda forma antissimétrica é reflexiva.
\end{proposicao}
\begin{proof}
Para $F$ antissimétrica e $v\in V^r$ e $i,j\in r$ tais que $i\neq j$, se $v_i=v_j$, sendo $v=v_i$, então:
\[
F(v_0,\dots,v,\dots,v,\dots,v_{r-1})=-F(v_0,\dots,v,\dots,v,\dots,v_{r-1}),
\]
aí:
\[
2F(v_0,\dots,v,\dots,v,\dots,v_{r-1})=0,
\]
aí:
\[
F(v_0,\dots,v,\dots,v,\dots,v_{r-1})=0.
\]
\end{proof}

\begin{definicao}
Seja $F\in L_r(V)$ e $\sigma\in S_r$ uma permutação. Então, para $v\in V^r$, definamos $(\sigma F)(v)=F(v\circ\sigma)$. Então é fácil ver que $\sigma F\in L_r(V)$.
\end{definicao}

\begin{observacao}
Para $F\in L_r(V)$, então $F$ é antissimétrica se e somente se para toda transposição $\tau\in S_r$ tivermos $\tau F=-F$.
\end{observacao}

\begin{proposicao}
Seja $F\in L_r(V)$ uma forma antissimétrica. Então para $\sigma\in S_r$, temos $\sigma F=(\sgn\sigma)F$.
\end{proposicao}
\begin{proof}
Para $\sigma\in S_r$, então $\sigma$ pode ser escrita como $\sigma=\tau_0\dots\tau_{k-1}$, em que $\tau_i$ são transposições, e $\sigma$ é par se e só se $k$ é par.

\medskip
\noindent
Temos $\sigma F=(\tau_0\dots\tau_{k-1})F=(-1)^k F=(\sgn\sigma)F$, pois $\sgn\sigma=(-1)^k$.
\end{proof}

\begin{proposicao}
Toda forma $r$-linear determina uma forma $r$-linear alternada da seguinte maneira:
\[
F\mapsto\varphi(F)=\sum_{\sigma\in S_r}\sgn\sigma(\sigma F).
\]
\end{proposicao}
\begin{proof}
Seja $v_i=v_j=v$ com $i\neq j$. Precisamos provar que $\varphi(F)(v)=0$. Seja $\tau$ a transposição $(i,j)$, então $S=A_r\cup A_r\tau$ e $A_r\cap A_r\tau=\emptyset$. Então temos o seguinte:
\[
\begin{array}{rcl}
\varphi(F)(v)&=&\sum_{\sigma\in S_r}(\sgn\sigma)(\sigma F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma\tau F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma F(v))\\&=&0
\end{array}
\]
\end{proof}

\begin{observacao}
Se $F\in A_r(V)$ e $v\in V^r$ é linearmente dependente, então:
\[
F(v)=0.
\]
\end{observacao}

\begin{lema}
Seja $\dim V=n$ e $F\in A_n(V)$. Seja $(e_i)_{i\in n}$ uma base de $V$, então $F$ é completamente determinada pelo valor $F(e)$.
\end{lema}
\begin{proof}
Seja $v\in V^n$. Então existe $\alpha:n\times n\rightarrow K$ tal que:
\[
v_i=\sum_{j\in n}\alpha_{i,j}e_j.
\]
Assim:
\[
\begin{array}{rcl}
F(v)&=&F((\sum_{j\in n}\alpha_{i,j}e_j)_{j\in n})\\&=&\sum_{j\in n^n}\prod_{i\in n}\alpha_{i,j(i)}F(e\circ j)\\&=&\sum_{\sigma\in S_n}\prod_{i\in n}\alpha_{i,\sigma(i)}F(e\circ \sigma)\\&=&\textcolor{red}{\left(\sum\limits_{\sigma\in S_n}\prod_{i\in n}\alpha_{i,\sigma(i)} \sgn\sigma \right)} F(e).
\end{array}
\]
\end{proof}

\noindent
Note então que o valor $\sum\limits_{\sigma\in S_n}\prod_{i\in n}\alpha_{i,\sigma(i)} \sgn\sigma$ \emph{determina} $F$ para qualquer $v \in V^n.$ Chamaremos este valor de \textbf{determinante} de $F.$

\begin{exemplo}

\end{exemplo}

\section{Determinantes}

Seja $K$ um corpo e consideremos o anel das matrizes $M_n(K)$. Identificaremos os elementos de $M_n(K)$ com os elementos de $(K^n)^n$ assim:
\[
A=(a_{i,j})_{(i,j)\in n\times n}\leftrightarrow ((a_{i,j})_{j\in n})_{i\in n}
\]
Portanto, uma função $n$-linear aqui é uma função $n$-linear nas linhas da matriz.

\begin{definicao}
Uma função $\det:M_n(K)\rightarrow K$ é dita uma função \textbf{determinante} se e só se $\det$ é $n$-linear alternada e $\det(I)=1$.
\end{definicao}

\noindent
Pelo que vimos, existe e é única a função determinante: É a forma $n$-linear alternada que vale $1$ na base canônica de $K^n$.

\medskip
\noindent
Logo, se $A=(a_{i,j})\in M_n(K)$, então:
\[
\det(A)=\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{i,\sigma(i)}.
\]

\begin{exemplo}
Para $n=2$, temos $S_2=\{I,(0,1)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{0,0}&a_{0,1}\\a_{1,0}&a_{1,1}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{0,0}a_{1,1}-a_{0,1}a_{1,0}.
\]
\end{exemplo}

\begin{exemplo}
Agora, se $n=3$, então $S_3=\{I,(0,1,2),(0,2,1),(0,1),(0,2),(1,2)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{0,0}&a_{0,1}&a_{0,2}\\a_{1,0}&a_{1,1}&a_{1,2}\\a_{2,0}&a_{2,1}&a_{2,2}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{0,0}a_{1,1}a_{2,2}+a_{0,1}a_{1,2}a_{2,0}+a_{0,2}a_{1,0}a_{2,1}-a_{0,1}a_{1,0}a_{2,2}-a_{0,2}a_{1,1}a_{2,0}-a_{0,0}a_{1,2}a_{2,1}.
\]
\end{exemplo}

\begin{proposicao}
Temos as seguintes propriedades:
\begin{itemize}
\item[1)] Para todo $A\in M_n(K)$ temos $\det(A)=\det(A^t)$.
\item[2)] Para $A,B\in M_n(K)$ vale $\det(AB)=\det(A)\det(B)$.
\item[3)] Para $A\in M_n(K)$, então $A$ é inversível se e só se $\det(A)\neq 0$. Neste caso, temos $\det(A^{-1})=(\det(A))^{-1}$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Sendo $A=(a_{i,j})_{(i,j)\in n\times n}$, então temos:
\[
\begin{array}{rcl}
\det(A)&=&\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{i,\sigma(i)}\\&=&\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{\sigma^{-1}(i),i}\\&=&\sum_{\tau\in S_n}\sgn(\tau^{-1})\prod_{i\in n}a_{\tau(i),i}\\&=&\sum_{\tau\in S_n}\sgn(\tau)\prod_{i\in n}a^t_{i,\tau(i)}\\&=&\det(A^t).
\end{array}
\]
\item[2)] Seja $F_A:M_n(K)\rightarrow K$ tal que $\forall X\in M_n(K):F_A(X)=\det(AX)$. Então a função $F_A$ é uma função $n$-linear alternada sobre as colunas, mas também $F_A(I)=\det(A)$, aí $F_A(B)=\det(A)\det(B)$, assim $\det(AB)=\det(A)\det(B)$
\item[3)] Se $A$ é inversível, então existe a inversa $A^{-1}$, assim $1=\det(I)=\det(AA^{-1})=\det(A)\det(A)^{-1}$, aí $\det(A)\neq 0$ e $\det(A^{-1})=(\det(A))^{-1}$. Por outro lado, se $\det(A)\neq 0$, então $\det(A^t)\neq 0$, aí as colunas de $A$ são linearmente independentes, aí consideremos $T:K^n\rightarrow K^n$ tal que $[T]_{\mathrm{can}}=A$, então $T$ é inversível, assim $A=[T]_{\mathrm{can}}$ é inversível.
\end{itemize}
\end{proof}

\noindent
Assim lembremo-nos do seguinte: a função $\det$ é uma função $n$-linear e alternada nas linhas (ou nas colunas) da matriz, logo:
\begin{itemize}
\item[1)] Trocar duas linhas (ou colunas) da matriz muda o sinal do determinante.
\item[2)] Somar a uma linha (ou coluna) uma combinação linear das demais linhas (colunas) não altera o valor do determinante.
\item[3)] Ao multiplicar uma linha (ou coluna) por um escalar, o determinante fica multiplicado por esse escalar.
\end{itemize}

\begin{proposicao}
Temos o seguinte:
\begin{itemize}
\item[1)] O determinante de uma matriz triangular é o produto dos elementos da diagonal da matriz.
\item[2)] Se:
\[
A=\begin{pmatrix}
B&0\\C&D
\end{pmatrix}
\]
em que $B\in M_r(K)$ e $D\in M_{n-r}(K)$ e $C\in M_{n-r,r}(K)$ e $0\in M_{r,n-r}(K)$, então:
\[
\det(A)=\det(B)\det(D).
\]
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Seja $A=(a_{i,j})_{(i,j)\in n\times n}$ uma matriz triangular inferior, então para $i,j\in n$ tais que $i<j$ temos $a_{i,j}=0$, mas a única permutação $\sigma\in S_n$ tal que $\forall i\in n:i\geq\sigma(i)$ é a identidade, assim temos:
\[
\det(A)=\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{i,\sigma(i)}=\sgn(I)\prod_{i\in n}a_{i,I(i)}=\prod_{i\in n}a_{i,i}.
\]
\item[2)] Seja $F:M_r(K)\rightarrow K$ tal que:
\[
F(X)=\det\begin{pmatrix}
X&0\\C&D
\end{pmatrix}.
\]
Então $F$ é $r$-linear alternada nas linhas de $X$, assim $F(X)=F(I)\det(X)$.

\medskip
\noindent
Agora consideremos $G:M_{n-r}(K)\rightarrow K$ tal que:
\[
G(Y)=\det\begin{pmatrix}
I&0\\C&Y
\end{pmatrix}
\]
Então $G$ é $(n-r)$-linear alternada nas colunas de $Y$, logo $G(Y)=G(I)\det(Y)$. Mas:
\[
G(I)=\det\begin{pmatrix}
I&0\\C&I
\end{pmatrix}=1,
\]
assim $G(Y)=\det(Y)$, aí $F(I)=G(D)=\det(D)$, assim $F(X)=F(I)\det(X)=\det(X)\det(D)$, aí acaba.
\end{itemize}
\end{proof}

\noindent
Agora temos a \textbf{regra de Laplace}:

\begin{teorema}
Dada $A\in M_n(K)$, indicaremos por $M_{i,j}$ a matriz quadrada de tamanho $n-1$ obtida a partir de $A$ eliminando a linha $i$ e a coluna $j$.

\smallskip
\noindent
Para cada $i\in n$, então vale:
\[
\det(A)=\sum_{j\in n}(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
Para cada $j\in n$, então vale:
\[
\det(A)=\sum_{i\in n}(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
\end{teorema}
\begin{proof}
Provaremos a primeira afirmação pois a segunda é análoga.
\end{proof}
\textcolor{red}{AULA DE 19 DE AGOSTO (COLOCAREI ASSIM QUE CONSEGUIR)- FICOU
  FALTANDO A PROVA DA REGRA DE LAPLACE E A PARTE DE MATRIZES SOBRE ANEIS COMUTATIVOS}
Bláa blá blá

\chapter{Formas Canônicas}
,\section{Autovalores e Autovetores}
 \begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). Um vetor 
\(v\in V\) é um \emph{vetor próprio} ou \emph{autovetor} de \(T\) se existe \(\lambda\in K\) tal que \(T(v)=\lambda v\). O escalar \(\lambda\) é um \emph{valor próprio} ou \emph{autovalor} do operador \(T\). Dizemos que \(v\) é um autovetor associado com o autovalor \(\lambda\).
\end{definicao}
\begin{exemplo}
Seja \(V=\mathbb{C}^1(\mathbb{R})\) e considere o operador linear \(T\in\mathcal{L}(V)\) tal que \(T(v)=v^\prime\) para cada \(v\in V\). Considere \(v=e^{\lambda x}\) com \(\lambda\in K\). Então \(T(v)=\lambda e^{\lambda x}=\lambda v\). Ou seja \(v\) é um autovetor associado com o autovalor \(\lambda\).
\end{exemplo}
\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \emph{spectrum} do operador \(T\) é o conjunto
\[\text{Spec}(T)\coloneqq\{\lambda\in K\,\colon \lambda \text{ é autovalor de } T\}.\]
Para cada \(\lambda\in\text{Spec}(T)\), denotamos o conjunto dos autovetores associados a \(\lambda\) como \(V_T(\lambda)\).
\end{definicao}
 No contexto da definição anterior, considere
 \(\lambda\in\text{Spec}(T)\). Então
 \begin{align*}
    v\in V_T(\lambda)&\iff T(v)=\lambda v\\&\iff (T-\lambda I)(v)=0\\&\iff v\in\text{Ker}(T-\lambda I).
 \end{align*}
\textcolor{red}{ALGUMA COISA QUE EU PERDI}


Ainda no mesmo contexto, vamos assumir agora que \(\text{dim}(V)=n<\infty\). Então temos 
que \[\lambda\in\text{Spec}(T)\implies\text{Ker}(T-\lambda I)\not =\{0\}\implies\text{det}(T-\lambda I)=0.\]
Reciprocamente, se \(\text{det}(T-\lambda I)=0\) então \(V_T(\lambda)=\text{Ker}(T-\lambda I)\not=\{0\}\).
\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \emph{ polinômio característico} de \(T\) é a função \(p_T(\lambda)\colon K\to K\) dada por
\[p_T(\lambda)\coloneqq\text{det}(T-\lambda I), \text{ para cada }\lambda\in K.\]
\end{definicao}
Note que \(\lambda\in\text{Spec}(T)\) se e só se \(\lambda\) é raiz de \(p_T(\lambda)\). Além disso, note que se  \(B\) e \(B^\prime\) são bases \(V\), então \(p_T(\lambda)=p_{[T]_B}(\lambda)\). De fato, se \(P\) é a matriz de mudança da base \(B\) para a base \(B^\prime\), então
\begin{align*}
    [\lambda I - T]_{B^\prime}&=P^{-1}[\lambda I - T]_{B}P
\end{align*}
Isso implica que \[\text{det}([\lambda I- T]_{B^\prime})=\text{det}(P^{-1})\text{det}([\lambda I - T]_B)\text{det}(P).\]
Ou seja, \(\text{det}([\lambda I- T]_{B^\prime})=\text{det}([\lambda I - T]_B)\).
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^2)\) tal que \[[T]_{\text{can}}=\begin{pmatrix}
0 & -1\\1 & 0
\end{pmatrix}.\]
Isto é, \(T((x,y))=(-y,x)\) para cada \((x,y)\in\mathbb{R}^2\). Então 
\begin{align*}
p_T(\lambda)&=\text{det}\Bigg(\begin{pmatrix}
-\lambda & -1\\1 & -\lambda
\end{pmatrix}\Bigg)\\&=\lambda^2+1.
\end{align*}
Dessa forma, \(\text{Spec}(T)=\varnothing\) pois \(p_T(\lambda)\) não possui raízes em \(K=\mathbb{R}\).
\end{exemplo}
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que\[[T]_{\text{can}}=\begin{pmatrix}
3 & 1 & -1\\2 & 2 & -1\\ 2 & 2 & 0
\end{pmatrix}.\]
Então \begin{align*}
p_T(\lambda)&=\text{det}\left(\begin{pmatrix}
3-\lambda & 1 & -1\\2 & 2-\lambda & -1\\ 2 & 2 & -\lambda
\end{pmatrix}\right)\\&=(\lambda-1)^2(\lambda-2).
\end{align*}
Isso implica que \(\text{Spec}(T)=\{1,2\}\). Além disso, temos que
\[V_T(1)=\text{Ker}(T-I)=\text{Ker}(\begin{pmatrix}
2 & 1 & -1\\2 & 1 & -1\\ 2 & 2 & -1
\end{pmatrix})=\langle (1,0,2)\rangle.\]
e ainda
\[V_T(2)=\text{Ker}(T-2 I)=\text{Ker}(\begin{pmatrix}
1 & 1 & -1\\2 & 0 & -1\\ 2 & 2 & -2
\end{pmatrix})=\langle (1,1,2)\rangle\]
\end{exemplo}
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que
\[[T]_{\text{can}}=\begin{pmatrix}
1 & 2 & -1\\-2 & -3 & -1\\ 2 & 2 & -2
\end{pmatrix}.\]
Neste caso temos que \begin{align*}
p_T(\lambda)&=\text{det}(\begin{pmatrix}
1- \lambda & 2 & -1\\-2 & -3 - \lambda & -1\\ 2 & 2 & -2-\lambda
\end{pmatrix})\\&=(\lambda+1)^2(\lambda+2).    \end{align*}
Isso implica que \(\text{Spec}(T)=\{-1,-2\}\) e ainda \[V_T(-1)=\langle\{ (1,0,2),(0,1,2)\}\rangle\] e
\[V_T(-2)=\langle (1,-1,1)\rangle\].
Uma vez que os autovetores acima são L.I, eles formam uma base \(B\) de \(\mathbb{R}^3\) e 
\[[T]_B=\begin{pmatrix}
-1 & 0 & 0\\0 & -1 & 0\\ 0 & 0 & -2
\end{pmatrix}\]
é uma matriz diagonal.
\end{exemplo}
\begin{teorema}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que \(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\). São equivalentes:
\begin{enumerate}
    \item \(T\) é diagonalizável.
    \item \(p_T(t)=(t-\lambda_1)^{n_1}\ldots(t-\lambda_k)^{n_k}\) e \(\text{dim}(V_T(\lambda_i))=n_i\) para cada \(i\in [k]\).
    \item\(\text{dim}((V_T(\lambda_1))+\ldots+\text{dim}(V_T(\lambda_k))=\text{dim}(V)=n.\)
\end{enumerate}
\end{teorema}
\begin{lema}
Seja \(\{\lambda_i\}_{i\in [k]}\subseteq\text{Spec}(T).\)
\begin{enumerate}
    \item Se \(v_i \in V_T(\lambda_i)\) para cada \(i\in [k]\) e \(v_1+\ldots+v_k=0\), então \(v_1=\ldots=v_k=0\).
    \item Se \(B_i\subseteq V_T(\lambda_i)\) é L.I para cada \(i\in [k]\), então \(\bigcup_{i\in [k]}B_i\) é L.I.
\end{enumerate}
\end{lema}
\begin{proof}[Demonstração do Lema]\hfill
\begin{enumerate}
\item Vamos provar essa afirmação por indução em \(k\). Primeiro note que o
      resultado é trivial quando \(k=1\). Agora seja \(k\in\mathbb{N}\) e
      assuma  que o resultado vale para cada natural \(i < k\). Sejam
      \(v_1,\ldots,v_k\) tais que \(v_i\in V_T(\lambda_i)\) para cada \(\i\in
      [k]\) e \(v_1+\ldots + v_k=0\). Então temos que 
      \begin{equation}\label{primeira}
      \lambda_1(v_1+\ldots+v_k)=0\lambda_1=0.
    \end{equation}
    Além disso, é claro que
      \begin{equation}\label{segunda}
      T(v_1+\ldots +v_k)=\lambda_1v_1+\ldots +\lambda_kv_k=0.
      \end{equation}
      Subtraindo a Equação \ref{primeira} de \ref{segunda}, obtemos
      \begin{equation}\label{terceira}
       (\lambda_1-\lambda_1)v_1+(\lambda_2-\lambda_1)v_2+\ldots +(\lambda_k-\lambda_1)v_k=0.
     \end{equation}
     Agora notamos que cada termo no lado esquerdo é um autovetor de \(T\)     e
     aplicamos a hipótese de indução para concluir que \(v_2=\ldots =v_k=0
     \). Finalmente, como sabemos que \(v_1+\ldots +v_k=0\) e \(v_2=\ldots =
     v_k=0\), obtemos que \(v_1=0\) também, o que conclui nossa prova.
     \item Seja \(S\subseteq\bigcup_{i\in [k]}B_i\) finito e seja \(\alpha\colon S\to\mathbb{R}\). Note que
       \(V_T(\lambda_i)\cap V_T(\lambda_j)=\{0\}\) sempre que \(i,j\in [k]\) e
       \(i\not=j\) e então podemos escrever
       \[\sum_{v\in S}\alpha_iv_i=\sum_{v\in S_1}\alpha_v v+\ldots+\sum_{v\in
           S_k}\alpha_vv,\]
       onde \(S_i\subseteq B_i\) é finito para cada \(i\in [k]\). Utilizan
       do o fato de que o termo \mbox{\(\sum_{v\in S_i}\alpha_v v\in V_T(\lambda_i)\)}
       para cada \(i\in [k]\) e aplicando o item anterior, obtemos que
       \[\sum_{v\in S_1}\alpha_vv=\ldots=\sum_{v\in S_k}\alpha_vv=0.\]
       Finalmente como como \(S_i\subseteq B_i\) para cada \(i\in [k]\) e
       \(B_i\) é sempre L.I por hipótese segue que a restrição de \(\alpha\) a
       cada \(S_i\) é identicamente nula. Como \(S=\bigcup_{i\in [k]}S_i\) segue
       que \(\alpha\) é indenticamente nula.\qedhere
    \end{enumerate}
\end{proof}

\begin{proof}
\begin{itemize}\hfill
\item (i)$\Rightarrow$(ii): Sejam $(v_{i,j})_{j\in n_i}$ autovetores associados
  a $\lambda_i\in\text{Spec}(T)$\textcolor{red}{TERMINAR ESSA IMPLICAÇÃO}  
\item (ii)\(\Rightarrow\)(iii):\[\text{dim}(V_T(\lambda_1))+\ldots +\text{dim}(V_T(\lambda_k))=n_1+\ldots +n_k=\text{deg}(p_T(t))=\text{dim}(V)=n.\]
\item (iii)\(\Rightarrow\)(i): Para cada \(i\in [k]\) considere uma base \(B_i\)
  de \(V_T(\lambda_i)\). Seja \(B=\bigcup_{i\in [k]}B_i\). Pelo lema anterior,
  temos que \(B\) é L.I. Como \(\vert B\vert =n\) segue que \(B\) é uma base de
  \(V\). Além disso, \(B\) é uma base de autovetores de \(T\). Logo, \(T\) é diagonalizável.
\end{itemize}
\end{proof}

\section{Polinômio Minimal}

\begin{definicao}
Seja $V$ um espaço sobre $K$, $\dim V=n<\infty$, $T\in\mathcal{L}(V)$. Definamos por recursão $T^0=I$ e $T^{k+1}=T^k\circ T$. Se $p(t)\in K[t]$, $p(t)=a_0+a_1t+\cdot+a_mt^m$, então está bem definido o operador $p(T)=a_0\cdot I+a_1\cdot I+\dots+a_m\cdot T^m\in\mathcal{L}(V)$.
\end{definicao}

\noindent
Lembremo-nos de que, se $\dim(U)=m$ e $\dim(V)=n$, então $\dim\mathcal{L}(U,V)=mn$. Assim, se $V$ é um espaço vetorial tal que $\dim(V)=n<\infty$, então $\dim\mathcal{L}(V)=n^2$, de modo que existe $m\leq n^2+1$ tal que os operadores $I,T,T^2,\dots,T^m$ sejam linearmente dependentes. Seja $m$ um número minimal tal que $T^m\in\langle I,T,\dots,T^{m-1}\rangle$. Então $T^m=a_0\cdot I+a_1\cdot T+\dots a_{m-1}T^{m-1}$, com $a_i\in K$. Seja $m_T(t)=t^m-a_{m-1}t^{m-1}-\dots-a_1t-a_0$, então $m_T(t)=0$, e $m_T(t)$ é um polinômio de menor grau tal que $m_T(t)=0$.

\begin{definicao}
Um polinômio mônico de grau mínimo tal que $m_T(t)\in K[t]$ tal que $m_T(t)=0$ chama-se um \textbf{polinômio minimal} do operador $T$. Chamemos um polinômio $f(t)\in K[t]$ de um \textbf{polinômio anulador} de $T$ se $f(T)=0$.
\end{definicao}

\begin{lema}
Seja $f(t)\in K[t]$ tal que $f(T)=0$. Então $m(t)\mid f(t)$.
\end{lema}
\begin{proof}
Dividimos $f(t)$ por $m(t)$ (com resto):
\[
f(t)=m_T(t)\cdot q(t)+r(t),\quad\quad\deg(r(t))<\deg(m_T(t))\text{ ou }r(t)=0.
\]
Como $f(T)=0$ e $m_T(t)=0$, então $r(T)=0$, aí $r(t)=0$.
\end{proof}

\begin{corolario}
O polinômio $m_T(t)$ é único.
\end{corolario}

\noindent
Se $V$ é um espaço vetorial e $T\in\mathcal{L}(V)$, então $V$ tem uma estrutura de $K[t]$ módulo à esquerda: Se $f(t)\in K[t]$, para $v\in V$ definimos:
\[
f(t)\cdot v=f(T)(v).
\]
Além disso, se considerarmos:
\[
\begin{array}{rcl}
\varphi:K[t]&\rightarrow&\mathrm{End}(V)\\f(t)&\mapsto&f(T),
\end{array}
\]
então $\varphi$ é um homomorfismo de $K$-álgebras e portanto $\mathrm{Ker}(\varphi)$ é um ideal de $K[t]$.

\begin{teorema}
Os polinômios $p_T(t)$ e $m_T(t)$ têm as mesmas raízes em $K$ (a menos de multiplicidade). Em outras palavras, $m_T(\lambda)=0\Leftrightarrow\lambda\in\mathrm{Spec}(T)$.
\end{teorema}
\begin{proof}
Se $m_T(\lambda)=0$, então $m_T(t)=(t-\lambda)q(t)$. Por minimalidade de $m_T(t)$, $q(T)\neq 0$, então existe $w\in V$ tal que $q(T)(w)\neq 0$, aí seja $v=q(T)(w)$, então $v\neq 0$ e:
\[
\begin{array}{rcl}
(T-\lambda I)(v)&=&(T-\lambda I)q(T)(w)\\&=&m_T(t)(w)=0,
\end{array}
\]
aí $T(v)=\lambda v$, aí $\lambda\in\mathrm{Spec}(T)$.

\medskip
\noindent
Por outro lado, se $\lambda\in\mathrm{Spec}(T)$, seja $v\in V$ tal que $v\neq 0$ e $T(v)=\lambda v$, então $T(T(v))=\lambda^2 v,\dots,T^m(v)=\lambda^m v,\dots$, aí para $f(t)\in K[t]$ temos $f(T)(v)=f(\lambda)\cdot v$, aí $0=m_T(T)(v)=m_T(\lambda)\cdot v$, aí $m_T(\lambda)=0$.
\end{proof}

\begin{corolario}
Se $T$ é diagonalizável e $\mathrm{Spec}(T)=\{\lambda_i\}_{i\in r}$, então $m_T(t)=\prod_{i\in r}(t-\lambda_i)$.
\end{corolario}

\noindent
Se $\mathrm{Spec}(T)=\{\lambda_i\}_{i\in r}$, então:
\[
V=\sum_{i\in r}V_T(\lambda_i).
\]

\begin{proof}
Já sabemos que $m_T(t)=\left(\prod_{i\in r}(t-\lambda_i)^{k_i}\right)$ em que $q(t)$ não tem raízes em $K$. Basta provar que $(T-\lambda_0 I)\dots(T-\lambda_{r-1} I)=0$. Seja $v\in V$, $v=v_0+\dots+v_{r-1}$, com $v_i\in V_T(\lambda_i)$, então temos $(T-\lambda_i I)(v_i)=0$, portanto $()(v_i)=0$, e logo $()(v)=0$. Então o polinômio $f(t)=(t-\lambda_0)\dots$ é um polinômio anulador para $T$, aí $m_T(t)\mid f(t)$, aí $m_T(t)=f(t)$.
\end{proof}

\section{Subespaços Invariantes}

\begin{definicao}
Seja $T\in\mathcal{L}(V)$. Um subespaço $W\subseteq V$ chama-se \textbf{$T$-invariante} se $T(W)\subseteq W$.
\end{definicao}

\begin{observacao}
Um subespaço é $T$-invariante se e só se é um $K[t]$-submódulo.
\end{observacao}
\begin{exemplo}
Seja \(V=\mathbb{C}(\mathbb{R})\) e considere o operador \(D\colon f\to
f^\prime\). Então o subespaço\[P_n\coloneqq\{f(t)\in\mathbb{R}[t]\,\colon
  \text{deg}(f)\leq n\}\] é \(D\)-invariante. 
\end{exemplo}
  \noindent
Seja $\dim(V)=n$ e $T\in\mathcal{L}(V)$, $W\subseteq V$ um subespaço
$T$-invariante. Escolhemos uma base $B_1=\{v_i\}_{i\in m}$ de $W$ e
completemo-la até uma base $B=\{v_i\}_{i\in n}$ do espaço $V$. Qual é a matriz
$[T]_B$?

Vamos começar notando que \(T\) é \(W\)-invariante e então 
\begin{align*}
&T(v_1)=\sum_{i\in m}\alpha_{1i}v_i\\&T(v_2)=\sum_{i\in m}\alpha_{2i}v_i\\&\ldots\\&T(v_m)=\sum_{i\in m}\alpha_{mi}v_i\\&T(v_{m+1})=\sum_{i\in n}\alpha_{(m+1)i}v_i\\&\ldots\\&T(v_n)=\sum_{i\in n}\alpha_{ni}v_i.
\end{align*}

Dessa forma segue que
\[[T]_B=\begin{pmatrix}
    \alpha_{11} & \ldots & \alpha_{1m} &\alpha_{1(m+1)}& \ldots & \alpha_{1n}
    \\ \ldots & \ldots & \ldots & \ldots &\ldots & \ldots
    \\\alpha_{m1} & \ldots & \alpha_{mm} & \alpha_{m(m+1)} & \ldots &
    \alpha_{mn}\\ 0 & \ldots & 0 & \alpha_{(m+1)(m+1)}&\ldots &\alpha_{(m+1)n}\\
    \ldots & \ldots & \ldots & \ldots &\ldots & \ldots \\ 0 & 0 & 0
    &\alpha_{n(m+1)} & \ldots & \alpha_{nn}
\end{pmatrix}.\]

Isto é, a matriz de \(T\) na base \(B\) tem a forma

\[[T]_B=\begin{pmatrix}
    A & * \\ 0 & B\end{pmatrix},\]
onde \(A\in M_m(K)\) e \(B\in M_{n-m}(K)\). Note que \(A\) é a matriz da
restrição de \(T\) a \(W\) na base \(B_1\).

Vamos agora considerar a restrição de \(T\) a \(W\), denotada por
\(T\upharpoonright W\). Claramente, temos que \(T\upharpoonright
W\in\mathcal{L}(W)\). Considere \(\bar{V}\coloneqq\frac{V}{W}\) e seja 
\(\pi\colon V\to\bar{V}\) uma projeção. Seja \(\bar{T}\coloneqq \pi\circ T\).
Então \(\bar{T}\in\mathcal{L}(V,\bar{V})\) e \(W\subseteq\text{Ker}(\bar{T})\).
De fato, para cada \(w\in W\) temos \[\pi(T(w))\in \pi(W)=0.\]
Além disso, \(\bar{T}\) induz um operador linear
\(\tilde{T}\in\mathcal{L}(\bar{V})\) definido por \(\tilde{T}(v+W)=\bar{T}(v)\).

\begin{proposicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\), seja \(T\in\mathcal{L}(V)\)
e seja \(W\) um subespaço \(T\)-invariante de \(V\). Considere \(B\coloneqq
B_1\cup B_2\) onde \(B\) é uma base de \(V\) e \(B_1\) é uma base de \(W\).
Então \[[T]_B=\begin{pmatrix}
    A & * \\ 0 & X 
  \end{pmatrix}\]
onde \(A=[T\upharpoonright W]_{B_1}\) e \(X=[\tilde{T}]_{B_2}\).
\end{proposicao}
\begin{proof}
\textcolor{red}{COMPLETAR}
\end{proof}


\begin{lema}
Seja $\dim(V)=n$, $T\in\mathcal{L}(V)$ e $W\subseteq V$ um subsepaço $T$-invariante. Então:
\[
p_T(t)=p_{T_1}(t)\cdot p_{T_2}(t)
\]
em que $T_1\in\mathcal{L}(W)$ e $T_2\in\mathcal{L}(W)$ com $T_1(w)=T(w)$ e $T_2(v+W)=T(v)+W$.
\end{lema}
\begin{proof}
Escolhamos $B_1$ e $B$ como bases de $W$ e $V$ tais que $B_1\subseteq B$, então:
\begin{align*}
  p_T(t)&=\det[tI-T]_B\\
        &=\text{det}\begin{pmatrix}
          tI_m-A & * \\ 0 & tI_{n-m}-B\end{pmatrix}\\
        &=\text{det}(tI_m-A)\text{det}(tI_{n-m}-B)\\
        &=p_A(t)p_B(t)=p_{T_1}(t)p_{T_2}(t)
\begin{align*}
\end{proof}

\begin{observacao}
O mesmo \textcolor{red}{não} ocorre para polinômios minimais. De fato, seja $T=I_V$ e seja $W$ um subespaço $T$-invariante (De fato, quando \(T\) é a identidade, todo subespaço de \(V\) é \(T\)-invariante), então $T_1=I_W$ e $T_2=I_{V/W}$ e aí $m_T(t)=m_{T_1}(t)=m_{T_2}(t)=t-1$.
\end{observacao}

  \begin{teorema}[Teorema da Cayley-Hamilton]
   Seja \(V\) um espaço vetorial de dimensão finita sobre um corpo \(K\), e seja \(T\in\mathcal{L}(V)\). Então \(p_T(T)=0\), onde \(p_T(t)\in K[t]\) é um polinômio característico de \(T\). 
  \end{teorema}
   \begin{proof}
     Basta provar que \(p_T(T)(v)=0\) para cada \(v\in V\). Se \(v=0\) o resultado é evidente.
     Então seja \(0\not=v\in V\).
     Note que como \(V\) tem dimensão finita temos que existe um \(m \leq n = \text{dim}(V)\) mínimo tal que existem coeficientes \(\alpha_0,\ldots,\alpha_{m-1}\) tais que \(T^m(v)=\sum_{i=0}^{m-1}\alpha_iT^i(v)\). Para tal \(m\in\mathbb{N}\), considere o conjunto
     \(\{v,T(v),\ldots, T^{m-1}\}\) e seja \(W\) o subespaço gerado por ele. Note que \(W\) é \(T\)-invariante e ainda
     \[[T\upharpoonright W]_B]\begin{pmatrix}
         0 & 0 & \ldots& 0 &\alpha_0
         \\ 1 & 0 & \ldots & 0 & \alpha_1
         \\ 0 & 1 & \ldots & 0 & \alpha_2
         \\ \ldots & \ldots  & \ldots  & \ldots  & \ldots \
         \\ 0 & 0 & \ldots & 1 & \alpha_{m-1} 
       \end{pmatrix}=A.\]
     Então, pelo Exercício 18 da lista 1, segue que \[p_A(t)=p_{T_{\upharpoonright W}}(t)= t^m -\alpha_{m-1}t^{m-1}- \ldots -\alpha_1t-\alpha_0.\]
     E também
     \[p_{T_{\upharpoonright W}}(T)=T^m-\alpha_{m-1}T^{m-1}-\ldots-\alpha_1 T-\alpha_o I.\]
     Aplicando essa última função a \(v\) segue:
     \[p_{T_{\upharpoonright W}}(T)(v)=T^m(v)-\alpha_{m-1}T^{m-1}(v)-\ldots-\alpha_1 T(v)-\alpha_o v=0.\]
     Para concluir que \(p_t(T)(v)=0\) escrevemos \(p_T(T)=p_{T_{\upharpoonright W}}q(T).\)
     \end{proof}
     \begin{corolario}
                                   Se \(A\in M_n(K)\) então \(P_A(A)=0\), onde \(P_A(t)=\text{det}(tI -A)\). 
                                   \end{corolario}

                                 \begin{exemplo}
                                   Considere a matriz \[\begin{pmatrix}
                                       a & b \\ c & d
                                     \end{pmatrix}.\]
                                   Então temos que \(p_A(t)=t^2-(a+d)t+(ad-bc)\) e também
                                   \begin{align*}
                                     P_A(A)&=A^2-(a+d)A+(ad-bc)I\\&=
                                     \begin{pmatrix}
                                       a^2+bc & ab+ad\\ac+dc & bc+d^2
                                       \end{pmatrix}
                                                               -\begin{pmatrix} a^2+ad & ab+bd \\ ac+dc & ad+d^2\end{pmatrix} + \text{det}(A)I
                                                                 \\&=\begin{pmatrix}bc-ad & 0 \\ 0 & bc- ad\end{pmatrix} + \begin{pmatrix}ad-bc & 0 \\ 0 & ad - bc\end{pmatrix} = 0.
                                     \end{align*}
                                 \end{exemplo}
                                                                                                                                                                       \begin{teorema}[Decomposição Primária]
                     Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que \(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\). Suponhamos que \(f(T)=0\), onde \[f(t)=p_1^{k_1}(t)\ldots p_r^{k_r}(t)\] e cada \(p_i(t)\in K[t]\) é irredutícel. Então \(V=V_1\oplus\ldots\oplus V_r\) onde cada \(V_i\) é \(T\)-invariante e \(p_i^{k_i}(T_{\upharpoonright V_i})=0\).
                     \end{teorema}
                                                                                                                                                                        \begin{lema}
                                                                                                                                                                          Seja \(f(T)=0\) onde \(f(t)=f_1(t)f_2(t)\) com \(f_1\) e \(f_2\) primas entre si. Então \(V=V_1\oplus V_2\) onde \(V_1\) e \(V_2\) são \(T\)-invariantes, \(f_1(T_{\upharpoonright V_1})=0\) e \(f_2(T_{\upharpoonright V_2})=0\)         \end{lema}

                                                                                                                                                                        \begin{sublema}[Identidade de Bezout]
                                                                                                                                                                          Se \(\text{m.d.c}(f_1(t),f_2(t))=1\) então existem
                                                                                                                                                                          \mbox{\(r(t),s(t)\in K[t]\)} tais que
                                                                                                                                                                          \[\f_1(t)r(t)+f_2(t)s(t)=1.\]                                                            \end{sublema}
\begin{proof}[Demonstração do Lema]
Considere \(V_1\coloneqq\text{Im}(f_1(T)) e \(V_2\coloneqq\text{Im}(f_1(T))\). Vamos verificar que \textcolor{red}{continuardaqui}                                                                                                                                                    \end{proof}
                                                                                                                                                                        \begin{proof}[Demonstração do Teorema]
                                                                                                                                                                         
                                                                                                                                                                        \end{proof}



                                                                                                                                                                        \printindex

\end{document}