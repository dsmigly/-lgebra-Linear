%%% ------- Readme -------%%%

%%-----------------------%%%
\documentclass[11pt,twoside,a4paper]{book}
\usepackage{estilos} 
\externaldocument{listas}
\makeindex
\title{Álgebra Linear \\  Douglas Smigly}
\author{MAT5730}
\date{$2^o$ semestre de 2019}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\chapter*{Informações da Disciplina}
\label{sec:intro}
\addcontentsline{toc}{section}{\nameref{sec:intro}}
\question{Informações Básicas}\label{new-question}

Essas são as notas de aula de Álgebra Linear(MAT5730), as aulas acontecem na sala B-134 às terças 10h e às quintas 8h.

\question{Informações do Professor}

O professor é o Ivan Shestakov, sua sala é a 290-A e o seu e-mail é shestak@ime.usp.br

\question{Bibliografia}
\nocite{*}
\bibliographystyle{plain}
\bibliography{samples}
\question{Avaliação}

A nota final da disciplina será a média aritimética de P1, P2, e P3. Todos os
alunos poderão fazer a prova sub para substituir a menor das suas notas (Sub
aberta). As datas das provas são as seguintes:

\begin{table}[h!]
\begin{center}

\label{tab:table1}
\begin{tabular}{l|r} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
\textbf{Prova} & \textbf{Data}\\
\hline
P1 & 10-09\\
P2 & 15-10\\
P3 & 12-11\\
SUB & 19-11
\end{tabular}
\end{center}
\end{table}

\question{Outras Informações}
\begin{enumerate}[label=(\roman*)]
\item Teremos listas, que não contarão para a nota
\item As listas serão publicadas em 
\item Não haverá monitoria
\end{enumerate}
\newpage 

\chapter{Espaços vetoriais}

\section{Definições Iniciais}

\begin{definicao}
Um \textbf{grupo abeliano} é um conjunto $X$ munido do seguinte:
\begin{itemize}
\item $+:X\times X\rightarrow X$,
\item $0\in X$,
\item $-:X\rightarrow X$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $x,y,z\in X$ então $(x+y)+z=x+(y+z)$,
\item Para $x,y\in X$ então $x+y=y+x$,
\item Para $x\in X$ então $x+0=x$,
\item Para $x\in X$ então $x+(-x)=0$.
\end{itemize}
\end{definicao}

\begin{definicao}
Um \textbf{corpo} é um grupo abeliano $(K,+,0,-)$ munido do seguinte:
\begin{itemize}
\item $\cdot:K\times K\rightarrow K$,
\item $1\in K$,
\item $\cdot^{-1}:K\setminus\{0\}\rightarrow K$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $x,y,z\in K$ então $(x\cdot y)\cdot z=x\cdot(y\cdot z)$,
\item Para $x,y\in K$ então $x\cdot y=y\cdot x$,
\item Para $x\in K$ então $x\cdot 1=x$,
\item Para $x\in K\setminus\{0\}$ então $x\cdot x^{-1}=1$,
\item Para $x,y,z\in K$ então $x\cdot(y+z)=(x\cdot y)+(x\cdot z)$.
\end{itemize}
\end{definicao}

\begin{definicao}
Dado $K$ um corpo, um \textbf{espaço vetorial sobre $K$} é um grupo abeliano $(V,+,0,-)$ munido do seguinte:
\begin{itemize}
\item $\cdot:K\times V\rightarrow V$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $a,b\in K$ e $x\in V$ então $(a\cdot b)\cdot x=a\cdot(b\cdot x)$,
\item Para $x\in V$ então $x\cdot 1=x$,
\item Para $a\in K$ e $x,y\in V$ então $a\cdot(x+y)=(a\cdot x)+(a\cdot y)$,
\item Para $a,b\in K$ e $x\in V$ então $(a+b)\cdot x=(a\cdot x)+(b\cdot x)$.
\end{itemize}
\end{definicao}

\section{Base e Dimensão}

Durante o restante deste capítulo, sempre adotaremos $K$ como sendo um corpo qualquer.

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Seja $I$ um conjunto e $v:I\rightarrow V$ uma função. Uma \textbf{combinação linear} de $v$ é um elemento $u\in V$ tal que existam um conjunto finito $J\subseteq I$ e uma função $\alpha:J\rightarrow K$ tais que:
\[
u=\sum_{i\in J}\alpha_iv_i.
\]
Dizemos que $v$ \textbf{gera} $V$ se e só se todo elemento de $V$ é combinação linear de $v$.
\end{definicao}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Dizemos que um conjunto $S\subseteq V$ \textbf{gera} $V$ se e só se a função $v:S\rightarrow V$ dada por $\forall s\in S:v_s=s$ gera $V$.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e sejam $I$ um conjunto e $v:I\rightarrow V$ uma função. Então $v$ gera $V$ se e somente se a imagem $S=\{v_i:i\in I\}$ gera $V$.
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Se $v$ gera $V$, então para $x\in V$ existem um conjunto finito $J\subseteq I$ e uma função $\alpha:J\rightarrow K$ tais que:
\[
x=\sum_{i\in J}\alpha_iv_i,
\]
aí seja $T=v[J]$, e seja:
\[
\forall s\in T:J_s=\{i\in J:v_i=s\};
\]
e seja $\beta:T\rightarrow K$ a função dada por:
\[
\forall s\in T:\beta_s=\sum_{i\in J_s}\alpha_i,
\]
então $T$ é finito, aí temos:
\[
x=\sum_{i\in J}\alpha_iv_i=\sum_{s\in T}\sum_{i\in J_s}\alpha_iv_i=\sum_{s\in T}\sum_{i\in J_s}\alpha_is=\sum_{s\in T}\beta_ss;
\]
logo $S$ gera $V$.
\item Se $S$ gera $V$, então para $x\in V$ existem um conjunto finito $T\subseteq S$ e uma função $\beta:T\rightarrow K$ tais que:
\[
x=\sum_{s\in T}\beta_ss,
\]
aí existe uma função $i:T\rightarrow I$ tal que:
\[
\forall s\in T:v_{i_s}=s,
\]
aí seja $J=\mathrm{Im}(i)$, então $J$ é finito e $i$ é uma bijeção de $T$ a $J$ e sua inversa é $u=v\upharpoonright J$, aí seja $\alpha=\beta\circ u$, então:
\[
x=\sum_{s\in T}\beta_ss=\sum_{j\in J}\beta_{u_j}u_j=\sum_{j\in J}\alpha_ju_j=\sum_{j\in J}\alpha_jv_j;
\]
logo $v$ gera $V$.\qedhere
\end{itemize}
\end{proof}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $I$ um conjunto e seja $v:I\rightarrow V$ uma função. Dizemos que $v$ é \textbf{linearmente independente} se e só se para todo conjunto finito $J\subseteq I$ e toda função $\alpha:J\rightarrow K$, então temos a implicação:
\[
\sum_{i\in J}\alpha_iv_i=0\quad\Rightarrow\quad\forall i\in J:\alpha_i=0.
\]
Dizemos que $v$ é \textbf{linearmente dependente} se e só se $v$ não é linearmente independente.
\end{definicao}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $S\subseteq V$ um conjunto. Dizemos que $S$ é \textbf{linearmente independente} se e só se a função $v:S\rightarrow V$ dada por $\forall s\in S:v_s=s$ é linearmente independente. Dizemos que $S$ é \textbf{linearmente dependente} se e só se não é linearmente independente.
\end{definicao}

\begin{exemplo}
Se $V$ é um espaço vetorial sobre um corpo $K$ e $u\in V$ é um elemento não nulo, então a função $v:\{0,1\}\rightarrow V$ dada por $v_0=u$ e $v_1=u$ é linearmente \textcolor{red}{dependente}, mas o conjunto $\{v_0,v_1\}=\{u\}$ é linearmente \textcolor{red}{independente}.
\end{exemplo}

\begin{definicao}\label{lideford}\index{Espaço Vetorial!Base ordenada}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $I$ um conjunto. Uma \textbf{base de $V$ ordenada por $I$} é uma função $b:I\rightarrow V$ tal que:
\begin{enumerate}[label=(\roman*)]
\item $b$ é linearmente independente.
\item $b$ gera $V$.
\end{enumerate}
\end{definicao}

\begin{definicao}\label{lidef}\index{Espaço Vetorial!Base}
Seja $V$ um espaço vetorial sobre um corpo $K$. Uma \textbf{base} de $V$ é um conjunto $B\subseteq V$ tal que:
\begin{enumerate}[label=(\roman*)]
\item $B$ é linearmente independente.
\item $B$ gera $V$.
\end{enumerate}
\end{definicao}

\begin{teorema}\label{existbase}
Seja $V$ um espaço vetorial e sejam $I\subseteq V$ linearmente independente e $S\subseteq V$ gerador de $V$ tais que $I\subseteq S$. Então existe uma base $B$ de $V$ tal que \[I\subseteq B\subseteq S.\]
\end{teorema}
\begin{proof}
Consideremos o conjunto:
\[
\mathcal{M}\coloneqq\{M\subseteq S\mid M\text{ é linearmente independente e }I\subseteq M\}
\]
Então $(\mathcal{M},\subseteq)$ é um conjunto parcialmente ordenado indutivo (ou seja, todo subconjunto totalmente ordenado possui uma cota superior). De fato, $I\in\mathcal{M}$, o que nos mostra que $\mathcal{M} \neq \emptyset,$ e para subconjunto totalmente ordenado não vazio $\mathcal{C}\subseteq\mathcal{M}$ então $\bigcup\mathcal{C}\in\mathcal{M}.$ 

\medskip
\noindent
Logo, pelo Lema de Zorn, $\mathcal{M}$ possui um elemento maximal $B$. Vamos provar que esse elemento maximal é de fato uma base para $V.$
\begin{enumerate}[label=(\roman*)]
\item $B$ é linearmente independente: segue da definição de $\mathcal{M}.$
\item $B$ gera $V$: Suponha por absurdo que $B$ não gera $V$. Então existe $v\in S$ que não é combinação linear de elementos de $B$, aí $B\cup\{v\}$ é linearmente independente e $I\subseteq B\cup\{v\}\subseteq S$. Então $B\cup\{v\}\in\mathcal{M}$, uma contradição, pois $B$ já é um elemento maximal de $\mathcal{M}$ e obviamente $B\subseteq B\cup\{v\}$. Logo $B$ gera $V$. Portanto, $B$ é uma base de $V$ e $I\subseteq B\subseteq S$.\qedhere
\end{enumerate}
\end{proof}

\noindent
O resultado acima mostra que todo espaço vetorial tem base, bastando para isso tomar $I=\emptyset$ e $S=V.$
\begin{corolario}
Seja $V$ um espaço vetorial sobre um corpo $K$,
seja $I\subseteq V$ um
conjunto linearmente independente e seja $S\subseteq V$
um conjunto que gere $V$. Então
\begin{enumerate}[label=(\roman*)]
\item O espaço $V$ tem uma base;
\item Existe uma base $B$ de $V$ tal que $I\subseteq B$; 
\item Existe uma base $B$ de $V$ tal que $B\subseteq S$.
\end{enumerate}
\end{corolario}

\begin{lema}\label{basefin1} Seja $V$ um espaço vetorial sobre um corpo $K$.
Sejam $(v_1,\dots,v_n)$ uma sequência linearmente independente e
$(u_1,\dots,u_m)$ uma sequência que gera $V$. Então $n\leq m$.
\end{lema}

\begin{sublema} Seja $V$ um espaço vetorial sobre um corpo $K$.
Uma sequência $(v_1,\dots,v_m)$ é linearmente dependente se e somente se existem $i$ e uma sequência $(\alpha_1,\dots,\alpha_{i-1})$ tais que \[v_i=\sum\limits_{j=1}^{i-1}\alpha_jv_j.\]
\end{sublema}
\begin{proof}[Demonstração do Sublema]
Se $(v_1,\dots,v_m)$ é linearmente dependente, então existe uma sequência $(\alpha_1,\dots,\alpha_m)$ não identicamente nula tal que:
\[
\sum_{i=1}^m\alpha_iv_i=0.
\]
Seja $i$ o maior índice tal que $\alpha_i\neq 0$.
Então segue que
\begin{align*}
&\alpha_1 v_1 + \ldots + \alpha_i v_i = 0 \\ \iff & \alpha_1v_1 + \ldots + \alpha_{i-1} v_{i-1} = - \alpha_i v_i \\ \iff &
v_i= - \sum\limits_{j=1}^{i-1}\frac{\alpha_j}{\alpha_i}v_j.\qedhere
\end{align*}
\end{proof}

\begin{proof}[Demonstração do Lema]
Primeiro, listamos os dois conjuntos de vetores: o conjunto gerador seguido do conjunto linearmente independente:
\[
u_1,\dots,u_m;v_1,\dots,v_n
\]
Então movemos o primeiro vetor $v_1$ para a esquerda da primeira lista:
\[
v_1,u_1,\dots,u_m;v_2,\dots,v_n
\]
Como $u_1,\dots,u_m$ gera $V$, $v_1$ é combinação linear dos $u_i$'s. Isso implica que
podemos remover um dos $s_i$'s, que indexando se necessário pode ser $u_1$,
da primeira lista, e ainda temos um conjunto gerador:
\[
v_1,u_2,\dots,u_m;v_2,\dots,v_n
\]
Note que o primeiro conjunto dos vetores ainda gera $V$ e o segundo conjunto ainda é linearmente
independente.

\medskip
\noindent
Agora repetimos o processo, movendo $v_2$ da segunda lista para a primeira lista:
\[
v_1,v_2,u_2,\dots,u_m;v_3,\dots,v_n
\]
Como antes, os vetores na primeira lista são linearmente dependentes, já que eles geravam
$V$ antes da inclusão de $v_2$. Entretanto, como os $v_i$'s são linearmente independentes,
qualquer combinação linear não trivial dos vetores na primeira lista que valha $0$
deve envolver pelo menos um dos $u_i$'s. Portanto, podemos remover este vetor, que
novamente reindexando se necessário pode ser $u_2$ e ainda temos um conjunto gerador:
\[
v_1,v_2,u_3,\dots,u_m;v_3,\dots,v_n
\]
Mais uma vez, o primeiro conjunto dos vetores gera $V$ e o segundo conjunto é linearmente
independente.

\medskip
\noindent
Agora, if $m<n$, então este processo eventualmente esgotará os $u_i$'s e nos levará à lista
\[
v_1,v_2,\dots,v_m;v_{m+1},\dots,v_n
\]
em que $v_1,v_2,\dots,v_m$ geram $V$, o que claramente não é possível pois $v_n$ não é combinação linear
dos $v_1,v_2,\dots,v_m$. Portanto $n\leq m$.
\end{proof}

\begin{observacao}\label{basefin2}
Com o lema, também podemos mostrar que, se existe um conjunto gerador finito, então podemos mostrar que todo conjunto linearmente independente é finito.

\medskip
\noindent
De fato, se existirem uma sequência geradora $(u_1,\dots,u_m)$ e um conjunto linearmente independente infinito $S$, então podemos pegar $m+1$ vetores distintos e assim formar uma sequência linearmente independente $(v_1,\dots,v_{m+1})$, contradizendo o lema \ref{basefin1}.
\end{observacao}

\noindent
Vamos relembrar o que fizemos até aqui com um exemplo:
\begin{exemplo}
Considere $V = \mathbb{R}^4$ um $\mathbb{R}$-espaço vetorial. Sejam os vetores:
\[
\begin{array}{rcl}
v_1 &=& (1,0,0,0) \\
v_2 &=& (0,1,0,-1) \\
v_3 &=& (0,0,1,-1) \\
v_4 &=& (1,-1,0,0) \\
v_5 &=& (1,2,1,0) 
\end{array}
\]
Considere $I = \{ v_1, v_2 \}$ e $S  =\{ v_1,v_2,v_3,v_4,v_5 \}.$ Observe que $I$ é LI; de fato,
\[
\alpha_1v_1 + \alpha_2v_2 = 0 \Rightarrow \alpha_1(1,0,0,0) + \alpha_2 (0,1,0,-1) = 0 \Rightarrow \left\{ \begin{array}{rcl} \alpha_1 &=& 0 \\ \alpha_2 &=& 0 \\ - \alpha_2 &=& 0 \end{array} \right. \Rightarrow \alpha_1 = \alpha_2 = 0
\]
Ademais, tomando $v = (x,y,z,w) \in \mathbb{R}^4,$ temos que
\[
(x-z+w+y)v_1 + (z- w - \varepsilon)v_2 + (z - \varepsilon)v_3 + (z-w-y + \varepsilon)v_4 + \varepsilon_5 = v,
\]
para todo $\varepsilon \in \mathbb{R}.$ Logo, $S$ gera $V.$ 

\medskip
\noindent
Então, existe uma base $B$ de $\mathbb{R}^4$ tal que 
\[
\{ v_1, v_2 \} \subseteq B \subseteq \{v_1,v_2,v_3,v_4,v_5 \}
\]
De fato, esta base é $B=\{v_1, v_2, v_3, v_4 \},$ pois percebe-se que
\[
v_5 = \frac{5}{2}v_1 + \frac{1}{2} v_2 - \frac{1}{2}v_3 - \frac{3}{2} v_4
\]
\end{exemplo}

\noindent
Para trabalhar com a cardinalidade das bases, utilizaremos alguns fatos
conhecidos, enunciados na próxima proposição:
\begin{proposicao}
Se $\lambda$ e $\mu$ são cardinais, então:
\begin{itemize}
\item Se $\lambda\leq\mu$ e $\mu\leq\lambda$, então $\lambda=\mu$. (Teorema de Cantor-Bernstein)\index{Teorema de Cantor-Bernstein}
\item Se $\lambda$ e $\mu$ são infinitos, então \[\lambda+\mu=\lambda\mu=\max\{\lambda,\mu\}.\]
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço vetorial, então duas bases quaisquer têm o mesmo cardinal.
\end{teorema}
\begin{proof}
Sejam $B$ e $C$ bases de $V$.
\begin{itemize}
\item Se $B$ ou $C$ são finitos, então pela observação \ref{basefin2} podemos inferir que $B$ e $C$ são ambos finitos e assim aplicar o lema \ref{basefin1}.
\item Se $B$ e $C$ são infinitos. Para $u\in C$ existem um conjunto finito $I_u\subseteq B$ e uma função $\alpha_u:I_u\rightarrow K$ tais que $u=\sum_{i\in I_u}(\alpha_u)_ii$. Seja $I\subseteq\bigcup_{u\in C}\subseteq B$. Então $I$ gera $V$, assim $I=C$. Desse modo:
\[
\abs{B}=\abs{I}=\abs{\bigcup_{u\in C}I_u}\leq\sum_{u\in C}\abs{I_u}\leq\aleph_0\cdot\abs{C}=\abs{C},
\]
assim $\abs{B}\leq\abs{C}$. Analogamente $\abs{C}\leq\abs{B}$. Portanto $\abs{B}=\abs{C}$.
\end{itemize}
\end{proof}
\begin{definicao}\index{Espaço Vetorial!Dimensão}
Dizemos que a \textbf{dimensão} de um espaço vetorial é a cardinalidade de sua base.
\end{definicao}

\section{Subespaços}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Um \textbf{subespaço} de $V$ é um conjunto $W\subseteq V$ tal que:
\begin{itemize}
\item $0\in W$,
\item Para $x,y\in W$ então $x+y\in W$,
\item Para $a\in K$ e $x\in W$ então $ax\in W$.
\end{itemize}
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial e seja $\mathcal{W}$ um conjunto de subespaços. Então $\bigcap\mathcal{W}$ é um subespaço de $V$.
\end{proposicao}

\begin{definicao}
Se $S$ é subconjunto de $V$, definimos:
\[
\langle S\rangle=\left\{\sum\limits_{v\in I}\alpha_vv\mid I\subseteq S\text{ e }I\text{ é finito e }\alpha\in K^I\right\}
\]
e chamamos de \textbf{subespaço gerado} por $S$.
\end{definicao}

\begin{proposicao}
Se $S$ é subconjunto de $V$, então:
\[
\langle S\rangle=\bigcap\{W\mid W\text{ é subespaço de }V\text{ e }S\subseteq W\}.
\]
\end{proposicao}
\begin{proof}
Seja:
\[
T=\bigcap\{W\mid W\text{ é subespaço de }V\text{ e }S\subseteq W\}.
\]
Para $x\in \langle S\rangle$, então existem um conjunto finito $I\subseteq S$ e uma função $\alpha:I\rightarrow V$ tal que:
\[
x=\sum\limits_{v\in I}\alpha_vv,
\]
aí para todo subespaço $W$ tal que $S\subseteq W$, então para todo $v\in I$ temos $v\in S$, aí $v\in W$; aí por indução finita temos $x\in W$; logo $x\in T$. Portanto $\langle S\rangle\subseteq T$.

\medskip
\noindent
Além disso, temos o seguinte:
\begin{itemize}
\item $\emptyset\subseteq S$ e $\emptyset$ é finito e $\emptyset\in K^\emptyset$ e:
\[
0=\sum_{v\in\emptyset}\emptyset_vv,
\]
aí $0\in\langle S\rangle$.
\item Para $x,y\in\langle S\rangle$, então existem conjuntos finitos $I,J\subseteq S$ e funções $\alpha\in K^I$ e $\beta\in K^J$ tais que:
\[
x=\sum\limits_{u\in I}\alpha_uu,\quad y=\sum\limits_{v\in J}\beta_vv,
\]
aí sendo $L=I\cup J$ então $L\subseteq S$ e $L$ é finito, e também sendo $\tilde{\alpha},\tilde{\beta}:L\rightarrow K$ dadas por:
\[
\tilde{\alpha}_l=\left\{\begin{array}{cl}\alpha_l&\text{se }l\in I\\0&\text{se }l\notin I\end{array}\right.,\quad\tilde{\beta}_l=\left\{\begin{array}{cl}\beta_l&\text{se }l\in J\\0&\text{se }l\notin J\end{array}\right.,
\]
e sendo $\gamma:L\rightarrow K$ dada por $\gamma_l=\tilde{\alpha}_l+\tilde{\beta}_l$, então:
\[
x+y=\sum_{l\in L}\gamma_ll,
\]
aí $x+y\in\langle S\rangle$.
\item Para $a\in K$ e $x\in\langle S\rangle$, então existem conjunto finito $I\subseteq S$ e $\alpha\in K^I$ tais que:
\[
x=\sum_{v\in I}\alpha_vv,
\]
aí sendo $\beta:I\rightarrow K$ dada por $\beta_v=a\alpha_v$, então:
\[
ax=\sum_{v\in I}\beta_vv,
\]
aí $ax\in\langle S\rangle$.
\item Para $s\in S$, então $\{s\}\subseteq S$ e $\{s\}$ é finito, e considerando a função $\alpha:\{s\}\rightarrow K$ dada por $\alpha_s=1$, então:
\[
s=\sum_{v\in\{s\}}\alpha_vv,
\]
aí $S\subseteq\langle S\rangle$.
\end{itemize}
Logo $\langle S\rangle$ é um subespaço de $V$ tal que $S\subseteq\langle S\rangle$, aí $T\subseteq\langle S\rangle$.
\end{proof}

\noindent
A intersecção de subsespaços sempre é um subespaço, mas o mesmo não acontece com a união de subespaços.
\begin{proposicao}\label{unotsubsp}
Se $A$ e $B$ são subespaços de $V$ tais que $A\nsubseteq B$ e $B\nsubseteq A$, então $A\cup B$ não é subespaço de $V$.
\end{proposicao}
\begin{proof}
Nesse caso, existe $a\in A$ tal que $a\notin B$ e existe $b\in B$ tal que $b\notin A$. Seja $c=a+b$. Então:
\begin{itemize}
    \item Se $c \in A,$ $b = c - a \in A,$ o que é impossível.
    \item Se $c \in B,$ $a = c - b \in b,$ o que é impossível.
\end{itemize}
Logo, concluímos que $c \notin A \cup B,$ absurdo. \qedhere
\end{proof}

\noindent
Portanto concluímos que $A \cup B$ é um subespaço se e somente se $A \subseteq B$ ou $B \subseteq A.$

\begin{observacao}
Seja $K = F_2 = \{ 0, 1 \},$ e tome $V = K^2.$ Então,
\[
V = \langle (0,1) \rangle \cup \langle (1,0) \rangle \cup \langle (1,1) \rangle
\]
Na verdade, $V$ só pode ser escrito como união de um número finito de subespaços próprios se $K$ for um corpo finito, conforme a seguinte proposição.
\end{observacao}

\begin{proposicao}
Um espaço vetorial $V$ sobre um corpo infinito $K$ não pode ser escrito como união de um número finito de subespaços próprios.
\end{proposicao}
\begin{proof}
Suponhamos que $V=S_1\cup\dots\cup S_n$, em que podemos assumir que:
\[
S_1\nsubseteq S_2\cup\dots\cup S_n,
\]
Seja $w\in S_1 \setminus (S_2\cup\dots\cup S_n)$ e seja $v\notin S_1$. Considere o conjunto infinito:
\[
A=\{rw+v\mid r\in K\},
\]
que é a ``reta'' passando por $v$ e paralela a $w$. Queremos mostrar que cada $S_i$
contém no máximo um vetor do conjunto infinito $A$, o que será uma contradição ao fato de que
$V=S_1\cup\dots\cup S_n$. Isto provará o teorema.

\medskip
\noindent
Se $rw+v\in S_1$ para algum $r\neq 0$, então $w\in S_1$ implicará $v\in S_1$, contrário às hipóteses.
Agora, suponha que $r_1w+v\in S_1$ e $r_2w+v\in S_1$, para algum $i\geq 2$, em que $r_1\neq r_2$.
Então:
\[
(r_1-r_2)w=(r_1w+v)-(r_2w+v)\in S_i,
\]
aí $w\in S_i$, que também contradiz as hipóteses.
\end{proof}

\noindent
Apesar de não podermos trabalhar com a união, podemos realizar a soma de subespaços, e esta sim é um subespaço:

\begin{definicao}
Sejam $W_i \subseteq V$, $i \in I,$ subespaços de $V.$ Definimos:
\[
\sum\limits_{i \in I} W_i = \{ w_{i_1} + \ldots + w_{i_k} \mid k \in \mathbb{N}, w_i \in W_i \}.
\]
\end{definicao}

\noindent
Pode-se mostrar que o conjunto:
\[
\sum\limits_{i \in I} W_i
\]
é subespaço de $V$.

\begin{definicao}\index{Espaço Vetorial!Soma direta}
Uma soma:
\[
\sum\limits_{i \in I} W_i
\]
é dita uma \textbf{soma direta} se para todo $i \in I$ tivermos:
\[
W_i \cap \left( \sum\limits_{j \neq i} W_j \right) = 0.
\]
\end{definicao}

\begin{teorema}
Para subespaço $A$ de $V$, então existe subespaço $B\subseteq V$ tal que $V=A\oplus B$.
\end{teorema}
\begin{proof}
Seja $E$ uma base de $A$. Então existe uma base $G$ de $V$ tal que $E\subseteq G$, aí seja $F=G\setminus E$, e seja $B$ o subespaço gerado por $F$. Então é fácil ver que $V=A\oplus B$.
\end{proof}

\begin{teorema}
\[
\dim(A+B)+\dim(A\cap B)=\dim(A)+\dim(B).
\]
\end{teorema}
\begin{proof}
Seja $E$ base de $A\cap B$. Então existe $F$ tal que $B\cap F=\emptyset$ e $E\cup F$ seja base de $A$ e existe $G$ tal que $A\cap G=\emptyset$ e $E\cup G$ seja base de $B$. Então $E\cup F\cup G$ é base de $A+B$. Daí:
\[
\textcolor{Green}{\dim(A+B)} + \textcolor{Blue}{\dim(A \cap B)} = \textcolor{Green}{\abs{E} + \abs{F} + \abs{G}} + \textcolor{Blue}{\abs{E}} = \textcolor{Red}{\abs{E} + \abs{F}} + \textcolor{Laranja}{\abs{E} + \abs{G}} = \textcolor{Red}{\dim(A)} + \textcolor{Laranja}{\dim(B)}
\]
\end{proof}

\begin{exemplo}
Considere novamente $V = \mathbb{R}^4$. Sejam
\[
W_1 = \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \},
\]
\[
W_2 = \{(x,y,z,t) \in \mathbb{R}^4 | x +y = 0 \mbox{ e } z - 2t = 0 \}.
\]
Então $W_1$ e $W_2$ são subespaços de $V.$ Assim, $W_1 + W_2$ e $W_1 \cap W_2$ são subespaços de $V.$ Vamos encontrar bases para eles.

\medskip
\noindent
Note que:
\[
\begin{array}{lcl}
W_1 &=& \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \} \\
&=& \{(x,y,z,-y-z) \in \mathbb{R}^4 | x,y,z \in \mathbb{R} \} \\
&=& \{ (x,0,0,0) + (0,y,0-y) + (0,0,z,-z) : x,y,z \in \mathbb{R} \} \\
&=& \langle (1,0,0,0), (0,1,0-1), (0,0,1,-1) \rangle\\
\end{array}
\]
Verifica-se também que $(1,0,0,0), (0,1,0-1), (0,0,1,-1)$ são linearmente independentes. Logo, $B_1 = \{ (1,0,0,0), (0,1,0-1), (0,0,1,-1) \}$ é base para $W_1.$

\medskip
\noindent
Analogamente, mostra-se que $B_2 = \{ (1,-1,0,0), (0,0,2,1) \}$ é base para $W_2.$

\medskip
\noindent
Agora, para determinar uma base de $W_1 + W_2,$ podemos escalonar a matriz
\[
\left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & -1 \\
1 & -1 & 0 & 0\\
0 & 0 & 2 & 1

\end{array} \right) \rightarrow \cdots \rightarrow \left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0& 0& 0 & 1\\
0 & 0 & 0 & 0

\end{array} \right)
\]
Portanto, o conjunto
\[
\mathcal{B} = \{ (1,0,0,0), (0,1,0,-1),(0,0,1,-1),(1,-1,0,0) \}\]
é base de $W_1 + W_2.$

\medskip
\noindent
Para determinar uma base de $W_1 \cap W_2,$ basta resolver o sistema
\[
\left\{ \begin{array}{l}
y+z+t = 0 \\x+y = 0 \\z - 2t = 0
\end{array} \right.
\]
Assim, $W_1 \cap W_2 = \langle (3,-3,2,1) \rangle.$

\medskip
\noindent
Observe que
\[
\dim(W_1 \cap W_2) + \dim(W_1 + W_2) = 1 + 4 = 5 = 3 + 2 = \dim(W_1) + \dim(W_2).
\]
Como $\dim(W_1 + W_2) = 4,$ temos que $W_1 + W_2 = V = \mathbb{R}^4.$

\medskip
\noindent
Observe também que, como $\dim(W_1 \cap W_2) = 1,$ a soma $W_1 + W_2$ não é direta. 
\end{exemplo}

\section{Coordenadas}

\begin{definicao}
Seja $V$ um espaço vetorial de dimensão finita. Seja $B$ uma base de $V$. Então para $v\in V$ existe um único $\alpha:B\rightarrow K$ tal que \[v=\sum\limits_{b\in B}\alpha_bb,\] e chamamos esse $\alpha$ de $[v]_B$.
\end{definicao}

\chapter{Transformações Lineares}
\index{Espaço Vetorial!Transformações Lineares}

\section{Definições}

\begin{definicao}
Sejam \(U\) e \(V\) espaços vetoriais sobre um corpo \(K\). Uma
\textbf{transformação linear} é uma função $T:U\rightarrow V$ tal que  $$T(\alpha
u+\beta v)=\alpha T(u)+\beta T(v)$$
 para quaisquer $\alpha,\beta\in K$ e $u,v\in V$. Além disso, denotamos o conjunto das transformações lineares de $U$ a $V$ por $\mathcal{L}(U,V).$
\end{definicao}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, seja $B$ uma base de $U$ e $f:B\rightarrow V$ uma função. Então existe uma única transformação linear $T\in\mathcal{L}(U,V)$ tal que $\forall b\in B:T(b)=f(b)$.
\end{teorema}

\begin{definicao}
Seja $T\in\mathcal{L}(U,V)$. Definimos $\mathrm{Ker}(T)=\{u\in U:T(u)=0\}$. Definimos $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$.
\end{definicao}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$. Então:
\begin{itemize}
\item $\mathrm{Ker}(T)$ é um subespaço de $U$.
\item $\mathrm{Im}(T)$ é um subespaço de $V$.
\item $T$ é injetora se e só se $\mathrm{Ker}(T)=0$.
\item Se $T$ é bijetora, então $T^{-1}\in\mathcal{L}(V,U)$.
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $\mathcal{L}(U,V)$, seja $B$ uma base de $\mathrm{Ker}(T)$, e seja $C$ um conjunto tal que $T[C]$ seja base de $\mathrm{Im}(T)$. Então $B\cup C$ é base $V$.
\end{teorema}
\begin{proof}
Para $v\in V$ então $T(v)\in\mathrm{Im}(T)$, então existem um conjunto finito $F\subseteq C$ e $\alpha:F\rightarrow K$ tais que:
\[
T(v)=\sum\limits_{w\in F}\alpha_wT(w),
\]
assim:
\[
T\left(v-\sum\limits_{w\in F}\alpha_ww\right)=0,
\]
aí:
\[
v-\sum\limits_{w\in F}\alpha_ww\in\mathrm{Ker}(T),
\]
assim existem um conjunto finito $E\subseteq B$ e função $\beta:B\rightarrow K$ tais que:
\[
v-\sum\limits_{w\in F}\alpha_ww=\sum\limits_{u\in E}\beta_uu,
\]
aí:
\[
v=\sum\limits_{u\in E}\beta_uu+\sum\limits_{w\in F}\alpha_ww.
\]
Por outro lado, para subconjunto finito $E\subseteq B\cup C$ e função $\alpha:E\rightarrow K$ tais que:
\[
\sum_{e\in E}\alpha_ee=0,
\]
então:
\[
\sum_{e\in E\cap C}\alpha_eT(e)=0,
\]
aí:
\[
\forall e\in E\cap C:\alpha_e=0,
\]
aí:
\[
\sum_{e\in E\setminus C}\alpha_ee=0,
\]
aí:
\[
\forall e\in E\setminus C:\alpha_e=0,
\]
portanto:
\[
\forall e\in E:\alpha_e=0.
\]
\end{proof}

\begin{teorema}[Teorema do Núcleo-Imagem] \index{Espaço Vetorial!Teorema do Núcleo-Imagem}
Seja $T \in \mathcal{L}(U,V).$ Então
\[
U = \mathrm{Ker}(T) \oplus \mathrm{Im}(T)
\]

\end{teorema}
\begin{corolario}
\[
\dim V=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T)).
\]
\end{corolario}

\begin{definicao}\index{Transformações Lineares!Isomorfismos}
Se $T\in\mathcal{L}(U,V)$ é bijetora, dizemos que $T$ é um \textbf{isomorfismo} de $U$ a $V$.
\end{definicao}
\begin{proposicao}
$T \in \mathcal{L}(U,V)$ é isomorfismo se e somente se $T^{-1}$ também o é.
\end{proposicao}

\begin{proposicao}
Dois espaços vetoriais $U$ e $V$ são isomorfos se e somente se quaisquer duas bases $B$ de $U$ e $C$ de $V$ possuem a mesma cardinalidade.
\end{proposicao}
\begin{teorema}
Para espaços vetoriais $U$ e $V$, então $U$ é isomorfo a $V$ se e só se $\dim(U)=\dim(V)$.
\end{teorema}

\section{Espaço Dual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. Denotamos $V^*=\mathcal{L}(V,K)$. O espaço $V^*$ chama-se o \textbf{espaço dual} de $V$. Os elementos de $V$ chamam-se \textbf{funcionais lineares}.
\end{definicao}

\noindent
Se $\dim(V)=n$, então $\dim(V^*)=n\cdot 1=n$, aí $V$ e $V^*$ são isomorfos.

\begin{teorema}\label{basedual}
Seja $V$ um espaço vetorial com $\dim(V)=n$ e $B=(v_1,\dots,v_n)$ uma base de $V$. Então existe uma base $B^*=(f_1,\dots,f_n)$ de $V^*$ tal que $f_i(v_j)=\delta_{i,j}$ para quaisquer $i,j$. Além disso:
\[
\forall v\in V:v=\sum_{i=1}^nf_i(v)v_i
\]
e:
\[
\forall f\in V^*:f=\sum_{i=1}^nf(v_i)f_i.
\]
\end{teorema}
\begin{proof}
Para $i=1,\dots,n$, existe uma única função linear $f_i:V\rightarrow K$ tal que:
\[
f_i(v_j)=\left\{\begin{array}{rl}0,&i\neq j\\1,&i=j\end{array}\right.
\]
Sejam $\alpha_1,\dots,\alpha_n\in K$ tais que:
\[
\sum_{i=1}^n\alpha_if_i=0.
\]
Para $j=1,\dots,n$, aplicando este funcional para o vetor $v_j\in B$, então:
\[
0=0(v_j)=\sum_{i=1}^n\alpha_if_i(v_j)=\alpha_j,
\]
ou seja, $\alpha_j=0$. Portanto $B^*$ é linearmente independente.

\medskip
\noindent
Além disso, para $v\in V$ existem $\alpha_1,\dots,\alpha_n\in K$ tais que:
\[
v=\sum_{i=1}^n\alpha_iv_i,
\]
aí para $i=1,\dots,n$ temos:
\[
f_i(v)=\alpha_if_i(v_i)=\alpha_i;
\]
logo:
\[
f(v)=\sum_{i=1}^n\alpha_if(v_i)=\sum_{i=1}^nf(v_i)f_i(v). \qedhere
\] 
\end{proof}

\begin{definicao}\index{Espaço Vetorial!Base dual}
A base $B^*$ chama-se a \textbf{base dual} da base $B$.
\end{definicao}

\noindent
Podemos estender o estudo do espaço dual para espaços vetoriais quaisquer.

\begin{definicao}
Seja $B$ uma base de $V$, então para cada $a\in B$ definimos a transformação linear $f_a\in V^*$ por $f_a(b)=\delta_{a,b}$.
\end{definicao}

\noindent
Nesse caso, podemos adaptar facilmente o argumento na demonstração do teorema \ref{basedual} para mostrar que $(f_a)_{a\in B}$ é linearmente independente em $V^*$ e para todo $v\in V$ existe um conjunto finito $F\subseteq B$ tal que:
\[
v=\sum_{b\in F}f_b(v)b.
\]

\section{Espaço Bidual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. O espaço $V^{**}=(V^*)^*$ chama-se o \textbf{espaço bidual} do espaço $V$.
\end{definicao}

\begin{definicao}
Para $v\in V$, definamos $\varphi_v:V^*\rightarrow K$ assim:
\[
\forall f\in V^*:\varphi_v(f)=f(v).
\]
Então $\varphi_v\in V^{**}$.
\end{definicao}

\begin{proposicao}
$\varphi\in\mathcal{L}(V,V^{**})$ e $\varphi$ é injetora.
\end{proposicao}
\begin{proof}
Seja $B$ uma base de $V$. Para $v\in\mathrm{Ker}(\varphi)$, então $\varphi_v=0$, aí temos $\forall b\in B:f_b(v)=\varphi_v(f_b)=0$, aí existe um conjunto finito $F\subseteq B$ tal que:
\[
v=\sum_{b\in F}f_b(v)b,
\]
aí $v=0$.
\end{proof}

\begin{corolario}
Se $\dim(V)$ é finita, então $\varphi:V\rightarrow V^{**}$ é um isomorfismo.
\end{corolario}
\begin{proof}
\[
\dim(V)=\dim(V^*)=\dim(V^{**}). \qedhere
\]
\end{proof}

\begin{observacao}
Nesse caso $\varphi$ é um isomorfismo natural, ou seja, não depende da escolha de uma base.
\end{observacao}

\begin{corolario}
Se $\dim(V)$ é finita, então toda base de $V^*$ é a base dual para uma base de $V$.
\end{corolario}
\begin{proof}
Seja $C=(f_1,\dots,f_n)$ uma base de $V^*$. Consideremos a base dual $C^*=(g_1,\dots,g_n)$ de $V^{**}$. Mas $\varphi$ é sobrejetora, então existem $v_1,\dots,v_n\in V$ tais que para todo $i$ tenhamos $g_i=\varphi_{v_i}$, assim:
\[
f_i(v_j)=\varphi_{v_j}(f_i)=g_j(f_i)=\delta_{j,i}=\delta_{i,j},
\]
logo $C=(f_1,\dots,f_n)$ é base dual da base $(v_1,\dots,v_n)$ de $V$.
\end{proof}

\section{Anuladores}

\begin{definicao}
Seja $V$ um espaço vetorial e seja $S\subseteq V$ um subconjunto. Então definimos:
\[
S^0=\{f\in V^*\mid\forall s\in S:f(s)=0\}.
\]
O conjunto $S^0$ chama-se o \textbf{anulador} de $S$.
\end{definicao}

\begin{proposicao}
$S^0$ é um subespaço de $V$.
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço de dimensão finita e $W\subseteq V$ um subespaço. Então:
\[
\dim(V)=\dim(W)+\dim(V^0).
\]
\end{teorema}
\begin{proof}
Seja $\dim(V)=n$ e $\dim(W)=m$. Escolhemos uma base $(v_1,\dots,v_m)$ de $W$ e completemo-la até uma base $(v_1,\dots,v_m,v_{m+1},\dots v_n)$ de $V$. Consideremos a base dual $(f_1,\dots,f_n)$ de $V^*$. Mostraremos que $(f_{m+1},\dots,f_n)$ é uma base de $W^0$. É claro que para todo $i=m+1,\dots,n$ temos $f_i\in W^0$. Seja $f\in W^0$, então:
\[
f=\sum_{i=1}^nf(v_i)f_i=\sum_{i=m+1}^nf(v_i)f_i. \qedhere
\]
\end{proof}

\begin{teorema}
Se $\dim(V)$ é finita e $V=U\oplus W$, então $V^*=U^0\oplus W^0$ e $U^0\cong W^*$ e $W_0\cong U^*$.
\end{teorema}
\begin{proof}
Seja $B=B_U\cup B_W$ base de $V$, em que $B_U$ é base de $U$ e $B_W$ é base de $W$. Então a base dual é $B^*=B_U^*\cup B_V^*$, e pelo teorema anterior temos $\langle B_U^*\rangle=W^0$ e $\langle B_V^*\rangle=U^0$.
\end{proof}

\section{Transpostas}

\begin{definicao}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e $T\in\mathcal{L}(U,V)$. Então definimos a \textbf{transposta} de $T$ como a função:
\[
\begin{array}{rcl}
T^t:V^t&\rightarrow&U^t\\f&\mapsto&T^t(f)=f\circ T
\end{array}
\]
\end{definicao}

\begin{proposicao}
Se $\dim(U)$ é finita e $T\in\mathcal{L}(U,V)$, então:
\begin{itemize}
\item[a)] $\mathrm{Ker}(T^t)=(\mathrm{Im}(T))^0$.
\item[b)] $\mathrm{Rank}(T^t)=\mathrm{Rank}(T)$.
\item[c)] $\mathrm{Im}(T^t)=(\mathrm{Ker}(T))^0$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[a)] Temos:
\[
\begin{array}{rcl}
\mathrm{Ker}(T^t)&=&\{f\in V^*\mid T^t(f)=0\}\\&=&\{f\in V^*\mid f\circ T=0\}\\&=&\{f\in V^*\mid \forall u\in U:f(T(u))=0\}\\&=&\{f\in V^*\mid f[\mathrm{Im}(T)]=0\}\\&=&(\mathrm{Im}(T))^0.
\end{array}
\]
\item[b)] Temos $\mathrm{Rank}(T^t)=\dim(\mathrm{Im}(T^t))$ e $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$. Além disso:
\[
\dim(V^*)=\dim(\mathrm{Im}(T^t))+\dim(\mathrm{Ker}(T^t))
\]
\[
\dim(V^*)=\dim(\mathrm{Im}(T))+\dim(\mathrm{Im}(T))^0
\]
mas $\dim(V^*)=\dim(V)$ e $\dim(\mathrm{Ker}(T^t))+\dim(\mathrm{Im}(T))^0$.
\item[c)] Temos $\mathrm{Im}(T^t)\subseteq(\mathrm{Ker}(T))^0$. Seja $\varphi\in\mathrm{Im}(T^t)$, então existe $g\in V^*$ tal que $\varphi=T^t(g)$, aí para todo $u\in U$ nós temos $\varphi(u)=T^t(g)(u)=g(T(u))$. Se $u\in\mathrm{Ker}(T)$ então $T(u)=0$, aí $\varphi(u)=0$; logo $\varphi\in(\mathrm{Ker}(T))^0$. Além disso:
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Ker}(T))^0
\]
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T))
\]
aí $\dim(\mathrm{Ker}(T))^0=\dim(\mathrm{Im}(T))$, aí $(\mathrm{Ker}(T))^0=\mathrm{Im}(T)$. \qedhere
\end{itemize}
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais de dimensão finita com bases $B$ e $C$ e bases duais $B^*$ e $C^*$. Se $T\in\mathcal{L}(U,V)$, então:
\[
[T]_{B,C}^t=[T^t]_{C^*,B^*}
\]
\end{teorema}

\begin{corolario}
Se $A\in M_{m,n}(K)$, então:
\[
\mathrm{Row Rank}(A)=\mathrm{Column Rank}(A).
\]
\end{corolario}
\begin{proof}
Consideremos $T:K^n\rightarrow K^m$ dada por $T(v)=Av$. Sejam $B$ e $C$ as bases canônicas de $K^n$ e $K^m$, então $[T]_{B,C}=A$. Temos:
\[
\begin{array}{ccccc}
\mathrm{Rank}(T)&=&\mathrm{Column Rank}(A)&&\\
\mathrm{Rank}(T^t)&=&\mathrm{Column Rank}(A^t)&=&\mathrm{Row Rank}(A).
\end{array} \qedhere
\]
\end{proof}

\section{Espaços Quocientes}

\begin{definicao}
Seja $V$ um espaço, $W\subseteq V$ um subespaço. Para $u,v\in V$, digamos que $u\sim v$ se e só se $u-v\in W$. Então $\sim$ é uma relação de equivalência, ou seja:
\begin{itemize}
\item Reflexiva, ou seja, $v\sim v$ sempre.
\item Simétrica, ou seja, se $v\sim u$ então $u\sim v$.
\item Transitiva, ou seja, se $v\sim u$ e $u\sim w$, então $v\sim w$.
\end{itemize}
Seja $V/W$ o conjunto das classes de equivalência relativamente a $\sim$. Para $v\in V$ seja $\overline{v}$ a classe de equivalência de $v$.
\begin{itemize}
\item Definamos em $V/W$ uma estrutura de espaço vetorial. Para $\overline{v},\overline{w}\in V/W$ definamos $\overline{v}+\overline{w}=\overline{v+w}$.
\item Para $\alpha\in K$ e $\overline{v}\in V$ definamos $\alpha\cdot\overline{v}=\overline{\alpha v}$. Então $V/W$ é um espaço vetorial chamado \textbf{espaço quociente}.
\end{itemize}
\end{definicao}

\begin{observacao}
As operações estão ``bem definidas'' pois:
\begin{itemize}
\item Se $\overline{v}=\overline{v'}$ e $\overline{u}=\overline{u'}$, então $v\sim v'$ e $u\sim u'$, aí $v-v',u-u'\in W$, aí $(v+u)-(v'+u')=(v-v')+(u-u')\in W$, aí $\overline{v+u}=\overline{v'+u'}$, aí $\overline{v}+\overline{u}=\overline{v'}+\overline{u'}$.
\item Analogamente para a outra propriedade.
\end{itemize}
Também verificaremos algumas propriedades, deixando o resto ao leitor.
\begin{itemize}
\item Temos a comutatividade da adição, pois $\overline{u}+\overline{v}=\overline{v}+\overline{u}$ equivale a $\overline{u+v}=\overline{v+u}$, que é verdade pois $u+v=v+u$.
\item O que é o $\overline{0}$ de $V/W$? Temos $\overline{0}=W$, e também para todo $w\in W$ temos $w\sim 0$, aí $\overline{w}=\overline{0}=W$.
\end{itemize}
Também temos o seguinte:
\begin{itemize}
\item Se $W=V$, então $V/V=\{\overline{0}\}$.
\item Se $W=\{0\}$, então $V/\{0\}\cong V$.
\end{itemize}
\end{observacao}

\begin{proposicao}
Consideremos a aplicação:
\[
\pi:V\rightarrow V/W,\quad\quad v\mapsto\overline{v}.
\]
Então $\pi\in\mathcal{L}(V,V/W)$, com $\mathrm{Ker}(\pi)=W$.
\end{proposicao}

\begin{notacao}
$\pi$ chama-se a \textbf{projeção canônica} de $V$ para $V/W$.
\end{notacao}

\begin{proof}
Temos o seguinte:
\begin{itemize}
\item $\pi(v+u)=\overline{v+u}=\overline{v}+\overline{u}=\pi(v)+\pi(u)$.
\item $\pi(\alpha v)=\overline{\alpha v}=\alpha\overline{v}=\alpha\pi(v)$.
\end{itemize}
Além disso, se $w\in W$ então $\pi(w)=\overline{w}=W$
\end{proof}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$ e $W\subseteq U$ tal que $W\subseteq\mathrm{Ker}(T)$. Então existe um único $\overline{T}\in\mathcal{L}(U/W,V)$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
\end{proposicao}
\begin{proof}
Temos o seguinte:

\medskip
\noindent
1) Mostraremos que $\overline{T}$ está ``bem definida''. Se $\overline{u}=\overline{v}$, então $u-v\in W\subseteq\mathrm{Ker}(T)$, aí $T(u-v)=0$, aí $T(u)=T(v)$.

\medskip
\noindent
2) Mostraremos que $\overline{T}$ é uma transformação linear.

\begin{itemize}
\item $\overline{T}(\overline{u}+\overline{v})=\overline{T}(\overline{u+v})=T(u+v)=T(u)+T(v)=\overline{T}(\overline{u})+\overline{T}(\overline{v})$.
\item $\overline{T}(\alpha\overline{v})=\overline{T}(\overline{\alpha v})=T(\alpha v)=\alpha T(v)=\alpha\overline{T}(\overline{v})$.
\end{itemize}
Agora, para todo $T'\in\mathcal{L}(U/W,V)$ tal que para todo $u\in U$ tenhamos:
\[
T'(\overline{u})=T(u),
\]
então para todo $v\in U/W$ existe um $u\in U$ tal que $v=\overline{u}$, aí:
\[
T'(v)=T'(\overline{u})=T(u)=\overline{T}(\overline{u})=\overline{T}(v);
\]
logo $T'=\overline{T}$.
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e seja $T\in\mathcal{L}(U,V)$. Então $U/\mathrm{Ker}(T)\cong\mathrm{Im}(T)$.
\end{teorema}
\begin{proof}
Pela proposição anterior, existe uma única $\overline{T}:U/\mathrm{Ker}(T)\rightarrow V$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
Observemos que $\mathrm{Im}(\overline{T})=\mathrm{Im}(T)=\{T(u)\mid u\in U\}$.

\medskip
\noindent
Além disso, para $\overline{u}\in\mathrm{Ker}(\overline{T})$, então $T(u)=\overline{T}(\overline{u})=0$, aí $u\in\mathrm{Ker}(T)$, aí $\overline{u}=\overline{0}$, de modo que $\overline{T}$ é injetora.
\end{proof}

\begin{teorema}
Seja $W$ subespaço de $V$. Então todos os complementos de $W$ em $V$ são isomorfos ao $V/W$.
\end{teorema}
\begin{proof}
Seja $V=W\oplus U$. Consideremos a projeção canônica:
\[
\pi:V\rightarrow V/W.
\]
Seja $\overline{\pi}=\pi\upharpoonright U$. Então $\mathrm{Ker}(\overline{\pi})=U\cap\mathrm{Ker}(\pi)=U\cap W=\{0\}$. Logo $\overline{\pi}$ é injetora.

\medskip
\noindent
Para $\overline{v}\in V/W$, seja $v=w+u$, com $w\in W$ e $u\in U$. Então $\pi(v)=\pi(w)+\pi(u)=\pi(u)=\overline{\pi}(u)$, aí $\overline{v}=\overline{\pi}(u)$, assim $\overline{\pi}$ é sobre $V/W$.
\end{proof}

\begin{corolario}
Seja $W\subseteq V$ um subespaço. Então $\dim V=\dim W+\dim V/W$.
\end{corolario}
\begin{proof}
Seja $V=W\oplus U$, então $\dim V=\dim W+\dim U$, mas $U\cong V/W$, aí $\dim U=\dim V/W$.
\end{proof}

\begin{observacao}
Existem espaços vetoriais $W$ e $U$ e $W'$ e $U'$ tais que $W\oplus U\cong W'\oplus U'$ e $W\cong W'$, mas $U\ncong U'$. De fato podemos tomar:
\[
W=\bigoplus_{i=0}^\infty Ke_{2i},\quad U=\bigoplus_{i=0}^\infty Ke_{2i+1},\quad W'=\bigoplus_{i=0}^\infty Ke_i,\quad U'=\{0\}.
\]
\end{observacao}

\chapter{Determinantes}

\section{Formas Multilineares}

\begin{definicao}
Seja $V$ um espaço vetorial e $V^r=V\times\dots\times V$. Uma \textbf{forma $r$-linear} sobre $V$ é uma função $F:V^r\rightarrow K$ que é linear em cada argumento, ou seja, para cada $i=1,\dots,r$ temos:
\[
F(v_1,\dots,\alpha v_i+\beta v'_i,\dots,v_r)=\alpha F(v_1,\dots,v_i,\dots,v_r)+\beta F(v_1,\dots,v'_i,\dots,v_r).
\]
Denotamos por $L_r(V)$ o conjunto das formas $r$-lineares sobre $V$.
\end{definicao}

\begin{exemplo}
Seja $V=K^2$ e:
\[
F((x_1,y_1),(x_2,y_2),(x_3,y_3))=x_1y_2x_3-x_1x_2x_3.
\]
Então $F$ é uma forma 3-linear.
\end{exemplo}

\begin{definicao}
Uma forma $F\in L_r(V)$ chama-se \textbf{alternada} se e só se para $(v_1,\dots,v_r)\in V^r$ e $i<j$ tais que $v_i=v_j$ então $F(v_1,\dots,v_r)=0$. Denotamos por $A_r(V)$ o conjunto das formas $r$-lineares alternadas.
\end{definicao}

\begin{definicao}
Uma forma $F$ é chamada \textbf{antissimétrica} se para $v\in V^r$ e para $i<j$ temos:
\[
F(v_1,\dots,v_i,\dots,v_j,\dots,v_r)=-F(v_1,\dots,v_j,\dots,v_i,\dots,v_r).
\]
\end{definicao}

\begin{proposicao}
Toda forma alternada é antissimétrica.
\end{proposicao}
\begin{proof}
Seja $F\in A_r(V)$. Sejam $v\in V^r$ e $i<j$, então:
\[
\begin{array}{rcl}
0&=&F(v_1,\dots,v_i+v_j,\dots,v_i+v_j,\dots,v_r)\\&=&F(v_1,\dots,v_i,\dots,v_i,\dots,v_r)+F(v_1,\dots,v_i,\dots,v_j,\dots,v_r)\\&+&F(v_1,\dots,v_j,\dots,v_i,\dots,v_r)+F(v_1,\dots,v_j,\dots,v_j,\dots,v_r)\\&=&F(v_1,\dots,v_i,\dots,v_j,\dots,v_r)+F(v_1,\dots,v_j,\dots,v_i,\dots,v_r)
\end{array}
\]
\end{proof}

\begin{proposicao}
Se a característica do corpo é $\neq 2$, então toda forma antissimétrica é reflexiva.
\end{proposicao}
\begin{proof}
Para $F$ antissimétrica e $v\in V^r$ e $i<j$, se $v_i=v_j$, sendo $v=v_i$, então:
\[
F(v_1,\dots,v,\dots,v,\dots,v_1)=-F(v_1,\dots,v,\dots,v,\dots,v_r),
\]
aí:
\[
2F(v_1,\dots,v,\dots,v,\dots,v_r)=0,
\]
aí:
\[
F(v_1,\dots,v,\dots,v,\dots,v_r)=0.
\]
\end{proof}

\begin{definicao}
Seja $F\in L_r(V)$ e $\sigma\in S_r$ uma permutação. Para $(v_1,\dots,v_r)\in V^r$ definimos:
\[
(\sigma F)\left(v_1,\dots,v_r\right)=F\left(v_{\sigma(1)},\dots,v_{\sigma(r)}\right).
\]
É fácil ver que $\sigma F\in L_r(V)$.
\end{definicao}

\begin{observacao}
Para $F\in L_r(V)$, então $F$ é antissimétrica se e somente se para toda transposição $\tau\in S_r$ tivermos $\tau F=-F$.
\end{observacao}

\begin{proposicao}
Seja $F\in L_r(V)$ uma forma antissimétrica. Então para $\sigma\in S_r$, temos:
\[
\sigma F=(\sgn \ \sigma)F.
\]
\end{proposicao}
\begin{proof}
Para $\sigma\in S_r$, então $\sigma$ pode ser escrita como um produto de transposições:
\[
\sigma=\tau_1\dots\tau_k,
\]
aí $\sigma$ é par se e só se $k$ é par. Portanto:
\[
\sigma F=(\tau_1\dots\tau_k)F=(-1)^k F=(\sgn \ \sigma)F,
\]
pois $\sgn \ \sigma=(-1)^k$.
\end{proof}

\begin{proposicao}
Toda forma $r$-linear determina uma forma $r$-linear alternada da seguinte maneira:
\[
F\mapsto\varphi(F)=\sum_{\sigma\in S_r}(\sgn \ \sigma)(\sigma F).
\]
\end{proposicao}
\begin{proof}
Seja $v_i=v_j=v$ com $i<j$. Precisamos provar que $\varphi(F)(v)=0$. Seja $\tau$ a transposição $(i,j)$, então $ S_r=A_r\cup A_r\tau$ e $A_r\cap A_r\tau=\emptyset$. Então temos o seguinte:
\[
\begin{array}{rcl}
\varphi(F)(v)&=&\sum_{\sigma\in S_r}(\sgn \ \sigma)(\sigma F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma\tau F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma F(v))\\&=&0.
\end{array}
\]
\end{proof}

\begin{observacao}
Se $F\in A_r(V)$ e $v\in V^r$ é linearmente dependente, então:
\[
F(v)=0.
\]
\end{observacao}

\begin{lema}
Seja $\dim V=n$ e $F\in A_n(V)$. Seja $(e_1,\dots,e_n)$ uma base de $V$, então $F$ é completamente determinada pelo valor $F(e)$.
\end{lema}
\begin{proof}
Seja $(v_1,\dots,v_n)\in V^n$. Então existe $(\alpha_{i,j})\in M_n(K)$ tal que:
\[
v_i=\sum_{j=1}^n\alpha_{i,j}e_j.
\]
Assim:
\[
\begin{array}{rcl}
F(v_1,\dots,v_n)&=&F\left(\sum\limits_{j_1=1}^n\alpha_{1,j_1}e_{j_1},\dots,\sum\limits_{j_n=1}^n\alpha_{n,j_n}e_{j_n}\right)\\&=&\sum\limits_{j_1,\dots,j_n=1}^n\alpha_{1,j_1}\dots\alpha_{n,j_n}F\left(e_{j_1},\dots,e_{j_n}\right)\\&=&\sum\limits_{\sigma\in S_n}\alpha_{1,\sigma_1}\dots\alpha_{n,\sigma_n}F\left(e_{\sigma_1},\dots,e_{\sigma_n}\right)\\&=&\textcolor{red}{\left(\sum\limits_{\sigma\in S_n}\alpha_{1,\sigma_1}\dots\alpha_{n,\sigma_n} \sgn \ \sigma \right)} F(e_1,\dots,e_n).
\end{array}
\]
\end{proof}

\noindent
Note então que o valor
\[
\sum\limits_{\sigma\in S_n}\alpha_{1,\sigma_1}\dots\alpha_{n,\sigma_n} \sgn \ \sigma 
\]
\emph{determina} $F$ para qualquer $v \in V^n.$ Chamaremos este valor de \textbf{determinante} de $F.$

\begin{exemplo}

\end{exemplo}

\section{Determinantes}

Seja $K$ um corpo e consideremos o anel das matrizes $M_n(K)$. Identificaremos os elementos de $M_n(K)$ com os elementos de $(K^n)^n$ assim:
\[
\begin{pmatrix}
a_{1,1}&&a_{1,n}\\&\ddots&\\a_{n,1}&&a_{n,n}
\end{pmatrix} \longleftrightarrow
\left((a_{1,1},\dots,a_{1,n}),\dots,(a_{n,1},\dots,a_{n,n})\right).
\]
Portanto, uma função $n$-linear aqui é uma função $n$-linear nas linhas da matriz.

\begin{definicao}
Uma função $\det:M_n(K)\rightarrow K$ é dita uma função \textbf{determinante} se e só se $\det$ é $n$-linear alternada e $\det(I)=1$.
\end{definicao}

\noindent
Pelo que vimos, existe e é única a função determinante: É a forma $n$-linear alternada que vale $1$ na base canônica de $K^n$.

\medskip
\noindent
Logo, se $A=(a_{i,j})\in M_n(K)$, então:
\[
\det(A)=\sum_{\sigma\in S_n}\sgn(\sigma)a_{1,\sigma_1}\dots a_{n,\sigma_n}.
\]

\begin{exemplo}
Para $n=2$, temos $S_2=\{I,(1,2)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{1,1}&a_{1,2}\\a_{2,1}&a_{2,2}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{1,1}a_{2,2}-a_{1,2}a_{2,1}.
\]
\end{exemplo}

\begin{exemplo}
Agora, se $n=3$, então $S_3=\{I,(1,2,3),(1,3,2),(1,2),(1,3),(2,3)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{1,1}&a_{1,2}&a_{1,3}\\a_{2,1}&a_{2,2}&a_{2,3}\\a_{3,1}&a_{3,2}&a_{3,3}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3}a_{2,2}a_{3,1}-a_{1,1}a_{2,3}a_{3,2}.
\]
\end{exemplo}

\begin{proposicao}
Temos as seguintes propriedades:
\begin{itemize}
\item[1)] Para todo $A\in M_n(K)$ temos $\det(A)=\det(A^t)$.
\item[2)] Para $A,B\in M_n(K)$ vale $\det(AB)=\det(A)\det(B)$.
\item[3)] Para $A\in M_n(K)$, então $A$ é inversível se e só se $\det(A)\neq 0$. Neste caso, temos $\det(A^{-1})=(\det(A))^{-1}$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Sendo $A=(a_{i,j})\in M_n(K)$, então temos:
\[
\begin{array}{rccll}
\det(A)&=&\sum\limits_{\sigma\in S_n}&\sgn(\sigma)&a_{1,\sigma_1}\dots a_{n,\sigma_n}\\&=&\sum\limits_{\sigma\in S_n}&\sgn(\sigma)&a_{\sigma^{-1}_1,1}\dots a_{\sigma^{-1}_n,n}\\&=&\sum\limits_{\tau\in S_n}&\sgn(\tau^{-1})&a_{\tau_1,1}\dots a_{\tau_n,n}\\&=&\sum\limits_{\tau\in S_n}&\sgn(\tau)&a^t_{1,\tau_1}\dots,a^t_{n,\tau_n}\\&=&\det(A^t).&&
\end{array}
\]
\item[2)] Seja $F_A:M_n(K)\rightarrow K$ tal que $\forall X\in M_n(K):F_A(X)=\det(AX)$. Então a função $F_A$ é uma função $n$-linear alternada sobre as colunas, mas também $F_A(I)=\det(A)$, aí $F_A(B)=\det(A)\det(B)$, assim $\det(AB)=\det(A)\det(B)$
\item[3)] Se $A$ é inversível, então existe a inversa $A^{-1}$, assim:
\[
1=\det(I)=\det(AA^{-1})=\det(A)\det(A)^{-1},
\]
aí $\det(A)\neq 0$ e $\det(A^{-1})=(\det(A))^{-1}$. Por outro lado, se $\det(A)\neq 0$, então $\det(A^t)\neq 0$, aí as colunas de $A$ são linearmente independentes, aí consideremos $T:K^n\rightarrow K^n$ tal que $[T]_{\mathrm{can}}=A$, então $T$ é inversível, assim $A=[T]_{\mathrm{can}}$ é inversível.
\end{itemize}
\end{proof}

\noindent
Assim lembremo-nos do seguinte: a função $\det$ é uma função $n$-linear e alternada nas linhas (ou nas colunas) da matriz, logo:
\begin{itemize}
\item[1)] Trocar duas linhas (ou colunas) da matriz muda o sinal do determinante.
\item[2)] Somar a uma linha (ou coluna) uma combinação linear das demais linhas (colunas) não altera o valor do determinante.
\item[3)] Ao multiplicar uma linha (ou coluna) por um escalar, o determinante fica multiplicado por esse escalar.
\end{itemize}

\begin{proposicao}
Temos o seguinte:
\begin{itemize}
\item[1)] O determinante de uma matriz triangular é o produto dos elementos da diagonal da matriz.
\item[2)] Se:
\[
A=\begin{pmatrix}
B&0\\C&D
\end{pmatrix}
\]
em que $B\in M_r(K)$ e $D\in M_{n-r}(K)$ e $C\in M_{n-r,r}(K)$ e $0\in M_{r,n-r}(K)$, então:
\[
\det(A)=\det(B)\det(D).
\]
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Seja $A=(a_{i,j})\in M_n(K)$ uma matriz triangular inferior, então para $i<j$ temos $a_{i,j}=0$, mas a única permutação $\sigma\in S_n$ tal que para todo $i=1,\dots,n$ tenhamos $i\geq\sigma_i$ é a permutação identidade $I$, assim temos:
\[
\begin{array}{rcl}
\det(A)&=&\sum_{\sigma\in S_n}\sgn(\sigma)a_{1,\sigma_1}\dots a_{n,\sigma_n}\\
&=&\sgn(I)a_{1,I_1}\dots a_{n,I_n}\\
&=&a_{1,1}\dots a_{n,n}.
\end{array}
\]
\item[2)] Seja $F:M_r(K)\rightarrow K$ tal que:
\[
F(X)=\det\begin{pmatrix}
X&0\\C&D
\end{pmatrix}.
\]
Então $F$ é $r$-linear alternada nas linhas de $X$, assim $F(X)=F(I)\det(X)$.

\medskip
\noindent
Agora consideremos $G:M_{n-r}(K)\rightarrow K$ tal que:
\[
G(Y)=\det\begin{pmatrix}
I&0\\C&Y
\end{pmatrix}
\]
Então $G$ é $(n-r)$-linear alternada nas colunas de $Y$, logo $G(Y)=G(I)\det(Y)$. Mas:
\[
G(I)=\det\begin{pmatrix}
I&0\\C&I
\end{pmatrix}=1,
\]
assim $G(Y)=\det(Y)$, aí $F(I)=G(D)=\det(D)$, assim $F(X)=F(I)\det(X)=\det(X)\det(D)$, aí acaba.
\end{itemize}
\end{proof}

\noindent
Agora temos a \textbf{regra de Laplace}:

\begin{teorema}
Dada $A=(a_{i,j})\in M_n(K)$, indicaremos por $M_{i,j}$ a matriz quadrada de tamanho $n-1$ obtida a partir de $A$ eliminando a linha $i$ e a coluna $j$.

\smallskip
\noindent
Para cada $i=1,\dots,n$, então vale:
\[
\det(A)=\sum_{j=1}^n(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
Para cada $j=1,\dots,n$, então vale:
\[
\det(A)=\sum_{i=1}^n(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
\end{teorema}
\begin{proof}
Provaremos a primeira afirmação pois a segunda é análoga.
\end{proof}

\medskip
\noindent
\textcolor{red}{AULA DE 19 DE AGOSTO (COLOCAREI ASSIM QUE CONSEGUIR)}

\medskip
\noindent
\textcolor{red}{FICOU FALTANDO A PROVA DA REGRA DE LAPLACE E A PARTE DE MATRIZES SOBRE ANEIS COMUTATIVOS}

\medskip
\noindent
Bláa blá blá

\chapter{Formas Canônicas}

\section{Espectro de um Operador}

\subsection{Autovalores e Autovetores}

\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\).
\begin{itemize}
\item Para $\lambda\in K$, dizemos que $\lambda$ é um \textbf{autovalor} de $T$ se existe um $v\neq 0$ tal que $T(v)=\lambda v$.\index{Autovalor}
\item Para $\lambda\in K$, um \textbf{autovetor} associado a $\lambda$ é um $v\in V$ tal que $T(v)=\lambda v$.\index{Autovetor}
\item Para $\lambda\in K$, chamamos de \textbf{autoespaço} associado a $\lambda$ o conjunto $V_T(\lambda)$ dos autovetores associados a $\lambda$.\index{Autoespaço}
\end{itemize}
\end{definicao}

\begin{exemplo}
Seja \(V=\mathbb{C}^1(\mathbb{R})\) e considere o operador linear \(T\in\mathcal{L}(V)\) tal que \(T(v)=v^\prime\) para cada \(v\in V\). Considere \(v=e^{\lambda x}\) com \(\lambda\in K\). Então \(T(v)=\lambda e^{\lambda x}=\lambda v\). Ou seja \(v\) é um autovetor associado a \(\lambda\).
\end{exemplo}

\begin{definicao}\index{Spectrum}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \textbf{spectrum} do operador \(T\) é o conjunto:
\[\text{Spec}(T)\coloneqq\{\lambda\in K\,\colon \lambda \text{ é autovalor de } T\}.\]
\end{definicao}

\noindent
No contexto da definição anterior, considere
\(\lambda\in\text{Spec}(T)\). Então
\begin{align*}
v\in V_T(\lambda)&\iff T(v)=\lambda v\\&\iff (T-\lambda I)(v)=0\\&\iff v\in\text{Ker}(T-\lambda I).
\end{align*}

\noindent
Ainda no mesmo contexto, vamos assumir agora que \(\text{dim}(V)=n<\infty\). Então temos 
que \[\lambda\in\text{Spec}(T)\implies\text{Ker}(T-\lambda I)\not =\{0\}\implies\text{det}(T-\lambda I)=0.\]
Reciprocamente, se \(\text{det}(T-\lambda I)=0\) então \(V_T(\lambda)=\text{Ker}(T-\lambda I)\not=\{0\}\).

\subsection{Polinômio Característico}

\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \textbf{polinômio característico} de \(T\) é o polinômio:
\[p_T(t)\coloneqq\text{det}(tI-T).\]
\end{definicao}

\noindent
Note que \(\lambda\in\text{Spec}(T)\) se e só se \(\lambda\) é raiz de \(p_T(\lambda)\). Além disso, note que se  \(B\) e \(B^\prime\) são bases \(V\), então \(p_T(\lambda)=p_{[T]_B}(\lambda)\). De fato, se \(P\) é a matriz de mudança da base \(B\) para a base \(B^\prime\), então
\begin{align*}
[\lambda I - T]_{B^\prime}&=P^{-1}[\lambda I - T]_{B}P
\end{align*}
Isso implica que \[\text{det}([\lambda I- T]_{B^\prime})=\text{det}(P^{-1})\text{det}([\lambda I - T]_B)\text{det}(P).\]
Ou seja,
\[\text{det}([\lambda I- T]_{B^\prime})=\text{det}([\lambda I - T]_B).\]
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^2)\) tal que \[[T]_{\text{can}}=\begin{pmatrix}
0 & -1\\1 & 0
\end{pmatrix}.\]
Isto é, \(T(x,y)=(-y,x)\) para cada \((x,y)\in\mathbb{R}^2\). Então 
\begin{align*}
p_T(x)&=\text{det}\begin{pmatrix}
x & 1\\-1 & x
\end{pmatrix}\\&=x^2+1.
\end{align*}
Dessa forma, \(\text{Spec}(T)=\varnothing\) pois \(p_T(x)\) não possui raízes em \(K=\mathbb{R}\).
\end{exemplo}

\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que\[[T]_{\text{can}}=\begin{pmatrix}
3 & 1 & -1\\2 & 2 & -1\\ 2 & 2 & 0
\end{pmatrix}.\]
Então:
\begin{align*}
p_T(x)&=\text{det}\begin{pmatrix}
x-3 & -1 & 1 \\ -2 & x-2 & 1 \\ -2 & -2 & x
\end{pmatrix}\\&=(x-1)^2(x-2).
\end{align*}
Isso implica que \(\text{Spec}(T)=\{1,2\}\). Além disso, temos que
\[V_T(1)=\text{Ker}(T-I)=\text{Ker}\begin{pmatrix}
2 & 1 & -1\\2 & 1 & -1\\ 2 & 2 & -1
\end{pmatrix}=\langle (1,0,2)\rangle.\]
e ainda
\[V_T(2)=\text{Ker}(T-2 I)=\text{Ker}\begin{pmatrix}
1 & 1 & -1\\2 & 0 & -1\\ 2 & 2 & -2
\end{pmatrix}=\langle (1,1,2)\rangle\]
\end{exemplo}

\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que
\[[T]_{\text{can}}=\begin{pmatrix}
1 & 2 & -1\\-2 & -3 & -1\\ 2 & 2 & -2
\end{pmatrix}.\]
Neste caso temos que
\begin{align*}
p_T(x)&=\text{det}\begin{pmatrix}
x-1 & -2 & 1\\2 & x+3 & 1\\ -2 & -2 & x+2
\end{pmatrix}\\&=(x+1)^2(x+2).
\end{align*}
Isso implica que \(\text{Spec}(T)=\{-1,-2\}\) e ainda \[V_T(-1)=\langle(1,0,2),(0,1,2)\rangle\] e
\[V_T(-2)=\langle (1,-1,1)\rangle.\]
Uma vez que os autovetores acima são L.I, eles formam uma base \(B\) de \(\mathbb{R}^3\) e 
\[[T]_B=\begin{pmatrix}
-1 & 0 & 0\\0 & -1 & 0\\ 0 & 0 & -2
\end{pmatrix}\]
é uma matriz diagonal.
\end{exemplo}
\begin{teorema}\label{diagonal}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que \(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\), e sejam $\lambda_1,\dots,\lambda_k$ os autovalores distintos, e para $i=1,\dots,k$ seja $n_k=\text{dim} \ V_T(\lambda_i)$. São equivalentes:
\begin{enumerate}
\item \(T\) é diagonalizável.
\item \(p_T(t)=(t-\lambda_1)^{n_1}\ldots(t-\lambda_k)^{n_k}\).
\item \(n_1+\ldots+n_k=n.\)
\end{enumerate}
\end{teorema}
\begin{lema}\label{autoespaco}
Sejam \(\lambda_1,\dots,\lambda_k\in K\) distintos. Então:
\begin{enumerate}
\item Se \(v_i \in V_T(\lambda_i)\) para cada \(i=1,\dots,k\) e \(v_1+\ldots+v_k=0\), então \(v_1=\ldots=v_k=0\).
\item Se \(B_i\subseteq V_T(\lambda_i)\) é L.I para cada \(i=1,\dots,k\), então $B_1\cup\dots\cup B_k$ é L.I.
\end{enumerate}
\end{lema}
\begin{proof}[Demonstração do Lema]\hfill
\begin{enumerate}
\item Vamos provar essa afirmação por indução em \(k\). Primeiro note que o
resultado é trivial quando \(k=1\). Agora seja \(k\in\mathbb{N}\) e
assuma  que o resultado vale para cada natural \(i < k\). Sejam
\(v_1,\ldots,v_k\) tais que \(v_i\in V_T(\lambda_i)\) para cada \(i=1,\dots,k\) 
e \(v_1+\ldots + v_k=0\). Então:
\begin{equation}\label{primeira}
0=\lambda_10=\lambda_1(v_1+v_2+\ldots+v_k)=\lambda_1v_1+\lambda_1v_2+\ldots+\lambda_1v_k.
\end{equation}
Além disso, é claro que:
\begin{equation}\label{segunda}
0=T(0)=T(v_1+v_2+\ldots+v_k)=\lambda_1v_1+\lambda_2v_2+\ldots+\lambda_kv_k.
\end{equation}
Subtraindo a Equação \ref{primeira} de \ref{segunda}, obtemos:
\begin{equation}\label{terceira}
(\lambda_2-\lambda_1)v_2+\ldots +(\lambda_k-\lambda_1)v_k=0.
\end{equation}
Agora notamos que cada termo $(\lambda_i-\lambda_1)v_i$ no lado esquerdo é um autovetor de \(T\) associado a $\lambda_i$ e 
aplicamos a hipótese de indução para concluir que \(v_2=\ldots =v_k=0\). 
Finalmente, como sabemos que \(v_1+\ldots +v_k=0\) e \(v_2=\ldots =
v_k=0\), obtemos que \(v_1=0\) também, o que conclui nossa prova.
\item Seja \(S\subseteq B_1\cup\dots\cup B_k\) finito e seja \(\alpha\colon S\to\mathbb{R}\) tal que:
\[
\sum_{v\in S}\alpha_vv=0.
\]
Note que
\(V_T(\lambda_i)\cap V_T(\lambda_j)=\{0\}\) sempre que e
\(i\not=j\) e então podemos escrever
\[\sum_{v\in S}\alpha_vv=\sum_{v\in S_1}\alpha_v v+\ldots+\sum_{v\in
S_k}\alpha_vv,\]
onde \(S_i\subseteq B_i\) é finito para cada \(i=1,\dots,k\). Utilizando o fato de que:
\[
\sum_{v\in S_i}\alpha_v v\in V_T(\lambda_i)
\]
para cada \(i=1,\dots,k\) e aplicando o item anterior, obtemos que
\[\sum_{v\in S_1}\alpha_vv=\ldots=\sum_{v\in S_k}\alpha_vv=0.\]
Finalmente como como \(S_i\subseteq B_i\) para cada \(i=1,\dots,k\) e
\(B_i\) é sempre L.I por hipótese segue que a restrição de \(\alpha\) a
cada \(S_i\) é identicamente nula. Como \(S=S_1\cup\dots\cup S_k\), segue
que \(\alpha\) é indenticamente nula. \qedhere
\end{enumerate}
\end{proof}

\begin{proof}
Temos o seguinte:
\begin{itemize}
\item (i)$\Rightarrow$(ii): Seja $B$ uma base tal que:
\[
[T]_B=\begin{pmatrix}
\lambda_1I_{m_1}&&\\&\ddots&\\&&\lambda_kI_{m_k}
\end{pmatrix},
\]
em que $m_1,\dots,m_k$ são inteiros positivos. Então o polinômio característico de $T$ é:
\[
p_T(t)=(t-\lambda_1)^{m_1}\ldots(t-\lambda_k)^{m_k}.
\]
Além disso, para $i=1,\dots,k$, então a matriz de $T-\lambda_iI$ é igual a:
\[
\begin{pmatrix}
(\lambda_1-\lambda_i)I_{m_1}&&&&\\&\ddots&&&\\&&0_{m_i}&&\\&&&\ddots&\\&&&&(\lambda_k-\lambda_i)I_{m_k}
\end{pmatrix},
\]
aí é fácil ver que:
\[
n_i=\mathrm{dim} \ V_T(\lambda_i)=\mathrm{dim} \ \mathrm{Ker}(T-\lambda_iI)=m_i,
\]
ou seja, $n_i=m_i$; assim:
\[
p_T(t)=(t-\lambda_1)^{n_1}\ldots(t-\lambda_k)^{n_k}.
\]
\item (ii)\(\Rightarrow\)(iii): O polinômio característico de $T$ tem grau $n$, aí:
\[
n_1+\ldots+n_k=\text{deg}(p_T(t))=n.
\]
\item (iii)\(\Rightarrow\)(i): Para cada \(i=1,\dots,k\) considere uma base \(B_i\)
de \(V_T(\lambda_i)\). Seja \(B=B_1\cup\dots\cup B_k\). Pelo Lema \ref{autoespaco},
temos que \(B\) é L.I. Como \(\vert B\vert =n\) segue que \(B\) é uma base de
\(V\). Além disso, \(B\) é uma base de autovetores de \(T\). Logo, \(T\) é diagonalizável.
\end{itemize}
\end{proof}

\subsection{Polinômio Minimal}

\begin{definicao}
Seja $V$ um espaço sobre $K$, $\dim V=n<\infty$, $T\in\mathcal{L}(V)$. Definamos por recursão $T^0=I$ e $T^{k+1}=T^k\circ T$. Se $p(t)\in K[t]$, $p(t)=a_0+a_1t+\dots+a_mt^m$, então está bem definido o operador $p(T)=a_0\cdot I+a_1\cdot I+\dots+a_m\cdot T^m\in\mathcal{L}(V)$.
\end{definicao}

\noindent
Lembremo-nos de que, se $\dim(U)=m$ e $\dim(V)=n$, então $\dim\mathcal{L}(U,V)=mn$. Assim, se $V$ é um espaço vetorial tal que $\dim(V)=n<\infty$, então $\dim\mathcal{L}(V)=n^2$, de modo que existe $m\leq n^2+1$ tal que os operadores $I,T,T^2,\dots,T^m$ sejam linearmente dependentes. Seja $m$ o menor deles. Então existem $a_0,\dots,a_{m-1}\in K$ tais que:
\[
T^m+a_{m-1}T^{m-1}+\dots+a_1T+a_0I=0.
\]
Seja:
\[
m_T(t)=t^m+a_{m-1}t^{m-1}+\dots+a_1t+a_0,
\]
então $m_T(T)=0$, e $m_T(t)$ é um polinômio mônico de grau mínimo tal que $m_T(T)=0$.

\begin{definicao}
Um polinômio mônico de grau mínimo tal que $m_T(t)\in K[t]$ tal que $m_T(t)=0$ chama-se um \textbf{polinômio minimal} do operador $T$.
\end{definicao}

\begin{lema}
Seja $f(t)\in K[t]$ tal que $f(T)=0$. Então $m(t)\mid f(t)$.
\end{lema}
\begin{proof}
Dividimos $f(t)$ por $m(t)$ (com resto):
\[
f(t)=m_T(t)\cdot q(t)+r(t),\quad\quad\deg(r(t))<\deg(m_T(t))\text{ ou }r(t)=0.
\]
Como $f(T)=0$ e $m_T(t)=0$, então $r(T)=0$, aí $r(t)=0$.
\end{proof}

\begin{corolario}
O polinômio $m_T(t)$ é único.
\end{corolario}

\noindent
Se $V$ é um espaço vetorial e $T\in\mathcal{L}(V)$, então $V$ tem uma estrutura de $K[t]$ módulo à esquerda: Se $f(t)\in K[t]$, para $v\in V$ definimos:
\[
f(t)\cdot v=f(T)(v).
\]
Além disso, se considerarmos:
\[
\begin{array}{rcl}
\varphi:K[t]&\rightarrow&\mathrm{End}(V)\\f(t)&\mapsto&f(T),
\end{array}
\]
então $\varphi$ é um homomorfismo de $K$-álgebras e portanto $\mathrm{Ker}(\varphi)$ é um ideal de $K[t]$.

\begin{teorema}
Os polinômios $p_T(t)$ e $m_T(t)$ têm as mesmas raízes em $K$ (a menos de multiplicidade). Em outras palavras, $m_T(\lambda)=0\Leftrightarrow\lambda\in\mathrm{Spec}(T)$.
\end{teorema}
\begin{proof}
Se $m_T(\lambda)=0$, então $m_T(t)=(t-\lambda)q(t)$. Por minimalidade de $m_T(t)$, $q(T)\neq 0$, então existe $w\in V$ tal que $q(T)(w)\neq 0$, aí seja $v=q(T)(w)$, então $v\neq 0$ e:
\[
\begin{array}{rcl}
(T-\lambda I)(v)&=&(T-\lambda I)q(T)(w)\\&=&m_T(t)(w)=0,
\end{array}
\]
aí $T(v)=\lambda v$, aí $\lambda\in\mathrm{Spec}(T)$.

\medskip
\noindent
Por outro lado, se $\lambda\in\mathrm{Spec}(T)$, seja $v\in V$ tal que $v\neq 0$ e $T(v)=\lambda v$, então:
\[
T(T(v))=\lambda^2 v,\dots,T^m(v)=\lambda^m v,\dots,
\]
aí para $f(t)\in K[t]$ temos $f(T)(v)=f(\lambda)\cdot v$, aí $0=m_T(T)(v)=m_T(\lambda)\cdot v$, aí $m_T(\lambda)=0$.
\end{proof}

\begin{corolario}\label{diagonal1}
Se $T$ é diagonalizável e $\mathrm{Spec}(T)=\{\lambda_1,\dots,\lambda_r\}$, então:
\[
m_T(t)=(t-\lambda_1)\dots(t-\lambda_r).
\]
\end{corolario}

\begin{proof}
Já sabemos que:
\[
m_T(t)=(t-\lambda_1)^{k_1}\dots(t-\lambda_r)^{k_r}q(t)
\]
em que $q(t)$ não tem raízes em $K$. Seja
\[
f(t)=(t-\lambda_1I)\dots(t-\lambda_rI).
\]
Seja $v\in V$, então $v=v_1+\dots+v_r$ para alguns $v_i\in V_T(\lambda_i)$, aí temos $(T-\lambda_iI)(v_i)=0$, aí $f(T)(v_i)=0$; logo $f(T)(v)=0$. Portanto $f(T)=0$, aí $m_T\mid f$, aí $m_T=f$.
\end{proof}

\section{Restrições de Operadores}

\subsection{Subespaços Invariantes}

\begin{definicao}
Seja $T\in\mathcal{L}(V)$. Um subespaço $W\subseteq V$ chama-se \textbf{$T$-invariante} se $T(W)\subseteq W$.
\end{definicao}

\begin{observacao}
Um subespaço é $T$-invariante se e só se é um $K[t]$-submódulo.
\end{observacao}
\begin{exemplo}
Seja \(V=\mathbb{C}(\mathbb{R})\) e considere o operador \(D\colon f\to
f^\prime\). Então o subespaço\[P_n\coloneqq\{f(t)\in\mathbb{R}[t]\,\colon
\text{deg}(f)\leq n\}\] é \(D\)-invariante. 
\end{exemplo}

\begin{proposicao}
Seja \(V\) um espaço vetorial de dimensão finita sobre um corpo \(K\), seja \(T\in\mathcal{L}(V)\)
e seja \(W\) um subespaço \(T\)-invariante de \(V\). Seja $B_1$ uma base de $W$ e seja $B$ uma base de $V$ tal que $B_1\subseteq B$. Então $B_2=\{\overline{b}:b\in B\setminus B_1\}$ é uma base de $V/W$ e, sendo $\overline{T}\in L(V/W)$ o operador induzido, temos:
\[
[T]_B=
\begin{pmatrix}
[T\upharpoonright_W]_{B_1}&*\\0&[\overline{T}]_{B_2}
\end{pmatrix},
\]
\end{proposicao}
\begin{proof}
Seja $\dim(V)=\textcolor{blue}{n}$ e $T\in\mathcal{L}(V)$, e seja $W\subseteq V$ um subespaço
$T$-invariante. Escolhemos uma base $B_1=\{v_1,\dots,v_{\textcolor{red}{m}}\}$ de $W$ e
completemos uma base $B=\{v_1,\dots,v_{\textcolor{red}{m}},v_{m+1},\dots,v_{\textcolor{blue}{n}}\}$ do espaço $V$. Qual é a matriz
$[T]_B$?

\medskip
\noindent
Vamos começar notando que \(T\) é \(W\)-invariante e então:
\[
\begin{array}{lcl}
T(v_1)&=&\sum\limits_{i=1}^{\textcolor{red}{m}}\alpha_{1,i}v_i\\T(v_2)&=&\sum\limits_{i=1}^{\textcolor{red}{m}}\alpha_{2,i}v_i\\&\vdots&\\T(v_m)&=&\sum\limits_{i=1}^{\textcolor{red}{m}}\alpha_{m,i}v_i\\T(v_{m+1})&=&\sum\limits_{i=1}^{\textcolor{blue}{n}}\alpha_{m+1,i}v_i\\&\vdots&\\T(v_n)&=&\sum\limits_{i=1}^{\textcolor{blue}{n}}\alpha_{n,i}v_i.
\end{array}
\]
Dessa forma segue que:
\[[T]_B=\begin{pmatrix}
\alpha_{1,1} & \ldots & \alpha_{1,m} &\alpha_{1,m+1}& \ldots & \alpha_{1,n}
\\\ldots & \ldots & \ldots & \ldots &\ldots & \ldots
\\\alpha_{m,1} & \ldots & \alpha_{m,m} & \alpha_{m,m+1} & \ldots &
\alpha_{m,n}\\ 0 & \ldots & 0 & \alpha_{m+1,m+1}&\ldots &\alpha_{m+1,n}\\
\ldots & \ldots & \ldots & \ldots &\ldots & \ldots \\ 0 & \ldots & 0
&\alpha_{n,m+1} & \ldots & \alpha_{n,n}
\end{pmatrix}.\]

\noindent
Isto é, a matriz de \(T\) na base \(B\) tem a forma:
\[[T]_B=\begin{pmatrix}
X & * \\ 0 & Y\end{pmatrix},\]
onde \(X\in M_m(K)\) e \(Y\in M_{n-m}(K)\). Note que \(X\) é a matriz da
restrição de \(T\) a \(W\) na base \(B_1\). Além disso, temos o seguinte:
\[
\begin{array}{lcl}
\bar{T}(\bar{v}_{m+1})&=&\sum\limits_{i={\textcolor{red}{m}}+1}^{\textcolor{blue}{n}}\alpha_{m+1,i}\bar{v}_i\\&\vdots&\\\bar{T}(\bar{v}_n)&=&\sum\limits_{i={\textcolor{red}{m}}+1}^{\textcolor{blue}{n}}\alpha_{n,i}\bar{v}_i.
\end{array}
\]
Portanto $Y$ é a matriz da transformação $\bar{T}$ na base $\{\bar{v}_{m+1},\dots,\bar{v}_n\}$.
\end{proof}

\begin{lema}
Seja $\dim(V)=n$, $T\in\mathcal{L}(V)$ e $W\subseteq V$ um subsepaço $T$-invariante. Então:
\[
p_T(t)=p_{T\upharpoonright_W}(t)\cdot p_{\bar{T}}(t)
\]
em que $\bar{T}$ é o operador induzido em $V/W$.
\end{lema}
\begin{proof}
Escolhamos $B_1$ e $B$ como bases de $W$ e $V$ tais que $B_1\subseteq B$, então, considerando $B_2=\{\bar{b}:b\in B\setminus B_1\}$, a matriz de \(T\) na base \(B\) tem a forma:
\[Z=\begin{pmatrix}
X & * \\ 0 & Y\end{pmatrix},\]
onde $X$ é a matriz de $T\upharpoonright_W$ em relação a $B_1$ e $Y$ é a matriz de $\bar{T}$ em relação a $B_2$, assim:
\begin{align*}
p_T(t)&=p_Z(t)\\
&=\det(tI-Z)\\
&=\text{det}\begin{pmatrix}
tI_m-X & * \\ 0 & tI_{n-m}-Y\end{pmatrix}\\
&=\text{det}(tI_m-X)\text{det}(tI_{n-m}-Y)\\
&=p_X(t)p_Y(t)\\
&=p_{T\upharpoonright_W}(t)p_{\bar{T}}(t)\qedhere
\end{align*}
\end{proof}

\begin{observacao}
O mesmo \textcolor{red}{não} ocorre para polinômios minimais. De fato, seja $T=I_V$ e seja $W$ um subespaço $T$-invariante (De fato, quando \(T\) é a identidade, todo subespaço de \(V\) é \(T\)-invariante), então $T_1=I_W$ e $T_2=I_{V/W}$ e aí $m_T(t)=m_{T_1}(t)=m_{T_2}(t)=t-1$. Não obstante, ainda temos um resultado interessante para isso.
\end{observacao}

\begin{lema}
Seja $\dim(V)=n$, $T\in\mathcal{L}(V)$ e $W\subseteq V$ um subsepaço $T$-invariante. Então:
\[
m_{T\upharpoonright_W}(t)\mid m_T(t),\quad m_{\bar{T}}(t)\mid m_T(t),
\]
em que $\bar{T}$ é o operador induzido em $V/W$.
\end{lema}
\begin{proof}
Escolhamos $B_1$ e $B$ como bases de $W$ e $V$ tais que $B_1\subseteq B$, então, considerando $B_2=\{\bar{b}:b\in B\setminus B_1\}$, a matriz de \(T\) na base \(B\) tem a forma:
\[Z=\begin{pmatrix}
X & * \\ 0 & Y\end{pmatrix},\]
onde $X$ é a matriz de $T\upharpoonright_W$ em relação a $B_1$ e $Y$ é a matriz de $\bar{T}$ em relação a $B_2$, assim é fácil de mostrar por indução que para todo $k\geq 0$ então temos uma matriz da forma:
\[
Z^k=
\begin{pmatrix}
X^k & * \\ 0 & Y^k
\end{pmatrix},
\]
assim temos uma matriz é da forma:
\[
m_T(Z)=
\begin{pmatrix}
m_T(X) & * \\ 0 & m_T(Y)
\end{pmatrix},
\]
mas sabemos que $m_T(Z)=0$, aí $m_T(X)=0$ e $m_T(Y)=0$, aí a conclusão segue.
\end{proof}

\subsection{Subespaços Cíclicos}

\begin{definicao}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$. Para $v\in V$, definimos o \textbf{subespaço $T$-cíclico gerado por $v$} como o conjunto $Z(v,T)$ de todos os vetores da forma $p(T)(v)$ em que $p\in K[x]$. Dizemos que $v$ é um \textbf{vetor cíclico} para $T$ se e só se $Z(v,T)=V$.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Para $v\in V$, então $Z(v,T)$ é o subespaço gerado pelo conjunto $\{T^n(v):n\in\mathbb{N}\}$.
\end{proposicao}

\begin{definicao}
Dado $v\in V$, um polinômio mônico de grau mínimo $m_{T,v}(t)\in K[t]$ tal que $m_{T,v}(T)(v)=0$ chama-se um \textbf{polinômio $T$-anulador} do vetor $v$.
\end{definicao}

\begin{lema}
Seja $f(t)\in K[t]$ tal que $f(T)(v)=0$. Então $m_{T,v}(t)\mid f(t)$.
\end{lema}
\begin{proof}
Dividimos $f(t)$ por $m_{T,v}(t)$ (com resto):
\[
f(t)=m_{T,v}(t)\cdot q(t)+r(t),\quad\quad\deg(r(t))<\deg(m_{T,v}(t))\text{ ou }r(t)=0.
\]
Como $f(T)(v)=0$ e $m_{T,v}(T)(v)=0$, então $r(T)(v)=0$, aí $r(t)=0$.
\end{proof}

\begin{corolario}
O polinômio $m_{T,v}(t)$ é único.
\end{corolario}

\begin{teorema}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$. Seja $v\in V$ que possua um polinômio $T$-anulador, e consideremos o polinômio $m_{T,v}$. Então:
\begin{itemize}
\item O grau de $m_{T,v}$ é igual à dimensão de $Z(v,T)$.
\item Se o grau de $m_{T,v}$ é $k$, então $v,T(v),\dots,T^{k-1}(v)$ formam uma base de $Z(v,T)$.
\item Se $W=Z(v,T)$, então $m_{T\upharpoonright_W}=m_{T,v}$.
\end{itemize}
\end{teorema}

\noindent
O teorema seguinte mostra como subespaços cíclicos podem ser compostos e decompostos.

\begin{teorema}\label{ciclico}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$.
\begin{itemize}
\item (Compondo subespaços cíclicos) Se $u_1,\dots,u_n\in V$ tem $T$-anuladores mutuamente primos entre si, então o $T$-anulador de $u=u_1+\dots+u_n$ é:
\[
m_{T,u}=m_{T,u_1}\dots m_{T,u_n}
\]
e também:
\[
Z(u,T)=Z(u_1,T)\oplus\dots\oplus Z(u_n,T).
\]
\item (Decompondo subespaços cíclicos) Se $m_{T,u}=f_1\dots f_n$ com $f_1,\dots,f_n$ mutuamente primos entre si, então $u$ tem a forma:
\[
u=u_1+\dots+u_n,
\]
em que $m_{T,u_i}=f_i$, e também:
\[
Z(u,T)=Z(u_1,T)\oplus\dots\oplus Z(u_n,T).
\]
\end{itemize}
\end{teorema}

\section{Teoremas de Cayley-Hamilton}

\subsection{Teorema de Cayley-Hamilton Usual}

\begin{teorema}[Teorema da Cayley-Hamilton]\label{cayley-hamilton}
Seja \(V\) um espaço vetorial de dimensão finita sobre um corpo \(K\), e seja \(T\in\mathcal{L}(V)\). Então \(p_T(T)=0\), onde \(p_T(t)\in K[t]\) é um polinômio característico de \(T\). 
\end{teorema}
\begin{proof}
Basta provar que \(\forall v\in V:p_T(T)(v)=0\). Seja $v\in V$. Consideremos:
\[
m_{T,v}(t)=t^m+\alpha_{m-1}t^{m-1}+\dots+\alpha_1 t+\alpha_0,
\]
o polinômio mônico de menor grau tal que \(m_{T,v}(T)(v)=0\). Então \(B_1=\{v,T(v),\ldots, T^{m-1}(v)\}\) é linearmente independente. Seja \(W\) o subespaço gerado por ele. Note que \(W\) é \(T\)-invariante e ainda:
\[[T\upharpoonright_W]_{B_1}=\begin{pmatrix}
0 & 0 & \ldots& 0 & -\alpha_0
\\ 1 & 0 & \ldots & 0 & -\alpha_1
\\ 0 & 1 & \ldots & 0 & -\alpha_2
\\ \ldots & \ldots  & \ldots  & \ldots  & \ldots \
\\ 0 & 0 & \ldots & 1 & -\alpha_{m-1} 
\end{pmatrix}\]
Então, pelo Exercício 18 da lista 1, segue que:
\[
p_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.\]
Aplicando essa função a \(v\) segue:
\[p_{T \upharpoonright_{W}}(T)(v)=m_{T,v}(T)(v)=0.\]
Para concluir que \(p_T(T)(v)=0\), notamos que \(p_T(t)=p_{T\upharpoonright_W}(t)\cdot p_{\bar{T}}(t),\) em que $\bar{T}$ é o operador induzido em $V/W$.
\end{proof}
\begin{corolario}
Se \(A\in M_n(K)\) então \(p_A(A)=0\),
onde \(p_A(t)=\text{det}(tI -A)\). \end{corolario}

\begin{exemplo}
Considere a matriz\[\begin{pmatrix}
a & b \\ c & d
\end{pmatrix}.\]
Então temos que \(p_A(t)=t^2-(a+d)t+(ad-bc)\) e também
\begin{align*}
P_A(A)&=A^2-(a+d)A+(ad-bc)I\\
&=
\begin{pmatrix} a^2+bc & ab+ad\\ac+dc & bc+d^2\end{pmatrix}
-\begin{pmatrix} a^2+ad & ab+bd \\ ac+dc & ad+d^2\end{pmatrix}
+ \text{det}(A)I
\\&=\begin{pmatrix}bc-ad & 0 \\ 0 & bc- ad\end{pmatrix}
+ \begin{pmatrix}ad-bc & 0 \\ 0 & ad - bc\end{pmatrix} = 0.
\end{align*}
\end{exemplo}

\subsection{Teorema de Cayley Hamilton ao Avesso}

\begin{teorema}[Teorema da Cayley-Hamilton ao Avesso]\label{cayley-hamilton-avesso}
Se $V$ é espaço de dimensão finita e $T\in L(V)$, então todo polinômio irredutível que divide $p_T$ também divide $m_T$.
\end{teorema}
\begin{proof}
Faremos a demonstração por indução na dimensão de $V$. Suponhamos o lema válido para $\dim(V)<n$. Seja $V$ espaço vetorial tal que $\dim(V)=n$ e seja $T\in L(V)$ e seja $p$ um polinômio irredutível que divide $p_T$. Se $V=0$, é fácil. Senão, então tome um $v\neq 0$ qualquer. Consideremos:
\[
m_{T,v}(t)=t^m+\alpha_{m-1}t^{m-1}+\dots+\alpha_1 t+\alpha_0,
\]
o polinômio mônico de menor grau tal que \(m_{T,v}(T)(v)=0\). Então \(B_1=\{v,T(v),\ldots, T^{m-1}(v)\}\) é linearmente independente. Seja \(W\) o subespaço gerado por ele. Note que \(W\) é \(T\)-invariante e ainda:
\[[T\upharpoonright_W]_{B_1}=\begin{pmatrix}
0 & 0 & \ldots& 0 & -\alpha_0
\\ 1 & 0 & \ldots & 0 & -\alpha_1
\\ 0 & 1 & \ldots & 0 & -\alpha_2
\\ \ldots & \ldots  & \ldots  & \ldots  & \ldots \
\\ 0 & 0 & \ldots & 1 & -\alpha_{m-1} 
\end{pmatrix}\]
Então, pelo Exercício 18 da lista 1, segue que:
\[
p_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.
\]
Além disso, vendo a definição de $m_{T,v}$, é fácil ver que:
\[
m_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.
\]
Sendo $\overline{T}\in L(V/W)$ o operador induzido, temos $p_T=p_{T\upharpoonright_W}p_{\overline{T}}$ e $m_{T\upharpoonright_W}\mid m_T$ e $m_{\overline{T}}\mid m_T$. Assim, como $p\mid p_T$, então $p\mid p_{T\upharpoonright_W}$ ou $p\mid p_{\overline{T}}$.
\begin{itemize}
\item Se $p\mid p_{T\upharpoonright_W}$, como $p_{T\upharpoonright_W}=m_{T\upharpoonright_W}$, então $p\mid m_{T\upharpoonright_W}$, aí $p\mid m_T$.
\item Se $p\mid p_{\overline{T}}$, então, por hipótese de indução, temos $p\mid m_{\overline T}$, aí $p\mid m_T$. \qedhere
\end{itemize}
\end{proof}

\section{Decomposições Primárias}

\subsection{Decomposição Primária Geral}

\begin{teorema}[Decomposição Primária Geral]
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que
\(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\). Suponhamos que
\(f(T)=0\) e também:
\[
f=f_1\ldots f_r,
\]
com $f_1,\dots,f_r$ mutuamente primos entre si. Para cada $i$ seja $V_i=\Ker \ f_i(T)$. Então:
\begin{itemize}
\item Cada $V_i$ é $T$-invariante.
\item $V=V_1\oplus\dots\oplus V_r$.
\item Cada projeção canônica $P_i:V\rightarrow V_i$ é um polinômio de $T$.
\end{itemize}
\end{teorema}
\begin{lema}[Identidade de Bézout]
Se \(\mathrm{mdc}(f_1,\dots,f_r)=1\) então existem $g_1,\dots,g_r\in K[t]$ tais que:
\[f_1g_1+\dots+f_rg_r=1.\]
\end{lema}
\begin{proof}[Demonstração do Teorema]
Para cada $i$, então para $v\in V_i$ temos $f_i(T)(v)=0$, aí $Tf_i(T)(v)=0$, aí $f_i(T)T(v)=0$, aí $T(v)\in V_i$; logo $V_i$ é $T$-invariante. Agora para cada $i$ seja:
\[
h_i=\frac{f}{f_i}.
\]
Então $\mathrm{mdc}(h_1,\dots,h_r)=1$, assim existem $g_1,\dots,g_r\in K[t]$ tais que:
\[h_1g_1+\dots+h_rg_r=1,\]
aí:
\[
h_1(T)g_1(T)+\dots+h_r(T)g_r(T)=I.
\]
Para $v_1,\dots,v_r\in V$, se para todo $i$ tivermos $v_i\in V_i$ e:
\[
v_1+\dots+v_r=0,
\]
então para cada $i$ temos:
\[
h_i(T)(v_1)+\dots+h_i(T)(v_r)=0,
\]
mas para cada $j\neq i$ então $f_j\mid h_i$, aí $h_i(T)(v_j)=0$; assim:
\[
h_i(T)(v_i)=0,
\]
mas:
\[
v_i=h_1(T)g_1(T)(v_i)+\dots+h_r(T)g_r(T)(v_i),
\]
e para cada $j\neq i$ temos $f_i\mid h_j$, aí $h_j(T)(v_i)=0$; assim:
\[
v_i=h_i(T)g_i(T)(v_i)=0;
\]
portanto:
\[
v_1=\dots=v_r=0.
\]
Para todo $v\in V$ então:
\[
v=h_1(T)g_1(T)(v)+\dots+h_r(T)g_r(T)(v),
\]
e para cada $i$ temos:
\[
f_i(T)h_i(T)g_i(T)(v)=f(T)g_i(T)(v)=0,
\]
aí:
\[
h_i(T)g_i(T)(v)\in V_i;
\]
logo:
\[
V=V_1\oplus\dots\oplus V_r,
\]
e, para cada $i$, a função $h_i(T)g_i(T)$ é a projeção canônica de $V$ em $V_i$.
\end{proof}

\subsection{Decomposição Primária para Minimais}

\begin{teorema}[Decomposição Primária para Minimais]\label{primaria minimal}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que
\(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\), e seja:
\[
m_T=p_1^{k_1}\ldots p_r^{k_r},
\]
com \(p_1\dots,p_r\) irredutíveis e mutuamente
primos entre si. Para cada $i$ seja $V_i=\Ker \ p_i(T)^{k_i}$. Então:
\begin{itemize}
\item Cada $V_i$ é $T$-invariante.
\item $V=V_1\oplus\dots\oplus V_r$.
\item Cada projeção canônica $P_i:V\rightarrow V_i$ é um polinômio de $T$.
\item Para cada $i$ então \(m_{T\upharpoonright_{V_i}}=p_i^{k_i}\).
\end{itemize}
\end{teorema}
\begin{proof}
Por definição temos \(m_T(T)=0\). Logo, pelo teorema da decomposição primária geral, temos os três primeiros itens. Agora considere \(T_i\coloneqq T\upharpoonright_{V_i}\)
para cada \(i=1,\dots,r\).
Temos que \(p_i(T_i)^{k_i}=0\). Então segue que \(m_{T_i}\mid
p_i^{k_i}\), ou seja,
\(m_{T_i}=p_i^{m_i}\), onde \(m_i\leq k_i\). Consideremos:
\[
g=p_1^{k_1}\ldots p_i^{m_i}\ldots
p_r^{k_r}.
\]
Para $j\neq i$ e para \(v\in V_j\), então \(p_j^{k_j}(v)=0\) e portanto
\(g(T)(v)=0\). Se \(v\in V_i\) então \(p_i^{m_i}(T)(v)=0\) e
\(g(T)(v)=0\). Assim, como $V=V_1\oplus\dots\oplus V_r$, concluímos que \(g(T)=0\).
Isso implica que \(m_T\mid g\), aí \(p_i^{k_i}\mid p_i^{m_i}\), aí $k_i\leq m_i$, assim $m_i=k_i$, aí \(m_{T_i}=p_i^{k_i}.\)
\end{proof}

\noindent
A decomposição primária para minimais também goza de uma propriedade muito importante para o estudo da álgebra linear.

\begin{teorema}[Unicidade da Decomposição Primária para Minimais]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que:
\[
V=U_1\oplus\dots\oplus U_m
\]
em que $U_i$ é um subespaço $T$-invariante tal que $m_{T\upharpoonright_{U_i}}=p_i^{e_i}$ e $p_1,\dots,p_m$ são polinômios mônicos irredutíveis distintos, e suponhamos que:
\[
V=W_1\oplus\dots\oplus W_n
\]
em que $W_j$ é um subespaço $T$-invariante tal que $m_{T\upharpoonright_{W_j}}=q_j^{f_j}$ e $q_1,\dots,q_n$ são polinômios mônicos irredutíveis distintos. Então $m=n$ e, depois de uma reindexação adequada, $U_k=W_k$ para todo $k$. Portanto $p_k=q_k$ e $e_k=f_k$ para todo $k$.
\end{teorema}
\begin{proof}
Para todo $i$, então $U_i$ contém um elemento $u_i$ tal que $m_{T,u_i}=p_i^{e_i}$; assim, definindo a soma $u=u_1+\dots+u_m$, então $m_{T,u}=p_1^{e_1}\dots p_m^{e_m}$. Logo $m_T=p_1^{e_1}\dots p_m^{e_m}$. Analogamente temos $m_T=q_1^{f_1}\dots q_n^{f_n}$.

\medskip
\noindent
Portanto, pela fatoração única em $K[t]$, então $m=n$ e, depois de uma reindexação apropriada, temos $p_k=q_k$ e $e_k=f_k$ para todo $k$. Para todo $k$, temos:
\[
U_k\subseteq\Ker \ p_k(T)^{e_k},
\]
mas pela decomposição primária geral temos:
\[
U_1\oplus\dots\oplus U_m=\left(\Ker \ p_1(T)^{e_1}\right)\oplus\dots\oplus\left(\Ker \ p_m(T)^{e_m}\right),
\]
aí para todo $k$ temos $U_k=\Ker \ p_k(T)^{e_k}$. Analogamente temos $W_k=\Ker \ q_k(T)^{f_k}$ para todo $k$.
\end{proof}

\subsection{Decomposição Primária para Característicos}

\begin{teorema}[Decomposição Primária para Característicos]\label{primaria caracteristico}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que
\(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\), e seja:
\[
p_T=p_1^{k_1}\ldots p_r^{k_r},
\]
com \(p_1\dots,p_r\) irredutíveis e mutuamente
primos entre si. Para cada $i$ seja $V_i=\Ker \ p_i(T)^{k_i}$. Então:
\begin{itemize}
\item Cada $V_i$ é $T$-invariante.
\item $V=V_1\oplus\dots\oplus V_r$.
\item Cada projeção canônica $P_i:V\rightarrow V_i$ é um polinômio de $T$.
\item Para cada $i$ então \(p_{T\upharpoonright_{V_i}}=p_i^{k_i}\).
\end{itemize}
\end{teorema}
\begin{proof}
Pelo teorema de Cayley-Hamilton temos \(p_T(T)=0\). Portanto, pelo teorema da decomposição primária geral, temos os três primeiros itens. Agora considere \(T_i\coloneqq T\upharpoonright_{V_i}\)
para cada \(i=1,\dots,r\).
Temos que \(p_i(T_i)^{k_i}=0\). Então segue que \(m_{T_i}\mid
p_i^{k_i}\), aí, pelo teorema de Cayley-Hamilton \textcolor{red}{ao avesso} (Teorema \ref{cayley-hamilton-avesso}), todo fator irredutível de $p_{T_i}$ deve dividir $m_{T_i}$, logo ser igual a $p_i$. Assim $p_{T_i}=p_i^{l_i}$ para algum $l_i$. Entretanto, considerando bases $B_i$ de $V_i$, e juntando numa base $B$ de $V$, é fácil ver que $p_T=p_{T_1}\dots p_{T_k}$, assim $p_T=p_1^{l_1}\ldots p_r^{l_r}$, aí pela fatoração única devemos ter $l_i=k_i$ para todo $i$, concluindo a demonstração.
\end{proof}

\section{Critérios de Diagonalização}

\subsection{Diagonalização Usual}

\begin{teorema}\label{diagonal2}
Um operador \(T\in\mathcal{L}(V)\) é diagonalizável se, e somente se:
\[
m_T(t)=(t-\lambda_1)\ldots(t-\lambda_r)
\]
com \(\lambda_i\not=\lambda_j\) sempre que \(i\not= j\).
\end{teorema}
\begin{proof}
A ida já foi provada no corolário \ref{diagonal1}, então
vamos mostrar apenas a volta. Pelo teorema da decomposição primária geral, sendo $V_i=\mathrm{Ker} \ (T-\lambda_iI)$ para todo $i$, então cada $V_i$ é $T$-invariante e \(V=V_1\oplus\ldots\oplus V_r\), aí para todo $i$ temos \(T\upharpoonright_{V_i}=\lambda_iI\), aí \(V_i\subseteq V_T(\lambda_i)\); logo $T$ é diagonalizável. 
\end{proof}

\subsection{Diagonalização Simultânea de Vários Operadores}

Considere \(\mathcal{F}\subseteq\mathcal{L}(V)\). Em que condições os operadores $T\in\mathcal{F}$ podem ser diagonalizados simultaneamente, ou seja, existe base $B$ de $V$ tal que para todo $T\in\mathcal{F}$ a matriz $[T]_B$ seja diagonal?

\begin{teorema}
Um conjunto \(\mathcal{F}\subseteq\mathcal{L}(V)\) pode ser diagonalizado
simultaneamente se, e somente se cada \(T\in\mathcal{F}\) é diagonalizável e \(TU=UT\) para quaisquer \(T,U\in\mathcal{F}\).
\end{teorema}
\begin{proof}
Mostraremos por indução na dimensão. Suponhamos que o teorema é válido para espaços de dimensão menor que $n$. Seja $V$ um espaço tal que $\dim(V)=n$ e seja $\mathcal{F}$ um conjunto de operadores que comutam um com outro. Se todo elemento de $\mathcal{F}$ é múltiplo da identidade, então acaba. Caso contrário, existe um $T\in\mathcal{F}$ que não é múltiplo da identidade. Sejam $c_1,\dots,c_k$ os autovalores de $T$. Para $i$ seja $W_i=\Ker(T-c_iI)$. Então, como os elementos de $\mathcal{F}$ comutam um com outro, então $W_i$ é invariante para todo elemento de $\mathcal{F}$. Para cada $U\in\mathcal{F}$, então $m_U$ é produto de fatores lineares distintos, aí, como $m_{U\upharpoonright_{W_i}}\mid m_U$, então $m_{U\upharpoonright_{W_i}}$ é produto de fatores lineares distintos, aí $U\upharpoonright_{W_i}$ é diagonalizável. Como $\dim(W_i)<n$, então existe uma base $B_i$ tal que para todo $U\in\mathcal{F}$ a matriz $[U\upharpoonright_{W_i}]_{B_i}$ seja diagonal. Portanto $B=B_1\cup\dots\cup B_k$ é a base que buscamos.
\end{proof}

\section{Triangularização de Matrizes}

\subsection{Triangularização Usual}

\begin{definicao}
Um operador linear $T\in\mathcal{L}(V)$ é dito \textbf{triangularizável} se existe uma
base ordenada $B=(v_1,\dots,v_n)$ de $V$ tal que a matriz $[T]_B$ seja triangular por cima, ou equivalentemente, se:
\[
T(v_i)\in\langle v_1\dots,v_i\rangle
\]
para todo $i=1,\dots,n$.
\end{definicao}

\begin{teorema}[Teorema de Schur]
Seja $V$ um espaço vetorial de dimensão finita sobre um corpo $K$ e seja $T\in\mathcal{L}(V)$. Se o polinômio característico (ou o polinômio minimal) de $T$ se decompõe em fatores lineares sobre $K$, então $T$ é triangularizável.
\end{teorema}
\begin{proof}
Provaremos por indução em $n$ que cada matriz $A\in\mathcal{M}_n(K)$
cujo polinômio característico se decomponha em fatores lineares sobre $K$ é similar
a uma matriz triangular por cima. Se $n=1$, então é fácil, já que todas as matrizes quadradas de tamanho $1$ são triangulares por cima. Assumamos o resultado para $n-1$ e seja $A\in M_n(K)$ tal que o polinômio característico se decomponha em fatores lineares sobre $K$.

\medskip
\noindent
Seja $\lambda$ um autovalor de $T$ e seja $e_1\neq 0$ tal que $T(e_1)=\lambda e_1$, e estenda $(e_1)$ a uma base ordenada $B=(e_1,\dots,e_n)$ de $K^n$. A matriz de $A$ em relação a $B$ tem a forma:
\[
[A]_B=\begin{pmatrix}
\lambda&*\\0&A'
\end{pmatrix}
\]
para alguma matriz $A'\in M_{n-1}(K)$. Como $[A]_B$ e $A$ são similares, temos:
\[
\det(xI-A)=\det(xI-[A]_B)=(x-\lambda)\det(xI-A').
\]
Portanto o polinômio característico de $A'$ também se decompõe em fatores lineares sobre $K$ a hipótese de indução implica que exista uma matriz inversível $P\in M_{n-1}(K)$ tal que:
\[
U=PA'P^{-1}
\]
seja triangular por cima. Assim, se:
\[
Q=\begin{pmatrix}
1&0\\0&P
\end{pmatrix},
\]
então $Q$ é inversível e:
\[
Q[A]_BQ^{-1}=\begin{pmatrix}
1&0\\0&P
\end{pmatrix}\begin{pmatrix}
\lambda&*\\0&A'
\end{pmatrix}\begin{pmatrix}
1&0\\0&P^{-1}
\end{pmatrix}=
\begin{pmatrix}
\lambda&*\\0&U
\end{pmatrix}
\]
é triangular por cima.
\end{proof}

\begin{corolario}
Num corpo algebricamente fechado $K$, então toda matriz sobre $K$ é semelhante sobre $K$ a uma matriz triangular superior.
\end{corolario}

\begin{observacao}
Por outro lado, se $T\in\mathcal{L}(V)$ é triangularizável, então é evidente que o polinômio característico de $T$ se decompõe em fatores lineares sobre $K$.
\end{observacao}
\begin{proof}
Para isto, apenas considere a matriz triangular superior:
\[
[T]_B=\begin{pmatrix}
a_{1,1}&a_{1,2}&a_{1,3}&\ldots&a_{1,n}\\0&a_{2,2}&a_{2,3}&\ldots&a_{2,n}\\0&0&a_{3,3}&\ldots&a_{3,n}\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\ldots&a_{n,n}
\end{pmatrix}.
\]
Então é fácil ver que:
\[
p_T(x)=(x-a_{1,1})\dots(x-a_{n,n}),
\]
ou seja, o polinômio característico se decompõe em fatores lineares sobre $K$.
\end{proof}

\subsection{Triangularização Simultânea de Vários Operadores}

Considere \(\mathcal{F}\subseteq\mathcal{L}(V)\). Em que condições os operadores \(T\in\mathcal{F}\) podem ser triangularizados simultaneamente, ou seja, existe base $B$ de $V$ tal que para todo $T\in\mathcal{F}$ a matriz $[T]_B$ seja triangular por cima?

\medskip
\noindent
Infelizmente não responderemos a esta pergunta, mas mostraremos uma condição suficiente para a triangularização simultânea.

\begin{teorema}
Seja \(\mathcal{F}\subseteq\mathcal{L}(V)\) um conjunto tal que cada $T\in\mathcal{F}$ seja triangularizável e \(TU=UT\) para quaisquer \(T,U\in\mathcal{F}\). Então $\mathcal{F}$ pode ser triangularizado simultaneamente.
\end{teorema}

\begin{lema}
Para todo espaço vetorial $V$ de dimensão finita tal que $V\neq 0$ e conjunto $\mathcal{F}\subseteq\mathcal{L}(V)$ de operadores triangularizáveis que comutam um com outro, então existe um $v\in V$ tal que $v\neq 0$ e para todo $T\in\mathcal{F}$ exista $\lambda\in K$ tal que $T(v)=\lambda v$.
\end{lema}

\begin{proof}
Suponhamos a afirmação para espaços vetoriais de dimensão menor que $n$. Seja $V$ um espaço com $\dim(V)=n$. Seja $\mathcal{F}$ conjunto de operadores triangularizáveis que comutam. Se todo elemento de $\mathcal{F}$ é múltiplo da identidade, então acaba. Senão, então existe $T\in\mathcal{F}$ que não é múltiplo da identidade, então, como $T$ é triangularizável, então existe um autovalor $c$, aí, sendo $W=\Ker(T-cI)$, então $0<\dim(W)<n$, aí, como os elementos de $\mathcal{F}$ se comutam, então $W$ é invariante para todo elemento de $\mathcal{F}$. Para $U\in\mathcal{F}$, então $m_U$ é produto de fatores lineares, aí, como $m_{U\upharpoonright_W}\mid m_U$, então $m_{U\upharpoonright_W}$ é produto de fatores lineares, aí $U\upharpoonright_W$ é triangularizável. Como $\dim(W)<n$, então existe um $v\in W$ não nulo tal que para cada $U\in\mathcal{F}$ exista um $\lambda\in K$ tal que $(U\upharpoonright_W)(v)=\lambda v$, aí $U(v)=\lambda v$; aí acaba.
\end{proof}

\begin{proof}[Demonstração do Teorema]

\end{proof}

\subsection{Quase Triangularização}

\section{Decomposições Cíclicas}

\subsection{Espaços Primários}

\begin{definicao}
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Dizemos que $V$ é \textbf{primário} em relação a $T$ se e só se existe um polinômio irredutível $p(t)\in K[t]$ e um natural $e$ tal que $p(T)^e=0$.
\end{definicao}

\noindent
O seguinte teorema mostra como podemos decompor um espaço vetorial anulável por uma potência de polinômio irredutível em subespaços cíclicos.

\begin{teorema}[Decomposição Cíclica de Espaços Primários]
Seja $V$ um espaço vetorial com \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$ tal que $m_T=p^e$, em que $p$ é um polinômio irredutível. Então $V$ é uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_n,T)
\]
de subespaços cíclicos com anuladores $m_{T,v_i}=p^{e_i}$, que podem ser arranjados em ordem decrescente:
\[
e=e_1\geq e_2\geq\dots\geq e_n.
\]
\end{teorema}
\begin{proof}
Seja $v_1\in V$ um vetor com anulador igual a polinômio minimal de $T$, ou seja:
\[
m_{T,v_1}=m_T=p^e.
\]
Tal elemento deve existir pois $m_{T,v}\mid m_T$ para todo $v\in V$ e se ninguém tiver anulador igual a $p(t)^e$, então $p(t)^{e-1}$ anulará $V$.

\medskip
\noindent
Se mostrarmos que $Z(v_1,T)$ possui complemento $T$-invariante, ou seja, $V=Z(v_1,T)\oplus S_1$ para algum subespaço $T$-invariante $S_1$, então, como $S_1$ tem dimensão finita sobre $K$, ao considerarmos $T\upharpoonright_{S_1}$, podemos repetir o processo para obter:
\[
V=Z(v_1,T)\oplus Z(v_2,T)\oplus S_2
\]
em que $m_{T,v_i}=p^{e_i}$. Podemos continuar esta decomposição:
\[
V=Z(v_1,T)\oplus Z(v_2,T)\oplus\dots\oplus Z(v_n,T)\oplus S_n
\]
enquanto $S_n\neq 0$. Mas a sequência ascendente de subespaços $T$-invariantes:
\[
Z(v_1,T)\subseteq Z(v_1,T)\oplus Z(v_2,T)\subseteq\dots
\]
deve terminar pois a sequência das dimensões é estritamente crescente e $V$ tem dimensão finita, assim existe um inteiro $n$ tal que $S_n=0$, fornecendo-nos a decomposição buscada.

\medskip
\noindent
Seja $v=v_1$. A soma direta de subespaços $T$-invariantes $V_1=Z(v,T)\oplus 0$ claramente existe. Suponhamos que a soma direta de subespaços $T$-invariantes:
\[
V_k=Z(v,T)\oplus W_k
\]
exista. Afirmamos que, se $V_k\neq V$, então é possível encontrar um subespaço $T$-invariante $W_{k+1}$ que contenha propriamente $W_k$ e para o qual a soma direta $V_{k+1}=Z(v,T)\oplus W_{k+1}$ exista. Esta processo deve também terminar após um número finito de passos, fornecendo-nos uma decomposição em soma direta de subespaços $T$-invariantes:
\[
V=Z(v,T)\oplus W
\]
como desejado.

\medskip
\noindent
Se $V_k\neq V$, então seja $u\in V\setminus V_k$, aí o polinômio de menor grau $r$ tal que $r(T)(u)\in V_k$ deve ser $p^f$ para algum $f\leq e$. Além disso, como $u\notin V_k$, então $f>0$. Assim existem $a(t)\in K[t]$ e $w\in W_k$ tais que:
\[
p(T)^f(u)=a(T)(v)+w.
\]
Logo:
\[
0=p(T)^e(u)=p(T)^{e-f}p(T)^f(u)=p(T)^{e-f}(a(T)(v)+w)=p(T)^{e-f}a(T)(v)+p(T)^{e-f}(w).
\]
Como $Z(v,T)\cap W_k=0$ então $p(T)^{e-f}a(T)(v)=0$, aí $p^e\mid p^{e-f}a$, aí $p^f\mid a$, aí existe $\alpha(t)\in K[t]$ tal que $a=p^f\alpha$, assim:
\[
p(T)^f(u)=a(T)(v)+w=p(T)^f\alpha(T)(v)+w,
\]
aí:
\[
p(T)^f(u-\alpha(T)(v))\in W_k.
\]
Assim seja:
\[
W_{k+1}=W_k+Z\left(u-\alpha(T)(v),T\right).
\]
Para $x\in Z(v,T)\cap W_{k+1}$, então existem $f(t)\in K[t]$ e $g(t)\in K[t]$ e $w\in W_k$ tais que:
\[
x=f(T)(v)=w+g(T)(u-\alpha(T)(v)),
\]
aí:
\[
g(T)(u)=(f-g\alpha)(T)(v)-w\in V_k,
\]
aí $p^f\mid g$, aí:
\[
g(T)(u-\alpha(T)(v))\in W_k,
\]
aí:
\[
x\in Z(v,T)\cap W_k,
\]
aí $x=0$. Logo a soma direta de subespaços $T$-invariantes:
\[
V_{k+1}=Z(v,T)\oplus W_{k+1}
\]
existe.
\end{proof}

\noindent
A decomposição cíclica de espaços primários, assim como a decomposição primária para minimais, goza da propriedade da unicidade.

\begin{teorema}[Unicidade da Decomposição Cíclica de Espaços Primários]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que $V$ é uma soma direta:
\[
V=Z(u_1,T)\oplus\dots\oplus Z(u_m,T),
\]
em que $m_{T,u_i}=p_i^{e_i}$ e também:
\[
e_1\geq e_2\geq\dots\geq e_m,
\]
e também suponhamos que $V$ é uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_n,T),
\]
em que $m_{T,v_j}=q_j^{f_j}$ e também:
\[
f_1\geq f_2\geq\dots\geq f_n.
\]
Então $m=n$ e $p=q$ e $e_k=f_k$ para todo $k$.
\end{teorema}

\begin{lema}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$ e seja $p(t)$ um polinômio irredutível.
\begin{itemize}
\item Se $p(T)=0$, então $V$ é um espaço vetorial sobre o corpo $K[t]/p(t)K[t]$ com a
multiplicação definida por:
\[
\overline{r}(v)=r(T)v
\]
para quaisquer $r(t)\in K[t]$ e $v\in V$.
\item Para qualquer subespaço $T$-invariante $W$ de $V$ o conjunto:
\[
W\cap \Ker \ p(T)=\{v\in W:p(T)(v)=0\}
\]
é um subespaço $T$-invariante de $V$ e se $V=U\oplus W$, então:
\[
\Ker \ p(T)=\left(U\cap \Ker \ p(T)\right)\oplus\left(W\cap \Ker \ p(T)\right).
\]
\end{itemize}
\end{lema}

\begin{proof}[Demonstração do Teorema]
Primeiro notemos que $m_T=p_1^{e_1}$ e $m_T=q_1^{f_1}$. Assim $p=q$ e $e_1=f_1$. Agora mostraremos que $m=n$. De acordo com o lema anterior, sendo $W=Ker p(T)$, então:
\[
W=\left(W\cap Z(u_1,T)\right)\oplus\dots\oplus\left(W\cap Z(u_m,T)\right)
\]
e também:
\[
W=\left(W\cap Z(v_1,T)\right)\oplus\dots\oplus\left(W\cap Z(v_n,T)\right).
\]
Como $p(T)[W]=0$, então $W$ é um espaço vetorial sobre o corpo $L=K[t]/p(t)K[t]$ e aí cada uma das duas decomposições expressa $W$ como uma soma direta de subespaços de dimensão $1$. Logo $m=\dim_L(W)=n$.

\medskip
\noindent
Finalmente mostraremos que os expoentes $e_i$ e $f_i$ são iguais usando indução em $e_1$. Se $e_1=1$, então $e_i=1$ para todo $i$ e como $f_1=e_1$, temos também $f_i=1$ para todo $i$. Suponhamos que o resultado seja válido para $e_1\leq k-1$ e seja $e_1=k$. Escreva:
\[
(e_1,\dots,e_n)=(e_1,\dots,e_s,1,\dots,1),\quad e_s>1
\]
e
\[
(f_1,\dots,f_n)=(f_1,\dots,f_t,1,\dots,1),\quad f_t>1.
\]
Então:
\[
p(T)[V]=p(T)[Z(u_1,T)]\oplus\dots\oplus p(T)[Z(u_m,T)]
\]
e
\[
p(T)[V]=p(T)[Z(v_1,T)]\oplus\dots\oplus p(T)[Z(v_n,T)],
\]
mas $p(T)[Z(v_1,T)]$ é um subespaço cíclico anulável por $p(T)^{e_1-1}$, aí pela hipótese de indução temos:
\[
s=t\quad\text{e}\quad e_1=f_1,\dots,e_s=f_s,
\]
concluindo assim a demonstração da unicidade.
\end{proof}

\subsection{Decomposição Cíclica em Divisores Elementares}

Agora podemos juntar a decomposição primária para minimais e a decomposição cíclica para espaços primários e obter a decomposição cíclica em divisores elementares.

\begin{teorema}[Decomposição Cíclica em Divisores Elementares]
Seja $V$ um espaço vetorial de \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$. Se o polinômio minimal é dado por:
\[
m_T=p_1^{e_1}\dots p_n^{e_n},
\]
em que os $p_i$ são polinômios irredutíveis mônicos distintos sobre $K$, então $V$ pode ser decomposto em uma soma direta:
\[
V=V_1\oplus\dots\oplus V_n,
\]
em que:
\[
V_i=\Ker \ p_i(T)^{e_i}
\]
é um subespaço $T$-invariante tal que $m_{T\upharpoonright_{V_i}}=p_i^{e_i}$. Finalmente, cada subespaço $T$-invariante $V_i$ pode ser escrito como uma soma direta de submódulos cíclicos, de modo que:
\[
V=\left(Z(v_{1,1},T)\oplus\dots\oplus Z(v_{1,k_1},T)\right)\oplus\left(Z(v_{n,1},T)\oplus\dots\oplus Z(v_{n,k_n},T)\right),
\]
em que $m_{T,v_{i,j}}=p_i^{e_{i,j}}$ e os termos de cada decomposição cíclica podem ser arranjados de modo que para cada $i$ tenhamos:
\[
e_i=e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i}.
\]

\end{teorema}

\noindent
Além disso, juntando os teoremas das unicidades da decomposição primária para minimais e da decomposição cíclica para espaços primários, então temos a unicidade da decomposição cíclica em divisores elementares.

\begin{teorema}[Unicidade da Decomposição Cíclica em Divisores Elementares]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que $V$ possa ser escrito como uma soma direta:
\[
V=\left(Z(u_{1,1},T)\oplus\dots\oplus Z(u_{1,k_1},T)\right)\oplus\dots\oplus\left(Z(u_{m,1},T)\oplus\dots\oplus Z(u_{m,k_m},T)\right)
\]
em que $m_{T,u_{i,j}}=p_i^{e_{i,j}}$ e:
\[
e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i},
\]
e suponhamos que $V$ possa ser escrito como uma soma direta:
\[
V=\left(Z(v_{1,1},T)\oplus\dots\oplus Z(v_{1,l_1},T)\right)\oplus\dots\oplus\left(Z(v_{n,1},T)\oplus\dots\oplus Z(v_{n,l_n},T)\right)
\]
em que $m_{T,v_{i,j}}=q_i^{f_{i,j}}$ e:
\[
f_{i,1}\geq f_{i,2}\geq\dots\geq f_{i,l_i}.
\]
Então:
\begin{itemize}
\item O número de somandos é o mesmo em ambas as decomposições; de fato,
$m=n$ e, depois de uma reindexação apropriada, $k_u=l_u$ para todo $u$.
\item Os subespaços primários são os mesmos; isto é, depois de uma reindexação apropriada, para todo $i$ temos $p_i=q_i$ e:
\[
Z(u_{i,1},T)\oplus\dots\oplus Z(u_{i,k_i},T)=Z(v_{i,1},T)\oplus\dots\oplus Z(v_{i,l_i},T).
\]
\end{itemize}
\end{teorema}

\begin{definicao}
O multiconjunto dos polinômios $p_i^{e_{i,j}}$ é unicamente determinado pela decomposição cíclica em divisores elementares, assim ele é chamado o multiconjunto dos \textbf{divisores elementares}.
\end{definicao}

\subsection{Decomposição Cíclica em Fatores Invariantes}

A forma mais usual de se apresentar uma decomposição em subespaços cíclicos é a decomposição cíclica em fatores invariantes.

\begin{teorema}[Decomposição Cíclica em Fatores Invariantes]
Seja $V$ um espaço vetorial de \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$. Então $V$ pode ser escrito como uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_r,T)
\]
em que $m_{T,v_i}=p_i$ e também:
\[
p_r\mid p_{r-1}\mid\dots\mid p_2\mid p_1.
\]
\end{teorema}

\begin{proof}
De acordo com a decomposição cíclica em divisores elementares, $V$ pode ser escrito como soma direta:
\[
V=\left(Z(u_{1,1},T)\oplus\dots\oplus Z(u_{1,k_1},T)\right)\oplus\dots\oplus\left(Z(u_{m,1},T)\oplus\dots\oplus Z(u_{m,k_m},T)\right)
\]
em que $m_{T,u_{i,j}}=p_i^{e_{i,j}}$ e:
\[
e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i},
\]
assim podemos combinar somandos cíclicos com polinômios minimais relativamente primos. Uma maneira de fazer isto é pegar o subespaço cíclico $Z(u_{i,1},T)$ de cada coleção:
\[
Z(u_{i,1},T)\oplus\dots\oplus Z(u_{i,k_i},T)
\]
para obtermos o seguinte:
\[
Z(v_1,T)=Z(u_{1,1},T)\oplus\dots\oplus Z(u_{m,1},T)
\]
e repetir o processo:
\[
Z(v_2,T)=Z(u_{1,2},T)\oplus\dots\oplus Z(u_{m,2},T)
\]
\[
Z(v_3,T)=Z(u_{1,3},T)\oplus\dots\oplus Z(u_{m,3},T)
\]
\[
\vdots
\]
É claro que alguns somandos podem estar faltando aqui pois os diferentes subespaços primários:
\[
Z(u_{i,1},T)\oplus\dots\oplus Z(u_{i,k_i},T)
\]
não necessariamente têm o mesmo número de somandos. De qualquer modo o resultado de rearranjarmos e combinarmos os somandos cíclicos é uma decomposição da forma:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_r,T).
\]
\end{proof}

\noindent
Por causa da unicidade da decomposição cíclica em divisores elementares, então temos a unicidade da decomposição cíclica em fatores invariantes

\begin{teorema}[Unicidade da Decomposição Cíclica em Fatores Invariantes]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que $V$ possa ser escrito como uma soma direta:
\[
V=Z(u_1,T)\oplus\dots\oplus Z(u_r,T)
\]
em que $m_{T,u_i}=p_i$ e também:
\[
p_r\mid p_{r-1}\mid\dots\mid p_2\mid p_1,
\]
e que $V$ possa ser escrito como uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_s,T)
\]
em que $m_{T,v_j}=q_j$ e também:
\[
q_s\mid q_{s-1}\mid\dots\mid q_2\mid q_1.
\]
Então $m=n$ e também $p_k=q_k$ para todo $k$.
\end{teorema}

\begin{definicao}
A sequência dos polinômios $p_i$ é unicamente determinado pela decomposição cíclica em fatores invariantes, assim ela é chamada a sequência dos \textbf{fatores invariantes}.
\end{definicao}

\section{Formas Racionais de Matrizes}

\subsection{Matrizes Companheiras}

\begin{definicao}
Definimos a \textbf{matriz companheira} do polinômio mônico:
\[
p(x)=x^n+a_{n-1}x^{n-1}+\dots+a_1x+a_0
\]
como a matriz:
\[
C[p(x)]=\begin{pmatrix}
0&0&\ldots&0&-a_0\\1&0&\ldots&0&-a_1\\0&1&\ddots&&\vdots\\\vdots&\vdots&\ddots&0&-a_{n-2}\\0&0&\ldots&1&-a_{n-1}
\end{pmatrix}
\]
\end{definicao}

\begin{teorema}
Dado polinômio $p(x)\in K[x]$, então temos:
\[
p_{C[p(x)]}(x)=m_{C[p(x)]}=p(x).
\]
Além disso, um espaço vetorial $V$ possui um vetor cíclico para $T\in\mathcal{L}(V)$ se e somente se $T$ pode ser representado por uma matriz companheira, e nesse caso a base representante é $T$-cíclica.
\end{teorema}
\begin{proof}
Veja o Exercício 15 da lista 2.
\end{proof}

\subsection{Forma Racional para Divisores Elementares}

Agora estamos prontos para determinar um conjunto de formas canônicas para similaridade entre matrizes. Faremos isto através das formas racionais. Há pelo menos dois tipos de formas racionais: a forma racional para divisores elementares e a forma racional para fatores invariantes. Primeiro apresentaremos as formas racionais para divisores elementares.

\begin{definicao}
Dizemos que uma matriz $A$ está na \textbf{forma racional para divisores elementares} se e só se $A$ é da forma:
\[
\begin{pmatrix}
C[r_1^{e_1}(x)]&&\\&\ddots&\\&&C[r_n^{e_n}(x)]
\end{pmatrix},
\]
em que os $r_i(x)$ são polinômios mônicos irredutíveis.
\end{definicao}

\begin{teorema}[Forma Racional para Divisores Elementares]
Cada classe de similaridade $S$ de matrizes contém uma matriz $A$ na forma racional para divisores elementares. Além disso, o conjunto das matrizes em $S$ que estão nessa forma
é o conjunto de matrizes obtidas de $A$ através de reordenação das matrizes companheiras no bloco diagonal.
\end{teorema}
\begin{proof}
Seja $V$ um espaço vetorial de \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$. Pela decomposição cíclica em divisores elementares, o espaço $V$ pode ser expressado pela seguinte soma direta:
\[
V=\left(Z(v_{1,1},T)\oplus\dots\oplus Z(v_{1,k_1},T)\right)\oplus\left(Z(v_{n,1},T)\oplus\dots\oplus Z(v_{n,k_n},T)\right),
\]
em que $m_{T,v_{i,j}}=p_i^{e_{i,j}}$ e também:
\[
e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i}.
\]
A concatenação $B$ das bases ordenadas:
\[
B_{i,j}=\left(v_{i,j},T(v_{i,j}),\dots,T^{d_{i,j}-1}(v_{i,j})\right),
\]
em que $d_{i,j}=\mathrm{deg}(p_i)\cdot e_{i,j}$, é uma base ordenada tal que:
\[
[T]_B=\begin{pmatrix}
C[p_1^{e_{1,1}}(x)]&&&&&&\\&\ddots&&&&&\\&&C[p_1^{e_{1,k_1}}(x)]&&&&\\&&&\ddots&&&\\&&&&C[p_n^{e_{n,1}}(x)]&&\\&&&&&\ddots&\\&&&&&&C[p_n^{e_{n,k_n}}(x)].
\end{pmatrix}
\]
O restante do enunciado pode ser demonstrada tendo em mente a unicidade da decomposição cíclica em divisores elementares.
\end{proof}

\subsection{Forma Racional para Fatores Invariantes}

Agora apresentaremos a forma racional para fatores invariantes, que é a forma racional usual da álgebra linear. Portanto, se alguém pedir que representemos uma matriz na forma racional, muito provavelmente está pedindo a forma racional para fatores invariantes.

\begin{definicao}
Dizemos que uma matriz $A$ está na \textbf{forma racional para fatores invariantes} se e só se $A$ é uma matriz da forma:
\[
\begin{pmatrix}
C[s_1(x)]&&\\&\ddots&\\&&C[s_n(x)]
\end{pmatrix},
\]
em que $s_{k+1}(x)\mid s_k(x)$ para $k=1,\dots,n-1$.
\end{definicao}

\begin{teorema}[Forma Racional para Fatores Invariantes]
Cada classe de similaridade $S$ de matrizes contém uma matriz $A$ na forma racional para fatores invariantes. Além disso, o conjunto das matrizes em $S$ que estão nessa forma
é o conjunto de matrizes obtidas de $A$ através de reordenação das matrizes companheiras no bloco diagonal.
\end{teorema}

\begin{lema}
Se $p(x),q(x)$ são polinômios mônicos primos entre si, então:
\[
C[p(x)q(x)]\sim\begin{pmatrix}
C[p(x)]&\\&C[q(x)]
\end{pmatrix}.
\]
\end{lema}
\begin{proof}[Demonstração do Lema]
Se uma matriz $A$ tem polinômio minimal:
\[
m_T=p_1^{e_1}\dots p_n^{e_n}
\]
de grau igual ao tamanho da matria, então o teorema das formas racionais para divisores elementares implica que os divisores elementares de $A$ são precisamente:
\[
p_1^{e_1},\dots,p_n^{e_n}.
\]
Como as matrizes $C[p(x)q(x)]$ e $C[p(x)]\oplus C[q(x)]$ têm o mesmo tamanho $m$ e o mesmo polinômio minimal $p(x)q(x)$ de grau $m$, segue que eles têm o mesmo multiconjunto dos divisores elementares e assim são similares.
\end{proof}

\begin{proof}[Demonstração do Teorema]
O lema pode ser usado para rearranjar e combinar as matrizes companheiras numa matriz $A$ em forma racional para divisores elementares para produzir uma matriz em forma racional para fatores invariantes que seja similar a $A$. Além disso, é fácil ver que este processo é reversível.
\end{proof}

\noindent
Finalizaremos esta seção mostrando uma aplicação das formas racionais para divisores elementares.

\begin{teorema}
O polinômio característico de uma matriz é igual ao produto de seus divisores elementares.
\end{teorema}
\begin{proof}
Seja $A$ uma matriz, então ela é similar a uma matriz na forma racional para divisores elementares:
\[
\begin{pmatrix}
C[r_1(x)]&&\\&\ddots&\\&&C[r_n(x)]
\end{pmatrix}.
\]
Sabemos que o polinômio característico de $C[r_i(x)]$ é igual a $r_i(x)$, assim o resultado segue.
\end{proof}

\section{Formas de Jordan de Matrizes}

\subsection{Forma de Jordan Usual}

As formas racionais, seja para divisores elementares ou para fatores invariantes, têm a façanha de que todo operador linear num espaço vetorial de dimensão finita tem uma forma racional canônica, seja para divisores elementares ou para fatores invariantes. Porém, as formas racionais podem estar longes do ideal de simplicidade que tínhamos em mente para um conjunto de formas canônicas fáceis de apresentar, assim servem mais como ferramentas teóricas do que práticas.

\medskip
\noindent
Quando o polinômio minimal $m_T(x)$ de $T$ se decompõe sobre $K$, ou seja:
\[
m_T(x)=(x-c_1)^{r_1}\dots(x-c_k)^{r_k},
\]
então existe uma outra forma canônica que é bem mais fácil de apresentar do que a forma racional.

\medskip
\noindent
De algum modo, a complexidade das formas racionais vem da escolha da base para os subespaços $T$-invariantes cíclicos $Z(v_{i,j},T)$. Recordemos que as bases $T$-cíclicas
têm a forma:
\[
B_{i,j}=(v_{i,j},T(v_{i,j}),\dots,T^{d_{i,j}-1}(v_{i,j})),
\]
em que $d_{i,j}=\mathrm{deg}(p_i)\cdot(e_{i,j})$. Com essa base, toda complexidade no final quando tentamos expressar:
\[
T(T^{d_{i,j}-1}(v_{i,j}))=T^{d_{i,j}}(v_{i,j})
\]
como combinação linear dos vetores da base.

\medskip
\noindent
No entanto, como $B_{i,j}$ tem a forma:
\[
(v,T(v),T^2(v),\dots,T^{d-1}(v)),
\]
então qualquer sequência da forma:
\[
(p_0(T)(v),p_1(T)(v),\dots,p_{d-1}(T)(v))
\]
em que $\mathrm{deg}(p_k)=k$ será uma base de $Z(v_{i,j},T)$. Em particular, quando $m_T(x)$ se decompõe sobre $K$, os divisores elementares são:
\[
p_i^{e_{i,j}}(x)=(x-c_i)^{e^{i,j}}
\]
e assim a sequência:
\[
C_{i,j}=(v_{i,j},(T-c_iI)(v_{i,j}),\dots,(T-c_iI)^{e_{i,j}-1}(v_{i,j}))
\]
é também uma base de $Z(v_{i,j},T)$.

\medskip
\noindent
Se temporariamente denotarmos o $k$-ésimo vetor da base $C_{i,j}$ por $b_k$, então para $k=0,\dots,e_{i,j}-2$ temos:
\[
\begin{array}{rcl}
T(b_k)&=&T((T-c_iI)^k(v_{i,j}))\\
&=&(T-c_iI+c_iI)((T-c_iI)^k(v_{i,j}))\\
&=&(T-c_iI)^{k+1}(v_{i,j})+c_i(T-c_i)^k(v_{i,j})\\
&=&b_{k+1}+c_ib_k.
\end{array}
\]
Para $k=e_{i,j}-1$, usando o fato de que:
\[
(T-c_iI)^{k+1}(v_{i,j})=(T-c_iI)^{e_{i,j}}(v_{i,j})=0,
\]
obtemos:
\[
T(b_{e_{i,j}-1})=c_ib_{e_{i,j}-1}.
\]
Logo, para esta base, a complexidade se espalha melhor, e a matriz de $T\upharpoonright_{Z(v_{i,j},T)}$ em relação a $C_{i,j}$ é a matriz de tamanho $e_{i,j}$:
\[
\begin{pmatrix}
c_i&0&\ldots&\ldots&0\\1&c_i&\ddots&&\vdots\\0&1&\ddots&\ddots&\vdots\\\vdots&\ddots&\ddots&\ddots&0\\0&\ldots&0&1&c_i
\end{pmatrix}.
\]
Assim apresentaremos o conceito das formas de Jordan.

\begin{definicao}
Um \textbf{bloco de Jordan} é uma matriz do tipo:
\[
J(c,n)=\begin{pmatrix}
c&0&\ldots&\ldots&0\\1&c&\ddots&&\vdots\\0&1&\ddots&\ddots&\vdots\\\vdots&\ddots&\ddots&\ddots&0\\0&\ldots&0&1&c
\end{pmatrix},
\]
em que $c\in K$ e a matriz tem tamanho $n$. Dizemos que uma matriz está na \textbf{forma de Jordan} se e só se é uma soma direta de blocos de Jordan.
\end{definicao}

\noindent
Com a discussão anterior, então temos as conhecidas formas de Jordan.

\begin{teorema}[Forma de Jordan]
Suponhamos que o polinômio minimal de $T\in\mathcal{L}(V)$ se decomponha sobre o corpo $K$, ou seja:
\[
m_T(x)=(x-c_1)^{e_1}\dots(x-c_n)^{e_n},
\]
em que $c_i\in K$. Então existe uma base $B$ tal que a matriz de $T$ seja:
\[
\begin{pmatrix}
J(c_1,e_{1,1})&&&&&&\\&\ddots&&&&&\\&&J(c_1,e_{1,k_1})&&&&\\&&&\ddots&&&\\&&&&J(c_n,e_{n,1})&&\\&&&&&\ddots&\\&&&&&&J(c_n,e_{n,k_n})
\end{pmatrix},
\]
em que os polinômios $(x-c_i)^{e_{i,j}}$ são os divisores elementares de $T$.
\end{teorema}

\begin{corolario}
Se $K$ é algebricamente fechado, então, a menos de ordem dos blocos na diagonal, o conjunto de matrizes na forma de Jordan constitui um conjunto de formas canônicas para a similaridade de matrizes.
\end{corolario}

\subsection{Forma de Jordan Generalizada}

\subsection{Forma de Jordan Real}

\section{Espaços Indecomponíveis}

\subsection{Espaços Cíclicos}

A decomposição cíclica em divisores elementares pode ser usada para caracterizar espaços vetoriais de dimensão finita com vetores cíclicos através de seus divisores elementares.

\begin{teorema}[Caracterização de Espaços Cíclicos]
Seja $V$ um espaço vetorial de dimensão finita e seja $T\in\mathcal{L}(V)$. Consideremos o polinômio minimal:
\[
m_T=p_1^{e_1}\dots p_n^{e_n}.
\]
As seguintes propriedades são equivalentes:
\begin{itemize}
\item[1)] $V$ possui um vetor cíclico para $T$.
\item[2)] $V$ é uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_n,T)
\]
de subespaços cíclicos primários $V_i=Z(v_i,T)$ tais que $m_{T\upharpoonright{V_i}}=p_i^{e_i}$.
\item[3)] Os divisores elementares de $V$ são precisamente:
\[
p_1^{e_1},\dots,p_n^{e_n}.
\]
\end{itemize}
\end{teorema}
\begin{proof}
Suponhamos que $V$ tenha vetor cíclico para $T$. Então a decomposição primária para minimais de $V$ é uma decomposição cíclica, pois pelo Exercício 1 da Lista 4 todo subespaço $T$-invariante de um espaço com vetor cíclico também tem um vetor cíclico. Logo, (1) implica (2). Reciprocamente, se (2) ocorre, então como os polinômios minimais são relativamente primos, o Teorema \ref{ciclico} implica que $V$ tem um vetor cíclico. Deixamos o restante da prova para o leitor.
\end{proof}

\subsection{Espaços Indecomponíveis}

A decomposição cíclica em divisores elementares é uma decomposição de $V$ numa soma dureta de subespaços $T$-invariantes que não podem ser decompostos em pedaços menores. De fato, isto caracteriza a decomposição cíclica em divisores elementares de $V$. Antes de justificar estas afirmações, apresentamos a seguinte definição.

\begin{definicao}
Um espaço vetorial $V$ é dito \textbf{indecomponível} para um $T\in\mathcal{L}(V)$ se e só se não puder ser escrito como uma soma direta de dois subespaços $T$-invariantes próprios.
\end{definicao}

\begin{teorema}[Caracterização de Espaços Indecomponíveis]
Seja $V$ um espaço vetorial de dimensão finita e seja $T\in\mathcal{L}(V)$. Então as seguinte propriedades são equivalentes:
\begin{itemize}
\item[1)] $V$ é indecomponível para $T$.
\item[2)] $V$ é um espaço primário e tem um vetor cíclico para $T$.
\item[3)] $V$ tem apenas um divisor elementar para $T$.
\end{itemize}
\end{teorema}

\noindent
Portanto, a decomposição cíclica em divisores elementares é uma decomposição de $V$ em uma soma direta de subespaços $T$-invariantes indecomponíveis para $T$. Reciprocamente, se:
\[
V=V_1\oplus\dots\oplus V_n
\]
é uma decomposição de $V$ numa soma direta de subespaços $T$-invariantes indecomponíveis para $T$, então cada subespaço $T$-invariante $V_i$ é primário e possui vetor cíclico, aí esta decomposição é a decomposição cíclica em divisores elementares de $V$.

\subsection{Teorema de Cauchy}

Quem já está familiarizado com teoria dos grupos sabem que todo grupo de ordem prima é cíclico. Entretanto, existem espaços com polinômio minimal irredutível que no entanto não possuem vetores cíclicos. Não obstante, espaços com polinômio minimal irredutível com vetores cíclicos são importantes.

\medskip
\noindent
De fato, se $V$ é um espaço vetorial de dimensão finita e $T\in\mathcal{L}(V)$, com polinômio minimal $m_T$, então cada fator primo de $p$ de $m_T$ nos oferece um subespaço $T$-invariante $W$ de $V$ com vetor cíclico cujo polinômio minimal é $p$ e aí $W$ é também indecomponível para $T$.
Infelizmente, $W$ não necessariamente tem um complemento $T$-invariante e aí não podemos usá-lo
para decompor $V$. Não obstante, o teorema ainda é útil.

\begin{teorema}[Teorema de ``Cauchy'' para Álgebra Linear]
Seja $V$ um espaço vetorial de dimensão finita e $T\in\mathcal{L}(V)$, com polinômio minimal $m_T$. Se $p$ é um divisor primo de $m_T$, então $V$ tem um subespaço $T$-invariante $W$ com vetor cíclico com $T$-anulador $p$.
\end{teorema}
\begin{proof}
Se $m_T=pq$, então existe um $v\in V$ tal que $w\coloneqq q(T)(v)\neq 0$ mas $p(T)(w)=0$.
Assim $W=Z(w,T)$ é anulado por $p(T)$ e assim $m_{T,w}\mid p$. Mas $p$ é primo e
$m_{T,w}\neq 1$, aí $m_{T,w}=p$.
\end{proof}

\chapter{Espaços com Produto Interno}

Neste capítulo, convencionaremos que $K$ é $\mathbb{R}$ ou $\mathbb{C}$.

\section{Definições e Exemplos}

\begin{definicao}
Um espaço vetorial $V$ é dito um \textbf{espaço com produto interno} se estiver munida com uma função:
\[
\begin{array}{rcl}
V\times V&\rightarrow&K\\
(u,v)&\mapsto&\langle u,v\rangle
\end{array}
\]
tal que:
\begin{itemize}
\item $\langle\alpha u+\beta v,w\rangle=\alpha\langle u,w\rangle+\beta\langle v,w\rangle$ para quaisquer $\alpha,\beta\in K$ e $u,v,w\in V$.
\item $\overline{\langle u,v\rangle}=\langle v,u\rangle$ para quaisquer $u,v\in V$.
\item $\langle v,v\rangle\geq 0$ para $v\in V$.
\item $\langle v,v\rangle=0\Rightarrow v=0$ para $v\in V$.
\end{itemize}
\end{definicao}

\begin{exemplo}
O exemplo principal é $V=K^n$, em que, tomando $u=(\alpha_1,\dots,\alpha_n)$ e $v=(\beta_1,\dots,\beta_n)$, definimos:
\[
\langle u,v\rangle=\alpha_1\overline{\beta_1}+\dots+\alpha_n\overline{\beta_n}.
\]
\end{exemplo}

\begin{exemplo}
Podemos considerar $V=\m{n}{K}\cong K^{n^2}$. Com base no exemplo anterior, definimos:
\[
\langle A,B\rangle=\sum_{i,j}a_{i,j}\overline{b_{i,j}}.
\]
Para toda $A=(a_{i,j})\in\m{n}{K}$, definimos:
\[
A^*=(\overline{a_{i,j}}),
\]
e
\[
\mathrm{tr}(A)=a_{1,1}+\dots+a_{n,n},
\]
então podemos ver que:
\[
\langle A,B\rangle=\mathrm{tr}(AB^*).
\]
\end{exemplo}

\begin{exemplo}
Podemos tomar $V=\mathcal{C}[a,b]$, o conjunto das funções contínuas de $[a,b]$ em $\mathbb{C}$, e para $f,g\in V$ definirmos:
\[
\langle f,g\rangle=\int_a^bf(t)\overline{g(t)}\dif t.
\]
\end{exemplo}

\begin{exemplo}
Seja $T\in\mathcal{L}(U,V)$ bijetora. Suponhamos que $V$ tenha um produto interno. Se $u_1,u_2\in U$, definimos:
\[
\langle u_1,u_2\rangle_U=\langle T(u_1),T(u_2)\rangle_V.
\]
Suponhamos que $U$ tenha uma base $B=(e_1,\dots,e_n)$. Então existe uma $T:U\rightarrow K^n$ tal que:
\[
T(e_i)=(0,\dots,1,\dots,0),
\]
em que a entrada $i$ vale $1$ e as outras valem $0$. Sendo:
\[
u=\alpha_1e_1+\dots+\alpha_ne_n,\quad v=\beta_1e_1+\dots+\beta_ne_n,
\]
então:
\[
\langle u,v\rangle_U=\langle T(u),T(v)\rangle_V=\sum_{i=1}^n\alpha_i\overline{\beta_i}.
\]
\end{exemplo}

\begin{definicao}
Definimos o seguinte:
\begin{itemize}
\item Se $\dim(V)=n<\infty$ e $K=\mathbb{R}$, então chamamos $V$ de \textbf{euclidiano}.
\item Se $\dim(V)=n<\infty$ e $K=\mathbb{C}$, então chamamos $V$ de \textbf{unitário}.
\end{itemize}
\end{definicao}

\begin{definicao}
Se $V$ é um espaço com produto interno e $V$ é um espaço completo em relação à norma $\norm{v}=\sqrt{\langle v,v\rangle}$, então $V$ se chama um \textbf{espaço de Hilbert}.
\end{definicao}

\section{Matriz de Gramm}

\begin{definicao}
Seja $V$ um espaço com produto interno. Sejam $v_1,\dots,v_k\in V$. Definimos:
\[
G(v_1,\dots,v_n)=\begin{pmatrix}
\langle v_1,v_1\rangle&\dots&\langle v_1,v_n\rangle\\\dots&\dots&\dots\\\langle v_n,v_1\rangle&\dots&\langle v_n,v_n\rangle
\end{pmatrix}.
\]
Nesse caso temos:
\[
G(v_1,\dots,v_n)^*=G(v_1,\dots,v_n).
\]
Toda matriz $A$ que satisfaz $A^*=A$ é chamada \textbf{Hermitiana}.
\end{definicao}

\begin{proposicao}
Seja $B=(e_1,\dots,e_n)$ uma base de $V$. Então $G(e_1,\dots,e_n)$ é inversível.
\end{proposicao}
\begin{proof}
Primeiro observemos que para quaisquer $u,v\in V$, com:
\[
[u]_B=(\alpha_1,\dots,\alpha_n),\quad [v]_B^*=(\beta_1,\dots,\beta_n)
\]
então:
\[
\langle u,v\rangle=[u]_BA[v]_B^*.
\]
De fato:
\[
\langle u,v\rangle=\sum_{i,j}\alpha_i\overline{\beta_j}\langle e_i,e_j\rangle,
\]
aí:
\[
\begin{array}{rcl}
[u]_BA[v]_B^*&=&(\alpha_1,\dots,\alpha_n)\begin{pmatrix}
\langle e_1,e_1\rangle&\dots&\langle e_1,e_n\rangle\\\dots&\dots&\dots\\\langle e_n,e_1\rangle&\dots&\langle e_n,e_n\rangle
\end{pmatrix}\begin{pmatrix}
\overline{\beta_1}\\\vdots\\\overline{\beta_n}
\end{pmatrix}\\
&=&\begin{pmatrix}
\alpha_1\langle e_1,e_1\rangle+\dots+\alpha_n\langle e_n,e_1\rangle\\\vdots\\\alpha_1\langle e_1,e_n\rangle+\dots+\alpha_n\langle e_n,e_n\rangle
\end{pmatrix}\begin{pmatrix}
\overline{\beta_1}\\\vdots\\\overline{\beta_n}
\end{pmatrix}
\end{array}.
\]
Se $A$ não for inversível, então existe $u\neq 0$ tal que $[u]_BA=0$. Neste caso, $\langle u,u\rangle=[u]_BA[u]_B^*=0$, aí $u=0$, contradição.
\end{proof}

\noindent
Se $B=(e_1,\dots,e_n)$ é uma base de $V$ e $A=G(e_1,\dots,e_n)$ então para todo $v\in V$ temos:
\[
[v]_BA[v]_B^*\geq 0
\]
e a igualdade ocorre se e somente se $v=0$.

\begin{definicao}
Uma matriz Hermitiana $A$ é dita:
\begin{itemize}
\item \textbf{Positiva semidefinitiva} se e só se para todo $x\in K^n$ temos $xAx^*\geq 0$.
\item \textbf{Positiva definitiva} se e só se para todo $x\in K^n$ tal que $x\neq 0$ temos $xAx^*>0$.
\end{itemize}
\end{definicao}

\begin{proposicao}
Seja $A\in\m{n}{K}$ uma matriz Hermitiana positiva definitiva. Seja $V$ um espaço vetorial com base $B=(e_1,\dots,e_n)$. Definamos, para $u,v\in V$, o seguinte:
\[
\langle u,v\rangle=[u]_BA[v]_B^*.
\]
Então $\langle u,v\rangle$ é um produto interno.
\end{proposicao}

\section{Espaços Normados}

\begin{definicao}
Um espaço vetorial $V$ é dito um \textbf{espaço normado} se estiver munido com uma função:
\[
\begin{array}{rcl}
V&\rightarrow&\mathbb{R}\\
v&\mapsto&\norm{v}
\end{array}
\]
que satisfaça as seguintes propriedades:
\begin{itemize}
\item $\norm{v}\geq 0$ para $v\in V$. 
\item $\norm{v}=0\Rightarrow v=0$ para $v\in V$.
\item $\norm{\alpha v}=\abs{\alpha}\norm{v}$ para $\alpha\in K$ e $v\in V$.
\item $\norm{u+v}\leq\norm{u}+\norm{v}$ para $u,v\in V$.
\end{itemize}
\end{definicao}

\begin{proposicao}[Desigualdade de Cauchy-Bunyakovski-Schwarz]
Seja $V$ um espaço com produto interno. Então para quaisquer $u,v\in V$ temos:
\[
\abs{\langle u,v\rangle}\leq\norm{u}\cdot\norm{v}.
\]
\end{proposicao}
\begin{proof}
Se $v=0$, então é fácil. Se $v\neq 0$, então $\norm{v}>0$, aí para quaisquer $\alpha,\beta\in K$ temos:
\[
\begin{array}{rcl}
0&\leq&\langle\alpha u-\beta v,\alpha u-\beta v\rangle\\
&=&\alpha\overline{\alpha}\langle u,u\rangle+\beta\overline{\beta}\langle v,v\rangle-\alpha\overline{\beta}\langle u,v\rangle-\beta\overline{\alpha}\langle v,u\rangle\\
&=&\abs{\alpha}^2\norm{u}^2+\abs{\beta}^2\norm{v}^2-\left(\alpha\overline{\beta}\langle u,v\rangle+\overline{\alpha\overline{\beta}\langle u,v\rangle}\right).
\end{array}
\]
Em particular, fazendo $\alpha=\norm{v}$ e $\beta=\langle u,v\rangle$, então:
\[
\begin{array}{rl}
&\norm{v}^4\norm{u}^2+\abs{\langle u,v\rangle}^2\norm{v}^2-2\norm{v}^2\langle u,v\rangle^2\geq0\\
\Rightarrow&\norm{v}^2\norm{u}^2-\abs{\langle u,v\rangle}^2\geq 0\\
\Rightarrow&\left(\norm{v}\norm{u}\right)^2\geq\abs{\langle u,v\rangle}^2.
\end{array}
\]
\end{proof}

\begin{proposicao}
Seja $V$ um espaço com produto interno e definamos $\norm{v}=\sqrt{\langle v,v\rangle}$. Então $\norm{v}$ é uma norma.
\end{proposicao}
\begin{proof}
Provaremos apenas a última propriedade requerida para espaço normado, deixando as outras para o leitor. Sabemos que para $z\in\mathbb{C}$ então $z+\overline{z}\leq 2\abs{z}$. De fato, sendo $z=a+bi$, então $\abs{z}=\sqrt{a^2+b^2}\geq a$, aí $z+\overline{z}=2a\leq 2\abs{z}$. Agora temos:
\[
\begin{array}{rcl}
\abs{u+v}^2&=&\langle u+v,u+v\rangle\\
&=&\norm{u}^2+\norm{v}^2+\left(\langle u,v\rangle+\overline{\langle u,v\rangle}\right)\\
&\leq&\norm{u}^2+\norm{v}^2+2\abs{\langle u,v\rangle}\\
&\leq&\norm{u}^2+\norm{v}^2+2\norm{u}\cdot\norm{v}\\
&=&\left(\norm{u}+\norm{v}\right)^2.
\end{array}
\]
\end{proof}

\section{Ortogonalidade}

\begin{definicao}
Dois vetores $u,v\in V$ são ditos \textbf{ortogonais} se e só se $\langle u,v\rangle=0$.

\smallskip
\noindent
Uma família $(v_i)_{i\in I}$ de vetores é chamado \textbf{ortogonal} se e só se para quaisquer $i,j\in I$ tais que $i\neq j$ os vetores $v_i$ e $v_j$ forem ortogonais.

\smallskip
\noindent
Uma família $(v_i)_{i\in I}$ de vetores é chamado \textbf{ortonormal} se e só se é ortogonal e para todo $i\in I$ temos $\norm{v_i}=1$.
\end{definicao}

\begin{proposicao}
Se $(v_i)_{i\in I}$ é uma família ortogonal de vetores \emph{não nulos}, então a família é L.I.
\end{proposicao}
\begin{proof}
Para conjunto finito $J\subseteq I$ e para $\alpha:J\rightarrow I$, se:
\[
\sum_{i\in J}\alpha_iv_i=0,
\]
então para $j\in J$ temos:
\[
0=\left\langle\sum_{i\in J}\alpha_iv_i,v_j\right\rangle=\sum_{i\in J}\alpha_i\langle v_i,v_j\rangle=\alpha_j\norm{v_j}^2,
\]
mas $v_j\neq 0$, aí $\norm{v_j}^2\neq 0$, aí $\alpha_j=0$.
\end{proof}

\begin{proposicao}[Ortogonalização de Gramm-Schmidt]
Para toda sequência linearmente independente $(v_1,\dots,v_k)$ de vetores, existe uma sequência ortogonal $(u_1,\dots,u_k)$ tal que $\langle v_1,\dots,v_k\rangle=\langle u_1,\dots,u_k\rangle$.
\end{proposicao}
\begin{proof}
Indução sobre $k$.
\end{proof}

\begin{corolario}
Todo espaço com produto interno de dimensão finita tem uma base ortonormal.
\end{corolario}

\section{Funcionais Lineares}

\begin{proposicao}
Seja $V$ um espaço com produto interno. Para $u\in V$, definimos $\varphi_u:V\rightarrow K$ assim:
\[
\varphi_u(v)=\langle v,u\rangle.
\]
Então $\varphi_u$ é um funcional linear.
\end{proposicao}

\begin{teorema}[Teorema de Riesz]
Se $\dim(V)<\infty$, então para todo $f\in V^*$ existe $u\in V$ tal que $f=\varphi_u$.
\end{teorema}
\begin{proof}
Seja $f\in V^*$. Escolhemos uma base ortonormal $B=(e_1,\dots,e_n)$ em $V$ e seja $\alpha_i=f(e_i)\in K$ para $i=1,\dots,n$. Consideremos $u=\overline{\alpha_1}e_1+\dots+\overline{\alpha_n}e_n.$ Então para $k=1,\dots,n$ temos:
\[
\begin{array}{rcl}
\varphi_u(e_k)=\langle e_k,u\rangle&=&\langle e_k,\overline{\alpha_1}e_1+\dots+\overline{\alpha_n}e_n\rangle\\
&=&\alpha_k\langle e_k,e_k\rangle=\alpha_k=f(e_k),
\end{array}
\]
ou seja, $\varphi_u(e_k)=f(e_k)$; logo $\varphi_u=f$.
\end{proof}

\begin{observacao}
O teorema de Riesz não é válido para espaços com produto interno de dimensão finita. De fato, se $V=\mathcal{C}[a,b]$, então seja $x_0\in[a,b]$ e seja $\varphi\in V^*$ dada por:
\[
\varphi(f)=f(x_0).
\]
\end{observacao}

\begin{definicao}
Agora seja $V$ um espaço com produto interno e $S$ um subconjunto de $V$. Definimos o conjunto:
\[
S^\perp=\{v\in V:\mid \forall s\in S:\langle v,s\rangle=0\},
\]
e chamamos de \textbf{complemento ortogonal} de $S$.
\end{definicao}

\begin{proposicao}
Para qualquer subconjunto $S$ temos:
\begin{itemize}
\item[1)] $S^\perp$ é um subespaço de $V$.
\item[2)] Se $W\subseteq V$ é um subespaço, então $V=W\oplus W^\perp$.
\item[3)] Se $W\subseteq V$ é um subespaço, então $W^{\perp\perp}=W$.
\end{itemize}
\end{proposicao}
\begin{proof}
\begin{itemize}
\item[2)] Escolhemos em $W$ uma base ortonormal $(v_1,\dots,v_k)$. Seja $u\in V$ arbitrário. Consideremos:
\[
\tilde{u}=\langle u,v_1\rangle v_1+dots+\langle u,v_k\rangle v_k\in W.
\]
Provemos que $u-\tilde{u}\in W^\perp$. Notemos que $W^\perp=\{v_1,\dots,v_k\}^\perp$. Para $i=1,\dots,k$, então temos:
\[
\begin{array}{rcl}
\langle u-\tilde{u},v_i\rangle&=&\langle u,v_i\rangle-\langle\sum_{j=1}^k\langle u,v_j\rangle,v_i\rangle\\
&=&\langle u,v_i\rangle-\sum_{j=1}^k\langle u,v_i\rangle\langle v_j,v_i\rangle\\
&=&\langle u,v_i\rangle-\langle u,v_i\rangle\langle u,v_i\rangle\\
&=&0;
\end{array}
\]
logo $u-\tilde{u}\in W^\perp$, aí $u=\tilde{u}+(u-\tilde{u})\in W+W^\perp$. Para $v\in W\cap W^\perp$, então $\langle u,u\rangle=0$, aí $u=0$.
\item[3)] Temos $V=W\oplus W^\perp=W^\perp\oplus W^{\perp\perp}$, aí:
\[
\dim(W)=\dim(V)-\dim(W^\perp)=\dim(W^{\perp\perp}),
\]
mas é fácil ver que $W\subseteq W^{\perp\perp}$, aí $W=W^{\perp\perp}$.
\end{itemize}
\end{proof}

\begin{definicao}
Para subespaço $W$ de $V$, definimos a \textbf{projeção ortogonal} como a função $E_W:V\rightarrow W$ tal que, para $v\in V$ tenhamos $v-E_W(v)\in W^\perp$.
\end{definicao}

\begin{proposicao}
Para subespaço $W$ e para $v\in V$, temos:
\[
\norm{v-E_W(v)}=\min\{\norm{v-w}\mid w\in W\}.
\]
\end{proposicao}
\begin{proof}
Seja $v^\perp=v-E_W(v)$. Então para $w\in W$ temos:
\[
v-w=v^\perp+E_W(v)-w,
\]
mas $v^\perp\in W^\perp$ e $E_W(v)-w\in W$, aí:
\[
\norm{v-w}^2=\norm{v^\perp}^2+\norm{E_W(v)-w}^2\geq\norm{v^\perp}^2=\abs{v-E_W(v)}^2.
\]
\end{proof}

\begin{teorema}[Desigualdade de Bessel]
Sejam $v_1,\dots,v_k\in V$ vetores não nulos mutuamente ortogonais. Então para todo $v\in V$ temos:
\[
\sum_{i=1}^k\frac{\abs{\langle v,v_i\rangle}^2}{\norm{v_i}^2}\leq\norm{v}^2.
\]
\end{teorema}
\begin{proof}
\textcolor{red}{Seilah}
\end{proof}

\section{Transformação Adjunta}

\begin{definicao}
Sejam $U$ e $V$ espaços com produto interno de dimensão finita e $T\in\mathcal{L}(U,V)$. Uma \textbf{transformação adjunta} para $T$ é uma função $T^*\in\mathcal{L}(V,U)$ tal que para quaisquer $u\in U$ e $v\in V$ tenhamos:
\[
\langle T(u),v\rangle_V=\langle u,T^*(v)\rangle_U.
\]
\end{definicao}

\begin{teorema}
$T^*$ sempre existe e é única.
\end{teorema}
\begin{proof}
\textcolor{red}{Preencher os detalhes.}

\medskip
\noindent
Seja $v\in V$. Consideremos a função:
\[
\varphi_v:u\mapsto\langle T(u),v\rangle\in K.
\]
Então $\varphi_v\in U^*$. Assim, pelo teorema de Riesz, então existe um único $T^*(v)\in U$ tal que:
\[
\forall u\in U:\varphi_v(u)=\langle u,T^*(v)\rangle.
\]
Consideremos a função:
\[
T^*:v\mapsto T^*(v).
\]
É fácil ver que $T^*\in\mathcal{L}(V,U)$.
\end{proof}

\begin{teorema}
Sejam $B=(e_1,\dots,e_n)$ e $C=(f_1,\dots,f_m)$ bases ortonormais de $U$ e $V$, e seja $T\in\mathcal{L}(U,V)$. Se $A=[T]_{B,C}$, então $A^*=[T^*]_{C,B}$.
\end{teorema}
\begin{proof}
\textcolor{red}{Seilah}
\end{proof}

\begin{observacao}
Por outro lado, consideremos $V=\mathbb{R}[t]$, e consideremos o produto interno:
\[
\langle f,g\rangle=\int_a^bf(t)g(t)\dif t.
\]
Consideremos o operador derivação $D=\mathcal{L}(V)$ dado por:
\[
\forall f\in\mathbb{R}:D(f)=f'.
\]
Então $D$ não tem um adjunto.
\end{observacao}

\begin{proposicao}
Sejam $U$ e $V$ e $W$ espaços com produto interno de dimensão finita e sejam $T,S\in\mathcal{L}(U,V)$ e $P\in\mathcal{L}(V,W)$ e $\alpha\in K$. Então:
\begin{itemize}
\item $(T+S)^*=T^*+S^*.$
\item $(\alpha T)^*=\overline{\alpha}T^*.$
\item $(TP)^*=P^*T^*.$
\item $T^{**}=T.$
\end{itemize}
\end{proposicao}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{definicao}
Um operador $L\in\mathcal{L}(V)$ se chama \textbf{autoadjunto}, ou também \textbf{Hermitiano}, se e só se $T^*=T.$
\end{definicao}

\noindent
Agora seja $T\in\mathcal{L}(U,V)$ e $b\in V$. Queremos encontrar $x\in U$ que dê a melhor aproximação da equação:
\[
T(x)=b,
\]
ou seja, queremos encontrar $x\in U$ tal que a distância:
\[
\norm{T(x)-b}
\]
seja \emph{mínima}. Isso significa encontrarmos um $x\in U$ tal que a projeção ortogonal de $b$ em $\mathrm{Im}(T)$ seja $T(x)$. Temos de encontrar $x\in U$ tal que para todo $v\in\mathrm{Im}(T)$ tenhamos:
\[
\langle T(x)-b,v\rangle=0;
\]
ou seja,
\[
T(x)-b\in(\mathrm{Im}(T))^\perp.
\]
Seja $(e_1,\dots,e_n)$ uma base de $U$. Então:
\[
\begin{array}{rcl}
T(x)-b\in(\mathrm{Im}(T))^\perp&\Leftrightarrow&\forall i=1,\dots,n:\langle T(x)-b,T(e_i)\rangle=0\\
&\Leftrightarrow&\forall i=1,\dots,n:\langle T(x),T(e_i)\rangle=\langle b,T(e_i)\rangle\\
&\Leftrightarrow&\forall i=1,\dots,n:\langle T^*T(x),e_i\rangle=\langle T^*(b),e_i\rangle\\
&\Leftrightarrow& T^*T(x)=T^*(b).
\end{array}
\]

\medskip
\noindent
\textcolor{red}{EXEMPLO NUMÉRICO QUE ESTOU COM PREGUIÇA DE ESCREVER PORQUE É CHATO DEMAIS}

\section{Operadores Unitários}

\begin{teorema}
Seja $U$ e $V$ espaços com produto interno de \emph{uma mesma} dimensão finita e seja $T\in\mathcal{L}(U,V)$. Sao equivalentes:
\begin{itemize}
\item Para $u,v\in U$ temos $\langle u,v\rangle=\langle T(u),T(v)\rangle$.
\item $T$ é um isomorfismo de espaços com produto interno.
\item Para toda base ortonormal $B$ de $U$, a sua imagem $T[B]$ é uma base ortonormal de $V$. 
\item Existe uma base ortonormal $B$ de $U$ tal que $T[B]$ seja uma base ortonormal de $V$.
\end{itemize}
\end{teorema}
\begin{proof}
Se (i), então primeiro provemos que $\Ker(T)=0$. Para $v\in \Ker(T)$, então $T(v)=0$, aí:
\[
\langle v,v\rangle=\langle T(v),T(v)\rangle=0,
\]
aí $v=0$. Como $\dim \ V=\dim \ U=\dim \ \mathrm{Im} \ T+\dim \ \Ker \ T$, então $\dim \ \mathrm{Im} \ T=\dim \ V$, aí $T$ é sobrejetora, aí (ii).

\medskip
\noindent
Se (ii), então para base ortonormal $B=(e_1,\dots,e_n)$ de $U$, então $(T(e_1),\dots,T(e_n))$ é um conjunto ortonormal em $V$, aí é uma base ortonormal de $V$; logo (iii).

\medskip
\noindent
Se (iii), como sempre existe uma base ortonormal de $U$, então (iv) é evidente.

\medskip
\noindent
Se (iv), então seja $B=(e_1,\dots,e_n)$ uma base ortonormal de $U$ tal que $(T(e_1),\dots,T(e_n))$ seja base ortonormal de $V$. Para $u,v\in U$, então existem $a_1,dots,a_n\in K$ e $b_1,\dots,b_n$ tais que:
\[
u=a_1e_1+\dots+a_ne_n,\quad\quad v=b_1e_1+\dots+b_ne_n,
\]
então:
\[
\langle u,v\rangle=a_1\overline{b_1}+\dots+a_n\overline{b_n},
\]
aí:
\[
T(u)=a_1T(e_1)+\dots+a_nT(e_n),\quad\quad T(v)=b_1T(e_1)+\dots+b_nT(e_n),
\]
aí:
\[
\begin{array}{rcl}
\langle T(u),T(v)\rangle&=&\sum_{i,j}\langle a_iT(e_i),b_jT(e_j)\rangle\\
&=&\sum_{i,j}a_i\overline{b_j}\langle T(e_i),T(e_j)\rangle\\
&=&\sum_{k=1}a_k\overline{b_k}\\
&=&\langle u,v\rangle.
\end{array}
\]
\end{proof}

\begin{corolario}
Dois espaços vetoriais $U$ e $V$ com produto interno de dimensão finita são isomorfos se e só se $\dim \ U=\dim \ V$.
\end{corolario}

\begin{proof}
$(\Rightarrow)$ é claro. Por outro lado, se $\dim \ U=\dim \ V$, então escolhamos bases ortonormais $B=(e_1,\dots,e_n)$ de $U$ e $C=(f_1,\dots,f_n)$ de $V$, aí seja $T:U\rightarrow V$ dada por:
\[
T(a_1e_1+\dots+a_ne_n)=a_1f_1+\dots+a_nf_n,
\]
então $T\in\mathcal{L}(U,V)$, e para $u,v\in U$, sendo:
\[
u=a_1e_1+\dots+a_ne_n,\dots\dots v=b_1e_1+\dots+b_ne_n,
\]
então:
\[
\langle u,v\rangle=a_1\overline{b_1}+\dots+a_n\overline{b_n}
\]
e também:
\[
\langle T(u),T(v)\rangle=a_1\overline{b_1}+\dots+a_n\overline{b_n},
\]
assim:
\[
\langle u,v\rangle=\langle T(u),T(v)\rangle.
\]
\end{proof}

\begin{definicao}
Se $V$ é um espaço com produto interno e $T\in\mathcal{L}(V)$, dizemos que $T$ é \textbf{operador unitário} se e só se $\forall u,v\in V:\langle T(u),T(v)\rangle=\langle u,v\rangle$.
\end{definicao}

\begin{teorema}
Seja $T\in\mathcal{L}(U,V)$. Então $T$ preserva o produto interno se e só se $\forall u\in U:\norm{T(u)}=\norm{u}$.
\end{teorema}
\begin{proof}
$(\Rightarrow)$ É fácil pois:
\[
\norm{T(u)}^2=\langle T(u),T(u)\rangle=\langle u,u\rangle=\norm{u}^2.
\]
$(\Leftarrow)$ Por outro lado, então:
\[
\norm{T(u+v)}^2=\norm{u+v}^2,
\]
aí:
\[
\norm{T(u)}^2+\langle T(u),T(v)\rangle+\langle T(v),T(u)\rangle+\norm{T(v)}^2=
\norm{u}^2+\langle u,v\rangle+\langle v,u\rangle+\norm{v}^2,
\]
aí:
\[
\langle T(u),T(v)\rangle+\langle T(v),T(u)\rangle=
\langle u,v\rangle+\langle v,u\rangle,
\]
\end{proof}

\begin{proposicao}
Sejam $T,S\in\mathcal{L}(V)$ unitários. Então $T\circ S$ é operador unitário. O conjunto dos operadores unitários formam um grupo em relação à composição.
\end{proposicao}
\begin{proof}
\[
\langle TS(u),TS(v)\rangle=\langle S(u),S(v)\rangle=\langle u,v\rangle.
\]
\end{proof}

\begin{definicao}
O conjunto dos operadores unitários no espaço $K^n$ se denota por $U(n,K)$.
\end{definicao}

\begin{teorema}
Para $T\in\mathcal{L}(V)$, então $T$ é unitário se e somente se $T^*=T^{-1}$.
\end{teorema}
\begin{proof}
$(\Rightarrow)$ Se $T$ é unitário, então para todo $v\in V$ temos:
\[
\forall u\in V: \langle u,v\rangle=\langle T(u),T(v)\rangle=\langle u,T^*T(v)\rangle,
\]
aí $T^*T(v)=v$; logo $T^*T=I$.

\medskip
\noindent
$(\Leftarrow)$ se $T^*=T^{-1}$, então para $u,v\in V$ temos:
\[
\langle T(u),T(v)\rangle=\langle u,T^*T(v)\rangle=\langle u,T^{-1}T(v)\rangle=\langle u,v\rangle;
\]
logo $T$ é unitário.
\end{proof}

\begin{definicao}
Uma matriz $A\in\m{n}{K}$ se chama \textbf{unitária} se e só se $A^*=A^{-1}$. 
\end{definicao}

\begin{corolario}
Para operador $T\in\mathcal{L}(V)$, então $T$ é unitário se e só se para toda base $B$ de $V$ a matriz $[T]_B$ é unitária.
\end{corolario}

\begin{teorema}
Para matriz $A\in\m{n}{K}$, então $A$ é unitária se e só se as linhas de $A$ formam conjunto ortonormal em $K^n$, e isso equivale a dizer que as colunas de $A$ formam um conjunto ortonormal em $K^n$.
\end{teorema}
\begin{proof}
Seja $A\in\m{n}{K}$ uma matriz unitária, então $AA^*=I$, aí sendo:
\[
A=\begin{pmatrix}
a_{1,1}&\dots&a_{1,n}\\\dots&\dots&\dots\\a_{n,1}&\dots&a_{n,n}
\end{pmatrix},
\]
então:
\[
A^*=\begin{pmatrix}
\overline{a_{1,1}}&\dots&\overline{a_{n,1}}\\\dots&\dots&\dots\\\overline{a_{1,n}}&\dots&\overline{a_{n,n}}
\end{pmatrix},
\]
então temos:
\[
A_i=(a_{i,1},\dots,a_{i,n}),\quad\quad(A^*)^j=\begin{pmatrix}
\overline{a_{j,1}}\\\vdots\\\overline{a_{j,n}}
\end{pmatrix},
\]
assim:
\[
a_{i,1}\overline{a_{j,1}}+\dots+a_{i,n}\overline{a_{j,n}}=\delta_{i,j},
\]
assim:
\[
\langle(a_{i,1},\dots,a_{i,n}),(a_{j,1},\dots,a_{j,n})\rangle=\delta_{i,j}.
\]
\end{proof}

\begin{definicao}
Para $A\in\m{n}{K}$, dizemos que $A$ se chama \textbf{ortogonal} se e só se $A^{-1}=A^t$.
\end{definicao}

\begin{observacao}
Toda matriz real ortogonal é unitária. Toda matriz unitária ortogonal é real.
\end{observacao}

\begin{definicao}
Definimos o \textbf{grupo ortogonal} $O(n,\mathbb{R})$ como o conjunto das matrizes reais ortogonais.
\end{definicao}

\begin{definicao}
Para matrizes $A,B\in\m{n}{K}$, dizemos que $A$ é \textbf{unitariamente equivalente} a $B$ se e só se existe uma matriz $U\in\m{n}{K}$ \emph{unitária} tal que $U^{-1}AU=B$.
\end{definicao}

\begin{definicao}
Para matrizes $A,B\in\m{n}{R}$, dizemos que $A$ é \textbf{ortogonalmente equivalente} a $B$ se e só se existe uma matriz $U\in\m{n}{\mathbb{R}}$ \emph{ortogonal} tal que $U^{-1}AU=B$.
\end{definicao}

\begin{proposicao}
Seja $T\in\mathcal{L}(V)$. Então, para quaisquer bases ortonormais $B$ e $C$ de $V$, as matrizes $[T]_B$ e $[T]_C$ são unitariamente equivalentes.
\end{proposicao}
\begin{proof}
Seja $P_{B,C}$ a matriz de mudança de $B$ para $C$. Então $P_{B,C}$ é unitária. Mas também:
\[
[T]_C=P^{-1}_{B,C}[T]_BP_{B,C}.
\]
\end{proof}

\section{Operadores Normais}

Assim como estudamos os operadores que possuíssem uma base de autovetores em espaços vetoriais, agora queremos estudar os operadores que possuam uma base \emph{ortonormal} de autovetores em espaços com \emph{produto interno}.

\medskip
\noindent
Seja $T\in\mathcal{L}(V)$ que possua uma base $B=(e_1,\dots,e_n)$ de autovetores. Então:
\[
[T]_B=\begin{pmatrix}
\lambda_1&&0\\&\dots&\\0&&\lambda_n
\end{pmatrix},
\]
e também:
\[
[T]_B=\begin{pmatrix}
\overline{\lambda_1}&&0\\&\dots&\\0&&\overline{\lambda_n}
\end{pmatrix},
\]
aí:
\[
[TT^*]_B=[T]_B[T^*]_B=\begin{pmatrix}
\lambda_1\overline{\lambda_1}&&0\\&\dots&\\0&&\lambda_n\overline{\lambda_n}
\end{pmatrix}=\begin{pmatrix}
\overline{\lambda_1}\lambda_1&&0\\&\dots&\\0&&\overline{\lambda_n}\lambda_n
\end{pmatrix}=[T^*]_B[T]_B=[T^*T]_B,
\]
assim $TT^*=T^*T$.

\begin{definicao}
Para $T\in\mathcal{L}(V)$, dizemos que $T$ é \textbf{normal} se e só se $TT^*=T^*T$.
\end{definicao}

\begin{exemplo}
Temos alguns exemplos:
\begin{itemize}
\item Operadores autoadjuntos (isto é, que satisfazem $T^*=T$) são normais.
\item Operadores unitários (isto é, que satisfazem $T^*=T^{-1}$) são normais.
\item Operadores antiadjuntos (isto é, que satisfazem $T^*=-T$) são normais.
\end{itemize}
\end{exemplo}

\begin{lema}
Seja $T\in\mathcal{L}(V)$ normal e $\lambda$ um autovalor de $T$. Então $\overline{\lambda}$ é um autovalor de $T$.
\end{lema}
\begin{proof}
Seja $v\in V$ tal que $v\neq 0$ e $T(v)=\lambda v$. Então $(T-\lambda I)(v)=0$, aí para $u\in V$:
\[
\begin{array}{rcl}
0&=&\langle(T-\lambda I)(v),(T-\lambda I)(v)\rangle\\
&=&\langle v,(T-\lambda I)^*(T-\lambda I)(v)\rangle\\
&=&\langle v,(T^*-\overline{\lambda}I)(T-\lambda I)(v)\rangle\\
&=&\langle v,(T-\lambda I)(T^*-\overline{\lambda}I)(v)\rangle\\
&=&\langle (T-\lambda I)^*(v),(T^*-\overline{\lambda}I)(v)\rangle\\
&=&\langle (T^*-\overline{\lambda}I)(v),(T^*-\overline{\lambda}I)(v)\rangle,
\end{array}
\]
aí $(T^*-\overline{\lambda}I)(v)=0$, aí $T^*(v)=\overline{\lambda}v$.
\end{proof}

\begin{lema}
Seja $T\in\mathcal{L}(V)$ e $W\subseteq V$ um subespaço $T$-invariante. Então $W^\perp$ é $T^*$-invariante.
\end{lema}
\begin{proof}
Seja $w'\in W^\perp$. Para $w\in W$ então $T(w)\in W$, aí $\langle T(w),w'\rangle=0$, aí $\langle w,T^*(w')\rangle=0$. Logo $T^*(w')\in W^\perp$.
\end{proof}

\begin{lema}
Seja $T\in\mathcal{L}(V)$ normal e sejam $\lambda,\mu$ tais que $\lambda\neq\mu$. Se $u$ e $v$ são autovetores associados a $\lambda$ e $\mu$ respectivamente, então $\langle u,v\rangle=0$.
\end{lema}
\begin{proof}
Temos o seguinte:
\[
\begin{array}{rcl}
\lambda\langle u,v\rangle&=&\langle\lambda u,v\rangle\\
&=&\langle T(u),v\rangle\\
&=&\langle u,T^*(v)\rangle\\
&=&\langle u,\overline{\mu}v\rangle\\
&=&\mu\langle u,v\rangle,
\end{array}
\]
aí $(\lambda-\mu)\langle u,v\rangle=0$, mas $\lambda-\mu\neq 0$, aí $\langle u,v\rangle=0$.
\end{proof}

\section{Formas Canônicas}

\subsection{Caso dos Números Complexos}

\begin{teorema}
Seja $V$ um espaço com produto interno sobre $\mathbb{C}$ e $T\in\mathcal{L}(V)$. Então $T$ é normal se e somente se existe uma base ortonormal $B$ de $V$ tal que $[T]_B$ seja diagonal.
\end{teorema}
\begin{proof}
$(\Leftarrow)$ Já foi provado.

\medskip
\noindent
$(\Rightarrow)$ Indução em $n=\dim(V)$. Para $n=1$ é claro. Para $n>1$, se assumirmos o teorema válido para $\dim(V)<n$, então seja $\lambda$ um autovalor e seja $v\neq 0$ tal que $T(v)=\lambda v$. Nós temos $V=\langle v\rangle\oplus\langle v\rangle^\perp$. Sabemos que $\langle v\rangle^\perp$ é $T^*$-invariante. Provaremos que $\langle v\rangle^\perp$ é $T$-invariante. Seja $u\in\langle v\rangle^\perp$, então para $u\in V$ temos:
\[
\langle v,T(u)\rangle=\langle T^*(v),u\rangle=\langle \overline{\lambda}v,u\rangle=\overline{\lambda}\langle v,u\rangle=0;
\]
logo $T(v)\in\langle v\rangle^\perp$. Além disso, $\dim\langle v\rangle=n-1$. Portanto podemos definir $T\upharpoonright_{\langle v\rangle^\perp}$ e ele é normal \textcolor{red}{(Cheque!)}. Assim por hipótese de indução existe uma base ortonormal $B'=(e_2,\dots,e_n)$ de $\langle v\rangle^\perp$ tal que $[T\upharpoonright_{\langle v\rangle^\perp}]_{B'}$ seja diagonal. Seja $e_1=\frac{v}{\norm{v}}$, então $B=(e_1,e_2,\dots,e_n)$ uma base ortonormal e é fácil ver que $[T]_B$ é diagonal.
\end{proof}

\noindent
\textcolor{red}{Outro exemplo numérico enfadonho!}

\subsection{Caso dos Números Reais}

Para isso primeiro definimos o seguinte.

\begin{definicao}
Seja $V$ um espaço vetorial sobre $\mathbb{R}$. Definimos a \textbf{complexificação} de $V$ como:
\[
V_\mathbb{C}=\{(x,y)\mid x,y\in V\},
\]
e munimo-lo do seguinte:
\[
\begin{array}{rcl}
(x,y)+(x',y')&=&(x+x',y+y').\\
(a+bi)(x,y)&=&(ax-by,bx+ay).
\end{array}
\]
\end{definicao}

\begin{proposicao}
Nas notações acima, verifica-se que $V_\mathbb{C}$ é um espaço vetorial sobre $\mathbb{C}$.
\end{proposicao}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{definicao}
Para espaço vetorial $V$ sobre $\mathbb{R}$, para $T\in\mathcal{L}(V)$ definimos:
\[
T_\mathbb{C}(x,y)=(T(x),T(y)).
\]
\end{definicao}

\begin{proposicao}
Nas notações acima, verifica-se que $T_\mathbb{C}\in\mathcal{L}(V_\mathbb{C})$.
\end{proposicao}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{proposicao}
Nas notações acima, verificam-se:
\begin{itemize}
\item Para toda base $B$ base de $V$, então $B_\mathbb{C}=\{(x,0)\mid x\in B\}$ é uma base de $V_\mathbb{C}$.
\item Para todo $T\in\mathcal{L}(V)$, então $p_T=p_{T_\mathbb{C}}$.
\item Para subespaço $W$ de $V_\mathbb{C}$, então $W$ é da forma $W=U_\mathbb{C}$ para algum subespaço $U$ de $V$ se e só se $W^*\subseteq W$, em que definimos $W^*=\{(x,-y)\mid(x,y)\in W\}$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Seja $B=(e_1,\dots,e_n)$ uma base ortonormal de $V$. Para $(x,y)\in V_\mathbb{C}$, existem números reais:
\[
a_1,\dots,a_n,b_1,\dots,b_n\in\mathbb{R}
\]
tais que:
\[
x=a_1e_1+\dots+a_ne_n,\quad\quad y=b_1e_1+\dots+b_ne_n,
\]
assim:
\[
(x,y)=(a_1+b_1i)(e_1,0)+\dots+(a_n+b_ni)(e_n,0).
\]
Logo $B_\mathbb{C}$ gera $V_\mathbb{C}$. Para:
\[
a_1,\dots,a_n,b_1,\dots,b_n\in\mathbb{R},
\]
se:
\[
(a_1+b_1i)(e_1,0)+\dots+(a_n+b_ni)(e_n,0)=0,
\]
então:
\[
(a_1e_1+\dots+a_ne_n,b_1e_1+\dots+b_ne_n)=0,
\]
aí:
\[
a_1e_1+\dots+a_ne_n=0,\quad\quad b_1e_1+\dots+b_ne_n=0,
\]
assim:
\[
a_1=\dots=a_n=b_1=\dots=b_n=0.
\]
Logo $B_\mathbb{C}$ é linearmente independente.
\item \textcolor{red}{É só aplicar o item anterior e fazer umas continhas.}
\item $(\Rightarrow)$ Se $W$ é da forma $U_\mathbb{C}$ para algum subespaço $U$ de $V$, então para todo $(x,y)\in W$ temos $x\in U$ e $y\in U$, aí $x\in U$ e $-y\in U$, aí $(x,-y)\in W$; logo $W^*\subseteq W$.

\medskip
\noindent
$(\Leftarrow)$ Se $W^*\subseteq W$, então seja $(v_1,\dots,v_k)$ uma base de $W$, e para todo $i=1,\dots,k$ seja $v_i=(e_i,f_i)$ com $e_i,f_i\in V$, então temos $v_i^*=(e_i,-f_i)\in W$, assim:
\[
\begin{array}{rcl}
(e_i,0)&=&\frac{1}{2}(v_i+v_i^*)\in W\\
(f_i,0)&=&\frac{1}{2i}(v_i-v_i^*)\in W,
\end{array}
\]
assim, sendo $U$ o subespaço gerado por $e_1,\dots,e_n,f_1,\dots,f_n$, é fácil ver que $W=U_\mathbb{C}$.
\end{itemize}
\end{proof}

\begin{definicao}
Seja $V$ um espaço com produto interno sobre os reais. Então definimos:
\[
\langle(x,y),(x',y')\rangle=\left(\langle x,x'\rangle+\langle y,y'\rangle,\langle y,x'\rangle-\langle x,y'\rangle\right).
\]
\end{definicao}

\begin{proposicao}
Nas notações acima, verificam-se:
\begin{itemize}
\item $V_\mathbb{C}$ é um espaço com produto interno sobre os complexos.
\item Para $T\in\mathcal{L}(V)$ então temos $(T_\mathbb{C})^*=(T^*)_\mathbb{C}.$
\end{itemize}
\end{proposicao}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{corolario}
$T\in\mathcal{L}(V)$ é normal (resp. unitário; autoadjunto) se e só se $T_\mathbb{C}$ é normal (resp. unitário; autoadjunto).
\end{corolario}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{teorema}
Seja $V$ um espaço com produto interno sobre os reais e seja $T\in\mathcal{L}(V)$. Então $T$ é normal se e só se existe uma base ortonormal $B$ de $V$ tal que:
\[
[T]_B=\begin{pmatrix}
A_1&&0\\&\ddots&\\0&&A_n
\end{pmatrix},
\]
em que $A_i$ esteja em uma das seguintes formas:
\begin{itemize}
\item
\[
A_i=\begin{pmatrix}
a_i
\end{pmatrix},\quad\quad a_i\in\mathbb{R}
\]
\item
\[
A_i=r_i\begin{pmatrix}
\cos(\varphi_i)&-\sin(\varphi_i)\\\sin(\varphi_i)&\cos(\varphi_i)
\end{pmatrix},\quad\quad r_i\neq 0,0<\varphi<\pi
\]
\end{itemize}
\end{teorema}

\printindex

\end{document}