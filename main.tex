%%% ------- Readme -------%%%

%%-----------------------%%%
\documentclass[11pt,twoside,a4paper]{book}
\usepackage{estilos} 
\externaldocument{listas}
\makeindex
\title{Álgebra Linear \\  Douglas Smigly}
\author{MAT5730}
\date{$2^o$ semestre de 2019}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\chapter*{Informações da Disciplina}
\label{sec:intro}
\addcontentsline{toc}{section}{\nameref{sec:intro}}
\question{Informações Básicas}\label{new-question}

Essas são as notas de aula de Álgebra Linear(MAT5730), as aulas acontecem na sala B-134 às terças 10h e às quintas 8h.

\question{Informações do Professor}

O professor é o Ivan Shestakov, sua sala é a 290-A e o seu e-mail é shestak@ime.usp.br

\question{Bibliografia}
\nocite{*}
\bibliographystyle{plain}
\bibliography{samples}
\question{Avaliação}

A nota final da disciplina será a média aritimética de P1, P2, e P3. Todos os
alunos poderão fazer a prova sub para substituir a menor das suas notas (Sub
aberta). As datas das provas são as seguintes:

\begin{table}[h!]
\begin{center}

\label{tab:table1}
\begin{tabular}{l|r} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
\textbf{Prova} & \textbf{Data}\\
\hline
P1 & 10-09\\
P2 & 15-10\\
P3 & 12-11\\
SUB & 19-11
\end{tabular}
\end{center}
\end{table}

\question{Outras Informações}
\begin{enumerate}[label=(\roman*)]
\item Teremos listas, que não contarão para a nota
\item As listas serão publicadas em 
\item Não haverá monitoria
\end{enumerate}
\newpage 

\chapter{Espaços vetoriais}

\section{Definições Iniciais}

\begin{definicao}
Um \textbf{grupo abeliano} é um conjunto $X$ munido do seguinte:
\begin{itemize}
\item $+:X\times X\rightarrow X$,
\item $0\in X$,
\item $-:X\rightarrow X$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $x,y,z\in X$ então $(x+y)+z=x+(y+z)$,
\item Para $x,y\in X$ então $x+y=y+x$,
\item Para $x\in X$ então $x+0=x$,
\item Para $x\in X$ então $x+(-x)=0$.
\end{itemize}
\end{definicao}

\begin{definicao}
Um \textbf{corpo} é um grupo abeliano $(K,+,0,-)$ munido do seguinte:
\begin{itemize}
\item $\cdot:K\times K\rightarrow K$,
\item $1\in K$,
\item $\cdot^{-1}:K\setminus\{0\}\rightarrow K$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $x,y,z\in K$ então $(x\cdot y)\cdot z=x\cdot(y\cdot z)$,
\item Para $x,y\in K$ então $x\cdot y=y\cdot x$,
\item Para $x\in K$ então $x\cdot 1=x$,
\item Para $x\in K\setminus\{0\}$ então $x\cdot x^{-1}=1$,
\item Para $x,y,z\in K$ então $x\cdot(y+z)=(x\cdot y)+(x\cdot z)$.
\end{itemize}
\end{definicao}

\begin{definicao}
Dado $K$ um corpo, um \textbf{espaço vetorial sobre $K$} é um grupo abeliano $(V,+,0,-)$ munido do seguinte:
\begin{itemize}
\item $\cdot:K\times V\rightarrow V$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $a,b\in K$ e $x\in V$ então $(a\cdot b)\cdot x=a\cdot(b\cdot x)$,
\item Para $x\in V$ então $x\cdot 1=x$,
\item Para $a\in K$ e $x,y\in V$ então $a\cdot(x+y)=(a\cdot x)+(a\cdot y)$,
\item Para $a,b\in K$ e $x\in V$ então $(a+b)\cdot x=(a\cdot x)+(b\cdot x)$.
\end{itemize}
\end{definicao}

\section{Base e Dimensão}

Durante o restante deste capítulo, sempre adotaremos $K$ como sendo um corpo qualquer.

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Seja $I$ um conjunto e $v:I\rightarrow V$ uma função. Uma \textbf{combinação linear} de $v$ é um elemento $u\in V$ tal que existam um conjunto finito $J\subseteq I$ e uma função $\alpha:J\rightarrow K$ tais que:
\[
u=\sum_{i\in J}\alpha_iv_i.
\]
Dizemos que $v$ \textbf{gera} $V$ se e só se todo elemento de $V$ é combinação linear de $v$.
\end{definicao}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Dizemos que um conjunto $S\subseteq V$ \textbf{gera} $V$ se e só se a função $v:S\rightarrow V$ dada por $\forall s\in S:v_s=s$ gera $V$.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e sejam $I$ um conjunto e $v:I\rightarrow V$ uma função. Então $v$ gera $V$ se e somente se a imagem $S=\{v_i:i\in I\}$ gera $V$.
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Se $v$ gera $V$, então para $x\in V$ existem um conjunto finito $J\subseteq I$ e uma função $\alpha:J\rightarrow K$ tais que:
\[
x=\sum_{i\in J}\alpha_iv_i,
\]
aí seja $T=v[J]$, e seja:
\[
\forall s\in T:J_s=\{i\in J:v_i=s\};
\]
e seja $\beta:T\rightarrow K$ a função dada por:
\[
\forall s\in T:\beta_s=\sum_{i\in J_s}\alpha_i,
\]
então $T$ é finito, aí temos:
\[
x=\sum_{i\in J}\alpha_iv_i=\sum_{s\in T}\sum_{i\in J_s}\alpha_iv_i=\sum_{s\in T}\sum_{i\in J_s}\alpha_is=\sum_{s\in T}\beta_ss;
\]
logo $S$ gera $V$.
\item Se $S$ gera $V$, então para $x\in V$ existem um conjunto finito $T\subseteq S$ e uma função $\beta:T\rightarrow K$ tais que:
\[
x=\sum_{s\in T}\beta_ss,
\]
aí existe uma função $i:T\rightarrow I$ tal que:
\[
\forall s\in T:v_{i_s}=s,
\]
aí seja $J=\mathrm{Im}(i)$, então $J$ é finito e $i$ é uma bijeção de $T$ a $J$ e sua inversa é $u=v\upharpoonright J$, aí seja $\alpha=\beta\circ u$, então:
\[
x=\sum_{s\in T}\beta_ss=\sum_{j\in J}\beta_{u_j}u_j=\sum_{j\in J}\alpha_ju_j=\sum_{j\in J}\alpha_jv_j;
\]
logo $v$ gera $V$.\qedhere
\end{itemize}
\end{proof}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $I$ um conjunto e seja $v:I\rightarrow V$ uma função. Dizemos que $v$ é \textbf{linearmente independente} se e só se para todo conjunto finito $J\subseteq I$ e toda função $\alpha:J\rightarrow K$, então temos a implicação:
\[
\sum_{i\in J}\alpha_iv_i=0\quad\Rightarrow\quad\forall i\in J:\alpha_i=0.
\]
Dizemos que $v$ é \textbf{linearmente dependente} se e só se $v$ não é linearmente independente.
\end{definicao}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $S\subseteq V$ um conjunto. Dizemos que $S$ é \textbf{linearmente independente} se e só se a função $v:S\rightarrow V$ dada por $\forall s\in S:v_s=s$ é linearmente independente. Dizemos que $S$ é \textbf{linearmente dependente} se e só se não é linearmente independente.
\end{definicao}

\begin{exemplo}
Se $V$ é um espaço vetorial sobre um corpo $K$ e $u\in V$ é um elemento não nulo, então a função $v:\{0,1\}\rightarrow V$ dada por $v_0=u$ e $v_1=u$ é linearmente \textcolor{red}{dependente}, mas o conjunto $\{v_0,v_1\}=\{u\}$ é linearmente \textcolor{red}{independente}.
\end{exemplo}

\begin{definicao}\label{lideford}\index{Espaço Vetorial!Base ordenada}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $I$ um conjunto. Uma \textbf{base de $V$ ordenada por $I$} é uma função $b:I\rightarrow V$ tal que:
\begin{enumerate}[label=(\roman*)]
\item $b$ é linearmente independente.
\item $b$ gera $V$.
\end{enumerate}
\end{definicao}

\begin{definicao}\label{lidef}\index{Espaço Vetorial!Base}
Seja $V$ um espaço vetorial sobre um corpo $K$. Uma \textbf{base} de $V$ é um conjunto $B\subseteq V$ tal que:
\begin{enumerate}[label=(\roman*)]
\item $B$ é linearmente independente.
\item $B$ gera $V$.
\end{enumerate}
\end{definicao}

\begin{teorema}\label{existbase}
Seja $V$ um espaço vetorial e sejam $I\subseteq V$ linearmente independente e $S\subseteq V$ gerador de $V$ tais que $I\subseteq S$. Então existe uma base $B$ de $V$ tal que \[I\subseteq B\subseteq S.\]
\end{teorema}
\begin{proof}
Consideremos o conjunto:
\[
\mathcal{M}\coloneqq\{M\subseteq S\mid M\text{ é linearmente independente e }I\subseteq M\}
\]
Então $(\mathcal{M},\subseteq)$ é um conjunto parcialmente ordenado indutivo (ou seja, todo subconjunto totalmente ordenado possui uma cota superior). De fato, $I\in\mathcal{M}$, o que nos mostra que $\mathcal{M} \neq \emptyset,$ e para subconjunto totalmente ordenado não vazio $\mathcal{C}\subseteq\mathcal{M}$ então $\bigcup\mathcal{C}\in\mathcal{M}.$ 

\medskip
\noindent
Logo, pelo Lema de Zorn, $\mathcal{M}$ possui um elemento maximal $B$. Vamos provar que esse elemento maximal é de fato uma base para $V.$
\begin{enumerate}[label=(\roman*)]
\item $B$ é linearmente independente: segue da definição de $\mathcal{M}.$
\item $B$ gera $V$: Suponha por absurdo que $B$ não gera $V$. Então existe $v\in S$ que não é combinação linear de elementos de $B$, aí $B\cup\{v\}$ é linearmente independente e $I\subseteq B\cup\{v\}\subseteq S$. Então $B\cup\{v\}\in\mathcal{M}$, uma contradição, pois $B$ já é um elemento maximal de $\mathcal{M}$ e obviamente $B\subseteq B\cup\{v\}$. Logo $B$ gera $V$. Portanto, $B$ é uma base de $V$ e $I\subseteq B\subseteq S$.\qedhere
\end{enumerate}
\end{proof}

\noindent
O resultado acima mostra que todo espaço vetorial tem base, bastando para isso tomar $I=\emptyset$ e $S=V.$
\begin{corolario}
Seja $V$ um espaço vetorial sobre um corpo $K$,
seja $I\subseteq V$ um
conjunto linearmente independente e seja $S\subseteq V$
um conjunto que gere $V$. Então
\begin{enumerate}[label=(\roman*)]
\item O espaço $V$ tem uma base;
\item Existe uma base $B$ de $V$ tal que $I\subseteq B$; 
\item Existe uma base $B$ de $V$ tal que $B\subseteq S$.
\end{enumerate}
\end{corolario}

\begin{lema}\label{basefin1} Seja $V$ um espaço vetorial sobre um corpo $K$.
Sejam $(v_i)_{i=1}^n$ uma sequência linearmente independente e
$(u_j)_{j=1}^m$ uma sequência que gera $V$. Então $n\leq m$.
\end{lema}

\begin{sublema} Seja $V$ um espaço vetorial sobre um corpo $K$.
Uma sequência $(v_i)_{i=1}^m$ é linearmente dependente se e somente se existem $i$ e uma sequência $(\alpha_j)_{j=1}^{i-1}$ tais que \[v_i=\sum\limits_{j=1}^{i-1}\alpha_jv_j.\]
\end{sublema}
\begin{proof}[Demonstração do Sublema]
Se $(v_i)_{i=1}^m$ é linearmente dependente, então existe uma sequência $(\alpha_i)_{i=1}^m$ não identicamente nula tal que:
\[
\sum_{i=1}^m\alpha_iv_i=0.
\]
Seja $i$ o maior índice tal que $\alpha_i\neq 0$.
Então segue que
\begin{align*}
&\alpha_1 v_1 + \ldots + \alpha_i v_i = 0 \\ \iff & \alpha_1v_1 + \ldots + \alpha_{i-1} v_{i-1} = - \alpha_i v_i \\ \iff &
v_i= - \sum\limits_{j=1}^{i-1}\frac{\alpha_j}{\alpha_i}v_j.\qedhere
\end{align*}
\end{proof}

\begin{proof}[Demonstração do Lema]
Primeiro, listamos os dois conjuntos de vetores: o conjunto gerador seguido do conjunto linearmente independente:
\[
u_1,\dots,u_m;v_1,\dots,v_n
\]
Então movemos o primeiro vetor $v_1$ para a esquerda da primeira lista:
\[
v_1,u_1,\dots,u_m;v_2,\dots,v_n
\]
Como $u_1,\dots,u_m$ gera $V$, $v_1$ é combinação linear dos $u_i$'s. Isso implica que
podemos remover um dos $s_i$'s, que indexando se necessário pode ser $u_1$,
da primeira lista, e ainda temos um conjunto gerador:
\[
v_1,u_2,\dots,u_m;v_2,\dots,v_n
\]
Note que o primeiro conjunto dos vetores ainda gera $V$ e o segundo conjunto ainda é linearmente
independente.

\medskip
\noindent
Agora repetimos o processo, movendo $v_2$ da segunda lista para a primeira lista:
\[
v_1,v_2,u_2,\dots,u_m;v_3,\dots,v_n
\]
Como antes, os vetores na primeira lista são linearmente dependentes, já que eles geravam
$V$ antes da inclusão de $v_2$. Entretanto, como os $v_i$'s são linearmente independentes,
qualquer combinação linear não trivial dos vetores na primeira lista que valha $0$
deve envolver pelo menos um dos $u_i$'s. Portanto, podemos remover este vetor, que
novamente reindexando se necessário pode ser $u_2$ e ainda temos um conjunto gerador:
\[
v_1,v_2,u_3,\dots,u_m;v_3,\dots,v_n
\]
Mais uma vez, o primeiro conjunto dos vetores gera $V$ e o segundo conjunto é linearmente
independente.

\medskip
\noindent
Agora, if $m<n$, então este processo eventualmente esgotará os $u_i$'s e nos levará à lista
\[
v_1,v_2,\dots,v_m;v_{m+1},\dots,v_n
\]
em que $v_1,v_2,\dots,v_m$ geram $V$, o que claramente não é possível pois $v_n$ não é combinação linear
dos $v_1,v_2,\dots,v_m$. Portanto $n\leq m$.
\end{proof}

\begin{observacao}\label{basefin2}
Com o lema, também podemos mostrar que, se existe um conjunto gerador finito, então podemos mostrar que todo conjunto linearmente independente é finito.

\medskip
\noindent
De fato, se existirem uma sequência geradora $(u_j)_{j=1}^m$ e um conjunto linearmente independente infinito $S$, então podemos pegar $m+1$ vetores distintos e assim formar uma sequência linearmente independente $(v_i)_{i=1}^{m+1}$, contradizendo o lema \ref{basefin1}.
\end{observacao}

\noindent
Vamos relembrar o que fizemos até aqui com um exemplo:
\begin{exemplo}
Considere $V = \mathbb{R}^4$ um $\mathbb{R}$-espaço vetorial. Sejam os vetores:
\[
\begin{array}{rcl}
v_1 &=& (1,0,0,0) \\
v_2 &=& (0,1,0,-1) \\
v_3 &=& (0,0,1,-1) \\
v_4 &=& (1,-1,0,0) \\
v_5 &=& (1,2,1,0) 
\end{array}
\]
Considere $I = \{ v_1, v_2 \}$ e $S  =\{ v_1,v_2,v_3,v_4,v_5 \}.$ Observe que $I$ é LI; de fato,
\[
\alpha_1v_1 + \alpha_2v_2 = 0 \Rightarrow \alpha_1(1,0,0,0) + \alpha_2 (0,1,0,-1) = 0 \Rightarrow \left\{ \begin{array}{rcl} \alpha_1 &=& 0 \\ \alpha_2 &=& 0 \\ - \alpha_2 &=& 0 \end{array} \right. \Rightarrow \alpha_1 = \alpha_2 = 0
\]
Ademais, tomando $v = (x,y,z,w) \in \mathbb{R}^4,$ temos que
\[
(x-z+w+y)v_1 + (z- w - \varepsilon)v_2 + (z - \varepsilon)v_3 + (z-w-y + \varepsilon)v_4 + \varepsilon_5 = v,
\]
para todo $\varepsilon \in \mathbb{R}.$ Logo, $S$ gera $V.$ 

\medskip
\noindent
Então, existe uma base $B$ de $\mathbb{R}^4$ tal que 
\[
\{ v_1, v_2 \} \subseteq B \subseteq \{v_1,v_2,v_3,v_4,v_5 \}
\]
De fato, esta base é $B=\{v_1, v_2, v_3, v_4 \},$ pois percebe-se que
\[
v_5 = \frac{5}{2}v_1 + \frac{1}{2} v_2 - \frac{1}{2}v_3 - \frac{3}{2} v_4
\]
\end{exemplo}

\noindent
Para trabalhar com a cardinalidade das bases, utilizaremos alguns fatos
conhecidos, enunciados na próxima proposição:
\begin{proposicao}
Se $\lambda$ e $\mu$ são cardinais, então:
\begin{itemize}
\item Se $\lambda\leq\mu$ e $\mu\leq\lambda$, então $\lambda=\mu$. (Teorema de Cantor-Bernstein)\index{Teorema de Cantor-Bernstein}
\item Se $\lambda$ e $\mu$ são infinitos, então \[\lambda+\mu=\lambda\mu=\max\{\lambda,\mu\}.\]
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço vetorial, então duas bases quaisquer têm o mesmo cardinal.
\end{teorema}
\begin{proof}
Sejam $B$ e $C$ bases de $V$.
\begin{itemize}
\item Se $B$ ou $C$ são finitos, então pela observação \ref{basefin2} podemos inferir que $B$ e $C$ são ambos finitos e assim aplicar o lema \ref{basefin1}.
\item Se $B$ e $C$ são infinitos. Para $u\in C$ existem um conjunto finito $I_u\subseteq B$ e uma função $\alpha_u:I_u\rightarrow K$ tais que $u=\sum_{i\in I_u}(\alpha_u)_ii$. Seja $I\subseteq\bigcup_{u\in C}\subseteq B$. Então $I$ gera $V$, assim $I=C$. Desse modo:
\[
\abs{B}=\abs{I}=\abs{\bigcup_{u\in C}I_u}\leq\sum_{u\in C}\abs{I_u}\leq\aleph_0\cdot\abs{C}=\abs{C},
\]
assim $\abs{B}\leq\abs{C}$. Analogamente $\abs{C}\leq\abs{B}$. Portanto $\abs{B}=\abs{C}$.
\end{itemize}
\end{proof}
\begin{definicao}\index{Espaço Vetorial!Dimensão}
Dizemos que a \textbf{dimensão} de um espaço vetorial é a cardinalidade de sua base.
\end{definicao}

\section{Subespaços}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Um \textbf{subespaço} de $V$ é um conjunto $W\subseteq V$ tal que:
\begin{itemize}
\item $0\in W$,
\item Para $x,y\in W$ então $x+y\in W$,
\item Para $a\in K$ e $x\in W$ então $ax\in W$.
\end{itemize}
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial e seja $\mathcal{W}$ um conjunto de subespaços. Então $\bigcap\mathcal{W}$ é um subespaço de $V$.
\end{proposicao}

\begin{definicao}
Se $S$ é subconjunto de $V$, definimos:
\[
\langle S\rangle=\left\{\sum\limits_{v\in I}\alpha_vv\mid I\subseteq S\text{ e }I\text{ é finito e }\alpha\in K^I\right\}
\]
e chamamos de \textbf{subespaço gerado} por $S$.
\end{definicao}

\begin{proposicao}
Se $S$ é subconjunto de $V$, então:
\[
\langle S\rangle=\bigcap\{W\mid W\text{ é subespaço de }V\text{ e }S\subseteq W\}.
\]
\end{proposicao}
\begin{proof}
Seja:
\[
T=\bigcap\{W\mid W\text{ é subespaço de }V\text{ e }S\subseteq W\}.
\]
Para $x\in \langle S\rangle$, então existem um conjunto finito $I\subseteq S$ e uma função $\alpha:I\rightarrow V$ tal que:
\[
x=\sum\limits_{v\in I}\alpha_vv,
\]
aí para todo subespaço $W$ tal que $S\subseteq W$, então para todo $v\in I$ temos $v\in S$, aí $v\in W$; aí por indução finita temos $x\in W$; logo $x\in T$. Portanto $\langle S\rangle\subseteq T$.

\medskip
\noindent
Além disso, temos o seguinte:
\begin{itemize}
\item $\emptyset\subseteq S$ e $\emptyset$ é finito e $\emptyset\in K^\emptyset$ e:
\[
0=\sum_{v\in\emptyset}\emptyset_vv,
\]
aí $0\in\langle S\rangle$.
\item Para $x,y\in\langle S\rangle$, então existem conjuntos finitos $I,J\subseteq S$ e funções $\alpha\in K^I$ e $\beta\in K^J$ tais que:
\[
x=\sum\limits_{u\in I}\alpha_uu,\quad y=\sum\limits_{v\in J}\beta_vv,
\]
aí sendo $L=I\cup J$ então $L\subseteq S$ e $L$ é finito, e também sendo $\tilde{\alpha},\tilde{\beta}:L\rightarrow K$ dadas por:
\[
\tilde{\alpha}_l=\left\{\begin{array}{cl}\alpha_l&\text{se }l\in I\\0&\text{se }l\notin I\end{array}\right.,\quad\tilde{\beta}_l=\left\{\begin{array}{cl}\beta_l&\text{se }l\in J\\0&\text{se }l\notin J\end{array}\right.,
\]
e sendo $\gamma:L\rightarrow K$ dada por $\gamma_l=\tilde{\alpha}_l+\tilde{\beta}_l$, então:
\[
x+y=\sum_{l\in L}\gamma_ll,
\]
aí $x+y\in\langle S\rangle$.
\item Para $a\in K$ e $x\in\langle S\rangle$, então existem conjunto finito $I\subseteq S$ e $\alpha\in K^I$ tais que:
\[
x=\sum_{v\in I}\alpha_vv,
\]
aí sendo $\beta:I\rightarrow K$ dada por $\beta_v=a\alpha_v$, então:
\[
ax=\sum_{v\in I}\beta_vv,
\]
aí $ax\in\langle S\rangle$.
\item Para $s\in S$, então $\{s\}\subseteq S$ e $\{s\}$ é finito, e considerando a função $\alpha:\{s\}\rightarrow K$ dada por $\alpha_s=1$, então:
\[
s=\sum_{v\in\{s\}}\alpha_vv,
\]
aí $S\subseteq\langle S\rangle$.
\end{itemize}
Logo $\langle S\rangle$ é um subespaço de $V$ tal que $S\subseteq\langle S\rangle$, aí $T\subseteq\langle S\rangle$.
\end{proof}

\noindent
A intersecção de subsespaços sempre é um subespaço, mas o mesmo não acontece com a união de subespaços.
\begin{proposicao}\label{unotsubsp}
Se $A$ e $B$ são subespaços de $V$ tais que $A\nsubseteq B$ e $B\nsubseteq A$, então $A\cup B$ não é subespaço de $V$.
\end{proposicao}
\begin{proof}
Nesse caso, existe $a\in A$ tal que $a\notin B$ e existe $b\in B$ tal que $b\notin A$. Seja $c=a+b$. Então:
\begin{itemize}
    \item Se $c \in A,$ $b = c - a \in A,$ o que é impossível.
    \item Se $c \in B,$ $a = c - b \in b,$ o que é impossível.
\end{itemize}
Logo, concluímos que $c \notin A \cup B,$ absurdo. \qedhere
\end{proof}

\noindent
Portanto concluímos que $A \cup B$ é um subespaço se e somente se $A \subseteq B$ ou $B \subseteq A.$

\begin{observacao}
Seja $K = F_2 = \{ 0, 1 \},$ e tome $V = K^2.$ Então,
\[
V = \langle (0,1) \rangle \cup \langle (1,0) \rangle \cup \langle (1,1) \rangle
\]
Na verdade, $V$ só pode ser escrito como união de um número finito de subespaços próprios se $K$ for um corpo finito, conforme a seguinte proposição.
\end{observacao}

\begin{proposicao}
Um espaço vetorial $V$ sobre um corpo infinito $K$ não pode ser escrito como união de um número finito de subespaços próprios.
\end{proposicao}
\begin{proof}
Suponhamos que $V=S_1\cup\dots\cup S_n$, em que podemos assumir que:
\[
S_1\nsubseteq S_2\cup\dots\cup S_n,
\]
Seja $w\in S_1 \setminus (S_2\cup\dots\cup S_n)$ e seja $v\notin S_1$. Considere o conjunto infinito:
\[
A=\{rw+v\mid r\in K\},
\]
que é a ``reta'' passando por $v$ e paralela a $w$. Queremos mostrar que cada $S_i$
contém no máximo um vetor do conjunto infinito $A$, o que será uma contradição ao fato de que
$V=S_1\cup\dots\cup S_n$. Isto provará o teorema.

\medskip
\noindent
Se $rw+v\in S_1$ para algum $r\neq 0$, então $w\in S_1$ implicará $v\in S_1$, contrário às hipóteses.
Agora, suponha que $r_1w+v\in S_1$ e $r_2w+v\in S_1$, para algum $i\geq 2$, em que $r_1\neq r_2$.
Então:
\[
(r_1-r_2)w=(r_1w+v)-(r_2w+v)\in S_i,
\]
aí $w\in S_i$, que também contradiz as hipóteses.
\end{proof}

\noindent
Apesar de não podermos trabalhar com a união, podemos realizar a soma de subespaços, e esta sim é um subespaço:

\begin{definicao}
Sejam $W_i \subseteq V$, $i \in I,$ subespaços de $V.$ Definimos:
\[
\sum\limits_{i \in I} W_i = \{ w_{i_1} + \ldots + w_{i_k} \mid k \in \mathbb{N}, w_i \in W_i \}.
\]
\end{definicao}

\noindent
Pode-se mostrar que o conjunto:
\[
\sum\limits_{i \in I} W_i
\]
é subespaço de $V$.

\begin{definicao}\index{Espaço Vetorial!Soma direta}
Uma soma:
\[
\sum\limits_{i \in I} W_i
\]
é dita uma \textbf{soma direta} se para todo $i \in I$ tivermos:
\[
W_i \cap \left( \sum\limits_{j \neq i} W_j \right) = 0
\]
\end{definicao}

\begin{teorema}
Para subespaço $A$ de $V$, então existe subespaço $B\subseteq V$ tal que $V=A\oplus B$.
\end{teorema}
\begin{proof}
Seja $E$ uma base de $A$. Então existe uma base $G$ de $V$ tal que $E\subseteq G$, aí seja $F=G\setminus E$, e seja $B$ o subespaço gerado por $F$. Então é fácil ver que $V=A\oplus B$.
\end{proof}

\begin{teorema}
\[
\dim(A+B)+\dim(A\cap B)=\dim(A)+\dim(B).
\]
\end{teorema}
\begin{proof}
Seja $E$ base de $A\cap B$. Então existe $F$ tal que $B\cap F=\emptyset$ e $E\cup F$ seja base de $A$ e existe $G$ tal que $A\cap G=\emptyset$ e $E\cup G$ seja base de $B$. Então $E\cup F\cup G$ é base de $A+B$. Daí:
\[
\textcolor{Green}{\dim(A+B)} + \textcolor{Blue}{\dim(A \cap B)} = \textcolor{Green}{\abs{E} + \abs{F} + \abs{G}} + \textcolor{Blue}{\abs{E}} = \textcolor{Red}{\abs{E} + \abs{F}} + \textcolor{Laranja}{\abs{E} + \abs{G}} = \textcolor{Red}{\dim(A)} + \textcolor{Laranja}{\dim(B)}
\]
\end{proof}

\begin{exemplo}
Considere novamente $V = \mathbb{R}^4$. Sejam
\[
W_1 = \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \},
\]
\[
W_2 = \{(x,y,z,t) \in \mathbb{R}^4 | x +y = 0 \mbox{ e } z - 2t = 0 \}.
\]
Então $W_1$ e $W_2$ são subespaços de $V.$ Assim, $W_1 + W_2$ e $W_1 \cap W_2$ são subespaços de $V.$ Vamos encontrar bases para eles.

\medskip
\noindent
Note que:
\[
\begin{array}{lcl}
W_1 &=& \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \} \\
&=& \{(x,y,z,-y-z) \in \mathbb{R}^4 | x,y,z \in \mathbb{R} \} \\
&=& \{ (x,0,0,0) + (0,y,0-y) + (0,0,z,-z) : x,y,z \in \mathbb{R} \} \\
&=& \langle (1,0,0,0), (0,1,0-1), (0,0,1,-1) \rangle\\
\end{array}
\]
Verifica-se também que $(1,0,0,0), (0,1,0-1), (0,0,1,-1)$ são linearmente independentes. Logo, $B_1 = \{ (1,0,0,0), (0,1,0-1), (0,0,1,-1) \}$ é base para $W_1.$

\medskip
\noindent
Analogamente, mostra-se que $B_2 = \{ (1,-1,0,0), (0,0,2,1) \}$ é base para $W_2.$

\medskip
\noindent
Agora, para determinar uma base de $W_1 + W_2,$ podemos escalonar a matriz
\[
\left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & -1 \\
1 & -1 & 0 & 0\\
0 & 0 & 2 & 1

\end{array} \right) \rightarrow \cdots \rightarrow \left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0& 0& 0 & 1\\
0 & 0 & 0 & 0

\end{array} \right)
\]
Portanto, o conjunto
\[
\mathcal{B} = \{ (1,0,0,0), (0,1,0,-1),(0,0,1,-1),(1,-1,0,0) \}\]
é base de $W_1 + W_2.$

\medskip
\noindent
Para determinar uma base de $W_1 \cap W_2,$ basta resolver o sistema
\[
\left\{ \begin{array}{l}
y+z+t = 0 \\x+y = 0 \\z - 2t = 0
\end{array} \right.
\]
Assim, $W_1 \cap W_2 = \langle (3,-3,2,1) \rangle.$

\medskip
\noindent
Observe que
\[
\dim(W_1 \cap W_2) + \dim(W_1 + W_2) = 1 + 4 = 5 = 3 + 2 = \dim(W_1) + \dim(W_2).
\]
Como $\dim(W_1 + W_2) = 4,$ temos que $W_1 + W_2 = V = \mathbb{R}^4.$

\medskip
\noindent
Observe também que, como $\dim(W_1 \cap W_2) = 1,$ a soma $W_1 + W_2$ não é direta. 
\end{exemplo}

\section{Coordenadas}

\begin{definicao}
Seja $V$ um espaço vetorial de dimensão finita. Seja $B$ uma base de $V$. Então para $v\in V$ existe um único $\alpha:B\rightarrow K$ tal que \[v=\sum\limits_{b\in B}\alpha_bb,\] e chamamos esse $\alpha$ de $[v]_B$.
\end{definicao}

\chapter{Transformações Lineares}
\index{Espaço Vetorial!Transformações Lineares}

\section{Definições}

\begin{definicao}
Sejam \(U\) e \(V\) espaços vetoriais sobre um corpo \(K\). Uma
\textbf{transformação linear} é uma função $T:U\rightarrow V$ tal que  $$T(\alpha
u+\beta v)=\alpha T(u)+\beta T(v)$$
 para quaisquer $\alpha,\beta\in K$ e $u,v\in V$. Além disso, denotamos o conjunto das transformações lineares de $U$ a $V$ por $\mathcal{L}(U,V).$
\end{definicao}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, seja $B$ uma base de $U$ e $f:B\rightarrow V$ uma função. Então existe uma única transformação linear $T\in\mathcal{L}(U,V)$ tal que $\forall b\in B:T(b)=f(b)$.
\end{teorema}

\begin{definicao}
Seja $T\in\mathcal{L}(U,V)$. Definimos $\mathrm{Ker}(T)=\{u\in U:T(u)=0\}$. Definimos $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$.
\end{definicao}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$. Então:
\begin{itemize}
\item $\mathrm{Ker}(T)$ é um subespaço de $U$.
\item $\mathrm{Im}(T)$ é um subespaço de $V$.
\item $T$ é injetora se e só se $\mathrm{Ker}(T)=0$.
\item Se $T$ é bijetora, então $T^{-1}\in\mathcal{L}(V,U)$.
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $\mathcal{L}(U,V)$, seja $B$ uma base de $\mathrm{Ker}(T)$, e seja $C$ um conjunto tal que $T[C]$ seja base de $\mathrm{Im}(T)$. Então $B\cup C$ é base $V$.
\end{teorema}
\begin{proof}
Para $v\in V$ então $T(v)\in\mathrm{Im}(T)$, então existem um conjunto finito $F\subseteq C$ e $\alpha:F\rightarrow K$ tais que:
\[
T(v)=\sum\limits_{w\in F}\alpha_wT(w),
\]
assim:
\[
T\left(v-\sum\limits_{w\in F}\alpha_ww\right)=0,
\]
aí:
\[
v-\sum\limits_{w\in F}\alpha_ww\in\mathrm{Ker}(T),
\]
assim existem um conjunto finito $E\subseteq B$ e função $\beta:B\rightarrow K$ tais que:
\[
v-\sum\limits_{w\in F}\alpha_ww=\sum\limits_{u\in E}\beta_uu,
\]
aí:
\[
v=\sum\limits_{u\in E}\beta_uu+\sum\limits_{w\in F}\alpha_ww.
\]
Por outro lado, para subconjunto finito $E\subseteq B\cup C$ e função $\alpha:E\rightarrow K$ tais que:
\[
\sum_{e\in E}\alpha_ee=0,
\]
então:
\[
\sum_{e\in E\cap C}\alpha_eT(e)=0,
\]
aí:
\[
\forall e\in E\cap C:\alpha_e=0,
\]
aí:
\[
\sum_{e\in E\setminus C}\alpha_ee=0,
\]
aí:
\[
\forall e\in E\setminus C:\alpha_e=0,
\]
portanto:
\[
\forall e\in E:\alpha_e=0.
\]
\end{proof}

\begin{teorema}[Teorema do Núcleo-Imagem] \index{Espaço Vetorial!Teorema do Núcleo-Imagem}
Seja $T \in \mathcal{L}(U,V).$ Então
\[
U = \mathrm{Ker}(T) \oplus \mathrm{Im}(T)
\]

\end{teorema}
\begin{corolario}
\[
\dim V=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T)).
\]
\end{corolario}

\begin{definicao}\index{Transformações Lineares!Isomorfismos}
Se $T\in\mathcal{L}(U,V)$ é bijetora, dizemos que $T$ é um \textbf{isomorfismo} de $U$ a $V$.
\end{definicao}
\begin{proposicao}
$T \in \mathcal{L}(U,V)$ é isomorfismo se e somente se $T^{-1}$ também o é.
\end{proposicao}

\begin{proposicao}
Dois espaços vetoriais $U$ e $V$ são isomorfos se e somente se quaisquer duas bases $B$ de $U$ e $C$ de $V$ possuem a mesma cardinalidade.
\end{proposicao}
\begin{teorema}
Para espaços vetoriais $U$ e $V$, então $U$ é isomorfo a $V$ se e só se $\dim(U)=\dim(V)$.
\end{teorema}

\section{Espaço Dual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. Denotamos $V^*=\mathcal{L}(V,K)$. O espaço $V^*$ chama-se o \textbf{espaço dual} de $V$. Os elementos de $V$ chamam-se \textbf{funcionais lineares}.
\end{definicao}

\noindent
Se $\dim(V)=n$, então $\dim(V^*)=n\cdot 1=n$, aí $V$ e $V^*$ são isomorfos.

\begin{teorema}\label{basedual}
Seja $V$ um espaço vetorial com $\dim(V)=n$ e $B=(v_i)_{i=1}^n$ uma base de $V$. Então existe uma base $B^*=(f_i)_{i=1}^n$ de $V^*$ tal que $f_i(v_j)=\delta_{i,j}$ para quaisquer $i,j$. Além disso:
\[
\forall v\in V:v=\sum_{i=1}^nf_i(v)v_i
\]
e:
\[
\forall f\in V^*:f=\sum_{i=1}^nf(v_i)f_i.
\]
\end{teorema}
\begin{proof}
Para $i=1,\dots,n$, existe uma única função linear $f_i:V\rightarrow K$ tal que:
\[
f_i(v_j)=\left\{\begin{array}{rl}0,&i\neq j\\1,&i=j\end{array}\right.
\]
Sejam $\alpha_1,\dots,\alpha_n\in K$ tais que:
\[
\sum_{i=1}^n\alpha_if_i=0.
\]
Para $j=1,\dots,n$, aplicando este funcional para o vetor $v_j\in B$, então:
\[
0=0(v_j)=\sum_{i=1}^n\alpha_if_i(v_j)=\alpha_j,
\]
ou seja, $\alpha_j=0$. Portanto $B^*$ é linearmente independente.

\medskip
\noindent
Além disso, para $v\in V$ existem $\alpha_1,\dots,\alpha_n\in K$ tais que:
\[
v=\sum_{i=1}^n\alpha_iv_i,
\]
aí para $i=1,\dots,n$ temos:
\[
f_i(v)=\alpha_if_i(v_i)=\alpha_i;
\]
logo:
\[
f(v)=\sum_{i=1}^n\alpha_if(v_i)=\sum_{i=1}^nf(v_i)f_i(v). \qedhere
\] 
\end{proof}

\begin{definicao}\index{Espaço Vetorial!Base dual}
A base $B^*$ chama-se a \textbf{base dual} da base $B$.
\end{definicao}

\noindent
Podemos estender o estudo do espaço dual para espaços vetoriais quaisquer.

\begin{definicao}
Seja $B$ uma base de $V$, então para cada $a\in B$ definimos a transformação linear $f_a\in V^*$ por $f_a(b)=\delta_{a,b}$.
\end{definicao}

\noindent
Nesse caso, podemos adaptar facilmente o argumento na demonstração do teorema \ref{basedual} para mostrar que $(f_a)_{a\in B}$ é linearmente independente em $V^*$ e para todo $v\in V$ existe um conjunto finito $F\subseteq B$ tal que:
\[
v=\sum_{b\in F}f_b(v)b.
\]

\section{Espaço Bidual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. O espaço $V^{**}=(V^*)^*$ chama-se o \textbf{espaço bidual} do espaço $V$.
\end{definicao}

\begin{definicao}
Para $v\in V$, definamos $\varphi_v:V^*\rightarrow K$ assim:
\[
\forall f\in V^*:\varphi_v(f)=f(v).
\]
Então $\varphi_v\in V^{**}$.
\end{definicao}

\begin{proposicao}
$\varphi\in\mathcal{L}(V,V^{**})$ e $\varphi$ é injetora.
\end{proposicao}
\begin{proof}
Seja $B$ uma base de $V$. Para $v\in\mathrm{Ker}(\varphi)$, então $\varphi_v=0$, aí temos $\forall b\in B:f_b(v)=\varphi_v(f_b)=0$, aí existe um conjunto finito $F\subseteq B$ tal que:
\[
v=\sum_{b\in F}f_b(v)b,
\]
aí $v=0$.
\end{proof}

\begin{corolario}
Se $\dim(V)$ é finita, então $\varphi:V\rightarrow V^{**}$ é um isomorfismo.
\end{corolario}
\begin{proof}
\[
\dim(V)=\dim(V^*)=\dim(V^{**}). \qedhere
\]
\end{proof}

\begin{observacao}
Nesse caso $\varphi$ é um isomorfismo natural, ou seja, não depende da escolha de uma base.
\end{observacao}

\begin{corolario}
Se $\dim(V)$ é finita, então toda base de $V^*$ é a base dual para uma base de $V$.
\end{corolario}
\begin{proof}
Seja $C=(f_i)_{i=1}^n$ uma base de $V^*$. Consideremos a base dual $C^*=(g_i)_{i=1}^n$ de $V^{**}$. Mas $\varphi$ é sobrejetora, então existem $v_1,\dots,v_n\in V$ tais que para todo $i$ tenhamos $g_i=\varphi_{v_i}$, assim:
\[
f_i(v_j)=\varphi_{v_j}(f_i)=g_j(f_i)=\delta_{j,i}=\delta_{i,j},
\]
logo $C=(f_i)_{i=1}^n$ é base dual da base $(v_i)_{i=1}^n$ de $V$.
\end{proof}

\section{Anuladores}

\begin{definicao}
Seja $V$ um espaço vetorial e seja $S\subseteq V$ um subconjunto. Então definimos:
\[
S^0=\{f\in V^*\mid\forall s\in S:f(s)=0\}.
\]
O conjunto $S^0$ chama-se o \textbf{anulador} de $S$.
\end{definicao}

\begin{proposicao}
$S^0$ é um subespaço de $V$.
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço com $\dim(V)<\aleph_0$ e $W\subseteq V$ um subespaço. Então:
\[
\dim(V)=\dim(W)+\dim(V^0).
\]
\end{teorema}
\begin{proof}
Seja $\dim(V)=n$ e $\dim(W)=m$. Escolhemos uma base $(v_i)_{i\in m}$ de $W$ e completemo-la até uma base $(v_i)_{i\in n}$ de $V$. Consideremos a base dual $(f_{v_i})_{i\in n}$ de $V^*$. Mostraremos que $(f_{v_i})_{i\in n\setminus m}$ é uma base de $W^0$. É claro que $\forall i\in n\setminus m:f_{v_i}\in W^0$. Seja $f\in W^0$, então $f=\sum_{i\in n}f(v_i)f_{v_i}=\sum_{i\in n\setminus m}f(v_i)f_{v_i}$.
\end{proof}

\begin{teorema}
Se $\dim(V)<\aleph_0$ e $V=U\oplus W$, então $V^*=U^0\oplus W^0$ e $U^0\cong W^*$ e $W_0\cong U^*$.
\end{teorema}
\begin{proof}
Seja $B=B_U\cup B_W$ uma base de $V$, em que $B_U$ é base de $U$ e $B_W$ é base de $W$. Então na base dual temos $B^*=B_U^*\cup B_V^*$, e pelo teorema anterior temos $\langle B_U^*\rangle=W^0$ e $\langle B_V^*\rangle=U^0$.
\end{proof}

\section{Transpostas}

\begin{definicao}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e $T\in\mathcal{L}(U,V)$. Então definimos a \textbf{transposta} de $T$ como a função:
\[
\begin{array}{rcl}
T^t:V^t&\rightarrow&U^t\\f&\mapsto&T^t(f)=f\circ T
\end{array}
\]
\end{definicao}

\begin{proposicao}
Se $\dim(U)<\aleph_0$ e $T\in\mathcal{L}(U,V)$, então:
\begin{itemize}
\item[a)] $\mathrm{Ker}(T^t)=(\mathrm{Im}(T))^0$.
\item[b)] $\mathrm{Rank}(T^t)=\mathrm{Rank}(T)$.
\item[c)] $\mathrm{Im}(T^t)=(\mathrm{Ker}(T))^0$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[a)] Temos:
\[
\begin{array}{rcl}
\mathrm{Ker}(T^t)&=&\{f\in V^*\mid T^t(f)=0\}\\&=&\{f\in V^*\mid f\circ T=0\}\\&=&\{f\in V^*\mid \forall u\in U:f(T(u))=0\}\\&=&\{f\in V^*\mid f[\mathrm{Im}(T)]=0\}\\&=&(\mathrm{Im}(T))^0.
\end{array}
\]
\item[b)] Temos $\mathrm{Rank}(T^t)=\dim(\mathrm{Im}(T^t))$ e $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$. Além disso:
\[
\dim(V^*)=\dim(\mathrm{Im}(T^t))+\dim(\mathrm{Ker}(T^t))
\]
\[
\dim(V^*)=\dim(\mathrm{Im}(T))+\dim(\mathrm{Im}(T))^0
\]
mas $\dim(V^*)=\dim(V)$ e $\dim(\mathrm{Ker}(T^t))+\dim(\mathrm{Im}(T))^0$.
\item[c)] Temos $\mathrm{Im}(T^t)\subseteq(\mathrm{Ker}(T))^0$. Seja $\varphi\in\mathrm{Im}(T^t)$, então existe $g\in V^*$ tal que $\varphi=T^t(g)$, aí para todo $u\in U$ nós temos $\varphi(u)=T^t(g)(u)=g(T(u))$. Se $u\in\mathrm{Ker}(T)$ então $T(u)=0$, aí $\varphi(u)=0$; logo $\varphi\in(\mathrm{Ker}(T))^0$. Além disso:
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Ker}(T))^0
\]
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T))
\]
aí $\dim(\mathrm{Ker}(T))^0=\dim(\mathrm{Im}(T))$, aí $(\mathrm{Ker}(T))^0=\mathrm{Im}(T)$.
\end{itemize}
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais de dimensão finita com bases $B$ e $C$ e bases duais $B^*$ e $C^*$. Se $T\in\mathcal{L}(U,V)$, então:
\[
([T]_{B,C})^t=[T^t]_{C^*,B^*}
\]
\end{teorema}

\begin{corolario}
Se $A\in M_{m,n}(K)$, então:
\[
\mathrm{Row Rank}(A)=\mathrm{Column Rank}(A).
\]
\end{corolario}
\begin{proof}
Consideremos $T:K^n\rightarrow K^m$ dada por $T(v)=Av$. Sejam $B$ e $C$ as bases canônicas de $K^n$ e $K^m$, então $[T]_{B,C}=A$. Temos:
\[
\begin{array}{ccccc}
\mathrm{Rank}(T)&=&\mathrm{Column Rank}(A)&&\\
\mathrm{Rank}(T^t)&=&\mathrm{Column Rank}(A^t)&=&\mathrm{Row Rank}(A).
\end{array}
\]

\noindent
\end{proof}

\section{Espaços Quocientes}

\begin{definicao}
Seja $V$ um espaço, $W\subseteq V$ um subespaço. Para $u,v\in V$, digamos que $u\sim v$ se e só se $u-v\in W$. Então $\sim$ é uma relação de equivalência, ou seja:
\begin{itemize}
\item Reflexiva, ou seja, $v\sim v$ sempre.
\item Simétrica, ou seja, se $v\sim u$ então $u\sim v$.
\item Transitiva, ou seja, se $v\sim u$ e $u\sim w$, então $v\sim w$.
\end{itemize}
Seja $V/W$ o conjunto das classes de equivalência relativamente a $\sim$. Para $v\in V$ seja $\overline{v}$ a classe de equivalência de $v$.
\begin{itemize}
\item Definamos em $V/W$ uma estrutura de espaço vetorial. Para $\overline{v},\overline{w}\in V/W$ definamos $\overline{v}+\overline{w}=\overline{v+w}$.
\item Para $\alpha\in K$ e $\overline{v}\in V$ definamos $\alpha\cdot\overline{v}=\overline{\alpha v}$. Então $V/W$ é um espaço vetorial chamado \textbf{espaço quociente}.
\end{itemize}
\end{definicao}

\begin{observacao}
As operações estão ``bem definidas'' pois:
\begin{itemize}
\item Se $\overline{v}=\overline{v'}$ e $\overline{u}=\overline{u'}$, então $v\sim v'$ e $u\sim u'$, aí $v-v',u-u'\in W$, aí $(v+u)-(v'+u')=(v-v')+(u-u')\in W$, aí $\overline{v+u}=\overline{v'+u'}$, aí $\overline{v}+\overline{u}=\overline{v'}+\overline{u'}$.
\item Analogamente para a outra propriedade.
\end{itemize}
Também verificaremos algumas propriedades, deixando o resto ao leitor.
\begin{itemize}
\item Temos a comutatividade da adição, pois $\overline{u}+\overline{v}=\overline{v}+\overline{u}$ equivale a $\overline{u+v}=\overline{v+u}$, que é verdade pois $u+v=v+u$.
\item O que é o $\overline{0}$ de $V/W$? Temos $\overline{0}=W$, e também para todo $w\in W$ temos $w\sim 0$, aí $\overline{w}=\overline{0}=W$.
\end{itemize}
Também temos o seguinte:
\begin{itemize}
\item Se $W=V$, então $V/V=\{\overline{0}\}$.
\item Se $W=\{0\}$, então $V/\{0\}\cong V$.
\end{itemize}
\end{observacao}

\begin{proposicao}
Consideremos a aplicação:
\[
\pi:V\rightarrow V/W,\quad\quad v\mapsto\overline{v}.
\]
Então $\pi\in\mathcal{L}(V,V/W)$, com $\mathrm{Ker}(\pi)=W$.
\end{proposicao}

\begin{notacao}
$\pi$ chama-se a \textbf{projeção canônica} de $V$ para $V/W$.
\end{notacao}

\begin{proof}
Temos o seguinte:
\begin{itemize}
\item $\pi(v+u)=\overline{v+u}=\overline{v}+\overline{u}=\pi(v)+\pi(u)$.
\item $\pi(\alpha v)=\overline{\alpha v}=\alpha\overline{v}=\alpha\pi(v)$.
\end{itemize}
Além disso, se $w\in W$ então $\pi(w)=\overline{w}=W$
\end{proof}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$ e $W\subseteq U$ tal que $W\subseteq\mathrm{Ker}(T)$. Então existe um único $\overline{T}\in\mathcal{L}(U/W,V)$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
\end{proposicao}
\begin{proof}
Temos o seguinte:

\medskip
\noindent
1) Mostraremos que $\overline{T}$ está ``bem definida''. Se $\overline{u}=\overline{v}$, então $u-v\in W\in\mathrm{Ker}(T)$, aí $T(u-v)=0$, aí $T(u)=T(v)$.

\medskip
\noindent
2) Mostraremos que $\overline{T}$ é uma transformação linear.

\begin{itemize}
\item $\overline{T}(\overline{u}+\overline{v})=\overline{T}(\overline{u+v})=T(u+v)=T(u)+T(v)=\overline{T}(\overline{u})+\overline{T}(\overline{v})$.
\item 
\end{itemize}
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e seja $T\in\mathcal{L}(U,V)$. Então $U/\mathrm{Ker}(T)\cong\mathrm{Im}(T)$.
\end{teorema}
\begin{proof}
Pela proposição anterior, existe uma única $\overline{T}:U/\mathrm{Ker}(T)\rightarrow V$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
Observemos que $\mathrm{Im}(\overline{T})=\mathrm{Im}(T)=\{T(u)\mid u\in U\}$.

\medskip
\noindent
Além disso, para $\overline{u}\in\mathrm{Ker}(\overline{T})$, então $T(u)=\overline{T}(\overline{u})=0$, aí $u\in\mathrm{Ker}(T)$, aí $\overline{u}=\overline{0}$, de modo que $\overline{T}$ é injetora.
\end{proof}

\begin{teorema}
Seja $W$ subespaço de $V$. Então todos os complementos de $W$ em $V$ são isomorfos ao $V/W$.
\end{teorema}
\begin{proof}
Seja $V=W\oplus U$. Consideremos a projeção canônica:
\[
\pi:V\rightarrow V/W.
\]
Seja $\overline{\pi}=\pi\upharpoonright U$. Então $\mathrm{Ker}(\overline{\pi})=U\cap\mathrm{Ker}(\pi)=U\cap W=\{0\}$. Logo $\overline{\pi}$ é injetora.

\medskip
\noindent
Para $\overline{v}\in V/W$, seja $v=w+u$, com $w\in W$ e $u\in U$. Então $\pi(v)=\pi(w)+\pi(u)=\pi(u)=\overline{\pi}(u)$, aí $\overline{v}=\overline{\pi}(u)$, assim $\overline{\pi}$ é sobre $V/W$.
\end{proof}

\begin{corolario}
Seja $W\subseteq V$ um subespaço. Então $\dim V=\dim W+\dim V/W$.
\end{corolario}
\begin{proof}
Seja $V=W\oplus U$, então $\dim V=\dim W+\dim U$, mas $U\cong V/W$, aí $\dim U=\dim V/W$.
\end{proof}

\begin{observacao}
Existem espaços vetoriais $W$ e $U$ e $W'$ e $U'$ tais que $W\oplus U\cong W'\oplus U'$ e $W\cong W'$, mas $U\ncong U'$. De fato podemos tomar $W=\bigoplus_{i=0}^\infty Ke_{2i}$ e $U=\bigoplus_{i=0}^\infty Ke_{2i+1}$ e $W'=\bigoplus_{i=0}^\infty Ke_i$ e $U'=\{0\}$.
\end{observacao}

\chapter{Determinantes}

\section{Formas Multilineares}

\begin{definicao}
Seja $V$ um espaço vetorial e $V^r=V\times\dots\times V$. Uma \textbf{forma $r$-linear} sobre $V$ é uma função:
\[
F:V^r\rightarrow K,\quad\quad (v_i)_{i\in r}\mapsto F((v_i)_{i\in r})\in K
\]
que é linear em cada argumento, ou seja, para cada $i\in r$ temos que
\[
F(v_0,\dots,\alpha v_i+\beta v'_i,\dots,v_{r-1})=\alpha F(v_0,\dots,v_i,\dots,v_{r-1})+\beta F(v_0,\dots,v'_i,\dots,v_{r-1}).
\]
Denotamos por $L_r(V)$ o conjunto das formas $r$-lineares sobre $V$.
\end{definicao}

\begin{exemplo}
Seja $V=K^2$ e:
\[
F((x_0,y_0),(x_1,y_1),(x_2,y_2))=x_0y_1x_2-x_0x_1x_2.
\]
Então $F$ é uma forma 3-linear.
\end{exemplo}

\begin{definicao}
Uma forma $F\in L_r(V)$ chama-se \textbf{alternativa} se e só se para $v\in V^r$, se $v$ não é injetora, então $F(v)=0$. Denotamos por $A_r(V)$ o conjunto das formas $r$-lineares alternativas.
\end{definicao}

\begin{definicao}
Uma forma $F$ é chamada \textbf{antissimétrica} se para $v\in V^r$ e para $i,j\in r$ tais que $i\neq j$, então:
\[
F(v_0,\dots,v_i,\dots,v_j,\dots,v_{r-1})=-F(v_0,\dots,v_j,\dots,v_i,\dots,v_{r-1}).
\]
\end{definicao}

\begin{proposicao}
Toda forma alternativa é antissimétrica.
\end{proposicao}
\begin{proof}
Seja $F\in A_r(V)$. Sejam $v\in V^r$ e $i,j\in r$ tais que $i\neq j$. Então:
\[
\begin{array}{rcl}
0&=&F(v_0,\dots,v_i+v_j,\dots,v_i+v_j,\dots,v_{r-1})\\&=&F(v_0,\dots,v_i,\dots,v_i,\dots,v_{r-1})+F(v_0,\dots,v_i,\dots,v_j,\dots,v_{r-1})\\&&+F(v_0,\dots,v_j,\dots,v_i,\dots,v_{r-1})+F(v_0,\dots,v_j,\dots,v_j,\dots,v_{r-1})\\&=&F(v_0,\dots,v_i,\dots,v_j,\dots,v_{r-1})+F(v_0,\dots,v_j,\dots,v_i,\dots,v_{r-1})
\end{array}
\]
\end{proof}

\begin{proposicao}
Se a característica do corpo é $\neq 2$, então toda forma antissimétrica é reflexiva.
\end{proposicao}
\begin{proof}
Para $F$ antissimétrica e $v\in V^r$ e $i,j\in r$ tais que $i\neq j$, se $v_i=v_j$, sendo $v=v_i$, então:
\[
F(v_0,\dots,v,\dots,v,\dots,v_{r-1})=-F(v_0,\dots,v,\dots,v,\dots,v_{r-1}),
\]
aí:
\[
2F(v_0,\dots,v,\dots,v,\dots,v_{r-1})=0,
\]
aí:
\[
F(v_0,\dots,v,\dots,v,\dots,v_{r-1})=0.
\]
\end{proof}

\begin{definicao}
Seja $F\in L_r(V)$ e $\sigma\in S_r$ uma permutação. Então, para $v\in V^r$, definamos $(\sigma F)(v)=F(v\circ\sigma)$. Então é fácil ver que $\sigma F\in L_r(V)$.
\end{definicao}

\begin{observacao}
Para $F\in L_r(V)$, então $F$ é antissimétrica se e somente se para toda transposição $\tau\in S_r$ tivermos $\tau F=-F$.
\end{observacao}

\begin{proposicao}
Seja $F\in L_r(V)$ uma forma antissimétrica. Então para $\sigma\in S_r$, temos $\sigma F=(\sgn\sigma)F$.
\end{proposicao}
\begin{proof}
Para $\sigma\in S_r$, então $\sigma$ pode ser escrita como $\sigma=\tau_0\dots\tau_{k-1}$, em que $\tau_i$ são transposições, e $\sigma$ é par se e só se $k$ é par.

\medskip
\noindent
Temos $\sigma F=(\tau_0\dots\tau_{k-1})F=(-1)^k F=(\sgn\sigma)F$, pois $\sgn\sigma=(-1)^k$.
\end{proof}

\begin{proposicao}
Toda forma $r$-linear determina uma forma $r$-linear alternada da seguinte maneira:
\[
F\mapsto\varphi(F)=\sum_{\sigma\in S_r}\sgn\sigma(\sigma F).
\]
\end{proposicao}
\begin{proof}
Seja $v_i=v_j=v$ com $i\neq j$. Precisamos provar que $\varphi(F)(v)=0$. Seja $\tau$ a transposição $(i,j)$, então $S=A_r\cup A_r\tau$ e $A_r\cap A_r\tau=\emptyset$. Então temos o seguinte:
\[
\begin{array}{rcl}
\varphi(F)(v)&=&\sum_{\sigma\in S_r}(\sgn\sigma)(\sigma F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma\tau F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma F(v))\\&=&0
\end{array}
\]
\end{proof}

\begin{observacao}
Se $F\in A_r(V)$ e $v\in V^r$ é linearmente dependente, então:
\[
F(v)=0.
\]
\end{observacao}

\begin{lema}
Seja $\dim V=n$ e $F\in A_n(V)$. Seja $(e_i)_{i\in n}$ uma base de $V$, então $F$ é completamente determinada pelo valor $F(e)$.
\end{lema}
\begin{proof}
Seja $v\in V^n$. Então existe $\alpha:n\times n\rightarrow K$ tal que:
\[
v_i=\sum_{j\in n}\alpha_{i,j}e_j.
\]
Assim:
\[
\begin{array}{rcl}
F(v)&=&F((\sum_{j\in n}\alpha_{i,j}e_j)_{j\in n})\\&=&\sum_{j\in n^n}\prod_{i\in n}\alpha_{i,j(i)}F(e\circ j)\\&=&\sum_{\sigma\in S_n}\prod_{i\in n}\alpha_{i,\sigma(i)}F(e\circ \sigma)\\&=&\textcolor{red}{\left(\sum\limits_{\sigma\in S_n}\prod_{i\in n}\alpha_{i,\sigma(i)} \sgn\sigma \right)} F(e).
\end{array}
\]
\end{proof}

\noindent
Note então que o valor $\sum\limits_{\sigma\in S_n}\prod_{i\in n}\alpha_{i,\sigma(i)} \sgn\sigma$ \emph{determina} $F$ para qualquer $v \in V^n.$ Chamaremos este valor de \textbf{determinante} de $F.$

\begin{exemplo}

\end{exemplo}

\section{Determinantes}

Seja $K$ um corpo e consideremos o anel das matrizes $M_n(K)$. Identificaremos os elementos de $M_n(K)$ com os elementos de $(K^n)^n$ assim:
\[
A=(a_{i,j})_{(i,j)\in n\times n}\leftrightarrow ((a_{i,j})_{j\in n})_{i\in n}
\]
Portanto, uma função $n$-linear aqui é uma função $n$-linear nas linhas da matriz.

\begin{definicao}
Uma função $\det:M_n(K)\rightarrow K$ é dita uma função \textbf{determinante} se e só se $\det$ é $n$-linear alternada e $\det(I)=1$.
\end{definicao}

\noindent
Pelo que vimos, existe e é única a função determinante: É a forma $n$-linear alternada que vale $1$ na base canônica de $K^n$.

\medskip
\noindent
Logo, se $A=(a_{i,j})\in M_n(K)$, então:
\[
\det(A)=\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{i,\sigma(i)}.
\]

\begin{exemplo}
Para $n=2$, temos $S_2=\{I,(0,1)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{0,0}&a_{0,1}\\a_{1,0}&a_{1,1}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{0,0}a_{1,1}-a_{0,1}a_{1,0}.
\]
\end{exemplo}

\begin{exemplo}
Agora, se $n=3$, então $S_3=\{I,(0,1,2),(0,2,1),(0,1),(0,2),(1,2)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{0,0}&a_{0,1}&a_{0,2}\\a_{1,0}&a_{1,1}&a_{1,2}\\a_{2,0}&a_{2,1}&a_{2,2}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{0,0}a_{1,1}a_{2,2}+a_{0,1}a_{1,2}a_{2,0}+a_{0,2}a_{1,0}a_{2,1}-a_{0,1}a_{1,0}a_{2,2}-a_{0,2}a_{1,1}a_{2,0}-a_{0,0}a_{1,2}a_{2,1}.
\]
\end{exemplo}

\begin{proposicao}
Temos as seguintes propriedades:
\begin{itemize}
\item[1)] Para todo $A\in M_n(K)$ temos $\det(A)=\det(A^t)$.
\item[2)] Para $A,B\in M_n(K)$ vale $\det(AB)=\det(A)\det(B)$.
\item[3)] Para $A\in M_n(K)$, então $A$ é inversível se e só se $\det(A)\neq 0$. Neste caso, temos $\det(A^{-1})=(\det(A))^{-1}$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Sendo $A=(a_{i,j})_{(i,j)\in n\times n}$, então temos:
\[
\begin{array}{rcl}
\det(A)&=&\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{i,\sigma(i)}\\&=&\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{\sigma^{-1}(i),i}\\&=&\sum_{\tau\in S_n}\sgn(\tau^{-1})\prod_{i\in n}a_{\tau(i),i}\\&=&\sum_{\tau\in S_n}\sgn(\tau)\prod_{i\in n}a^t_{i,\tau(i)}\\&=&\det(A^t).
\end{array}
\]
\item[2)] Seja $F_A:M_n(K)\rightarrow K$ tal que $\forall X\in M_n(K):F_A(X)=\det(AX)$. Então a função $F_A$ é uma função $n$-linear alternada sobre as colunas, mas também $F_A(I)=\det(A)$, aí $F_A(B)=\det(A)\det(B)$, assim $\det(AB)=\det(A)\det(B)$
\item[3)] Se $A$ é inversível, então existe a inversa $A^{-1}$, assim $1=\det(I)=\det(AA^{-1})=\det(A)\det(A)^{-1}$, aí $\det(A)\neq 0$ e $\det(A^{-1})=(\det(A))^{-1}$. Por outro lado, se $\det(A)\neq 0$, então $\det(A^t)\neq 0$, aí as colunas de $A$ são linearmente independentes, aí consideremos $T:K^n\rightarrow K^n$ tal que $[T]_{\mathrm{can}}=A$, então $T$ é inversível, assim $A=[T]_{\mathrm{can}}$ é inversível.
\end{itemize}
\end{proof}

\noindent
Assim lembremo-nos do seguinte: a função $\det$ é uma função $n$-linear e alternada nas linhas (ou nas colunas) da matriz, logo:
\begin{itemize}
\item[1)] Trocar duas linhas (ou colunas) da matriz muda o sinal do determinante.
\item[2)] Somar a uma linha (ou coluna) uma combinação linear das demais linhas (colunas) não altera o valor do determinante.
\item[3)] Ao multiplicar uma linha (ou coluna) por um escalar, o determinante fica multiplicado por esse escalar.
\end{itemize}

\begin{proposicao}
Temos o seguinte:
\begin{itemize}
\item[1)] O determinante de uma matriz triangular é o produto dos elementos da diagonal da matriz.
\item[2)] Se:
\[
A=\begin{pmatrix}
B&0\\C&D
\end{pmatrix}
\]
em que $B\in M_r(K)$ e $D\in M_{n-r}(K)$ e $C\in M_{n-r,r}(K)$ e $0\in M_{r,n-r}(K)$, então:
\[
\det(A)=\det(B)\det(D).
\]
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Seja $A=(a_{i,j})_{(i,j)\in n\times n}$ uma matriz triangular inferior, então para $i,j\in n$ tais que $i<j$ temos $a_{i,j}=0$, mas a única permutação $\sigma\in S_n$ tal que $\forall i\in n:i\geq\sigma(i)$ é a identidade, assim temos:
\[
\det(A)=\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i\in n}a_{i,\sigma(i)}=\sgn(I)\prod_{i\in n}a_{i,I(i)}=\prod_{i\in n}a_{i,i}.
\]
\item[2)] Seja $F:M_r(K)\rightarrow K$ tal que:
\[
F(X)=\det\begin{pmatrix}
X&0\\C&D
\end{pmatrix}.
\]
Então $F$ é $r$-linear alternada nas linhas de $X$, assim $F(X)=F(I)\det(X)$.

\medskip
\noindent
Agora consideremos $G:M_{n-r}(K)\rightarrow K$ tal que:
\[
G(Y)=\det\begin{pmatrix}
I&0\\C&Y
\end{pmatrix}
\]
Então $G$ é $(n-r)$-linear alternada nas colunas de $Y$, logo $G(Y)=G(I)\det(Y)$. Mas:
\[
G(I)=\det\begin{pmatrix}
I&0\\C&I
\end{pmatrix}=1,
\]
assim $G(Y)=\det(Y)$, aí $F(I)=G(D)=\det(D)$, assim $F(X)=F(I)\det(X)=\det(X)\det(D)$, aí acaba.
\end{itemize}
\end{proof}

\noindent
Agora temos a \textbf{regra de Laplace}:

\begin{teorema}
Dada $A\in M_n(K)$, indicaremos por $M_{i,j}$ a matriz quadrada de tamanho $n-1$ obtida a partir de $A$ eliminando a linha $i$ e a coluna $j$.

\smallskip
\noindent
Para cada $i\in n$, então vale:
\[
\det(A)=\sum_{j\in n}(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
Para cada $j\in n$, então vale:
\[
\det(A)=\sum_{i\in n}(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
\end{teorema}
\begin{proof}
Provaremos a primeira afirmação pois a segunda é análoga.
\end{proof}
\textcolor{red}{AULA DE 19 DE AGOSTO (COLOCAREI ASSIM QUE CONSEGUIR)- FICOU
  FALTANDO A PROVA DA REGRA DE LAPLACE E A PARTE DE MATRIZES SOBRE ANEIS COMUTATIVOS}
Bláa blá blá

\chapter{Formas Canônicas}

\section{Autovalores e Autovetores}

\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). Um vetor \(v\in V\) é um \textbf{autovetor} de \(T\) se existe \(\lambda\in K\) tal que \(T(v)=\lambda v\). O escalar \(\lambda\) é um \textbf{autovalor} do operador \(T\). Dizemos que \(v\) é um autovetor associado com o autovalor \(\lambda\).
\end{definicao}

\begin{exemplo}
Seja \(V=\mathbb{C}^1(\mathbb{R})\) e considere o operador linear \(T\in\mathcal{L}(V)\) tal que \(T(v)=v^\prime\) para cada \(v\in V\). Considere \(v=e^{\lambda x}\) com \(\lambda\in K\). Então \(T(v)=\lambda e^{\lambda x}=\lambda v\). Ou seja \(v\) é um autovetor associado com o autovalor \(\lambda\).
\end{exemplo}

\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \emph{spectrum} do operador \(T\) é o conjunto
\[\text{Spec}(T)\coloneqq\{\lambda\in K\,\colon \lambda \text{ é autovalor de } T\}.\]
Para cada \(\lambda\in\text{Spec}(T)\), denotamos o conjunto dos autovetores associados a \(\lambda\) como \(V_T(\lambda)\).
\end{definicao}
No contexto da definição anterior, considere
\(\lambda\in\text{Spec}(T)\). Então
\begin{align*}
v\in V_T(\lambda)&\iff T(v)=\lambda v\\&\iff (T-\lambda I)(v)=0\\&\iff v\in\text{Ker}(T-\lambda I).
\end{align*}

\noindent
\textcolor{red}{ALGUMA COISA QUE EU PERDI}

\medskip
\noindent
Ainda no mesmo contexto, vamos assumir agora que \(\text{dim}(V)=n<\infty\). Então temos 
que \[\lambda\in\text{Spec}(T)\implies\text{Ker}(T-\lambda I)\not =\{0\}\implies\text{det}(T-\lambda I)=0.\]
Reciprocamente, se \(\text{det}(T-\lambda I)=0\) então \(V_T(\lambda)=\text{Ker}(T-\lambda I)\not=\{0\}\).
\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \emph{ polinômio característico} de \(T\) é a função \(p_T(\lambda)\colon K\to K\) dada por
\[p_T(\lambda)\coloneqq\text{det}(T-\lambda I), \text{ para cada }\lambda\in K.\]
\end{definicao}
Note que \(\lambda\in\text{Spec}(T)\) se e só se \(\lambda\) é raiz de \(p_T(\lambda)\). Além disso, note que se  \(B\) e \(B^\prime\) são bases \(V\), então \(p_T(\lambda)=p_{[T]_B}(\lambda)\). De fato, se \(P\) é a matriz de mudança da base \(B\) para a base \(B^\prime\), então
\begin{align*}
    [\lambda I - T]_{B^\prime}&=P^{-1}[\lambda I - T]_{B}P
\end{align*}
Isso implica que \[\text{det}([\lambda I- T]_{B^\prime})=\text{det}(P^{-1})\text{det}([\lambda I - T]_B)\text{det}(P).\]
Ou seja, \(\text{det}([\lambda I- T]_{B^\prime})=\text{det}([\lambda I - T]_B)\).
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^2)\) tal que \[[T]_{\text{can}}=\begin{pmatrix}
0 & -1\\1 & 0
\end{pmatrix}.\]
Isto é, \(T((x,y))=(-y,x)\) para cada \((x,y)\in\mathbb{R}^2\). Então 
\begin{align*}
p_T(\lambda)&=\text{det}\Bigg(\begin{pmatrix}
-\lambda & -1\\1 & -\lambda
\end{pmatrix}\Bigg)\\&=\lambda^2+1.
\end{align*}
Dessa forma, \(\text{Spec}(T)=\varnothing\) pois \(p_T(\lambda)\) não possui raízes em \(K=\mathbb{R}\).
\end{exemplo}
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que\[[T]_{\text{can}}=\begin{pmatrix}
3 & 1 & -1\\2 & 2 & -1\\ 2 & 2 & 0
\end{pmatrix}.\]
Então \begin{align*}
p_T(\lambda)&=\text{det}\left(\begin{pmatrix}
3-\lambda & 1 & -1\\2 & 2-\lambda & -1\\ 2 & 2 & -\lambda
\end{pmatrix}\right)\\&=(\lambda-1)^2(\lambda-2).
\end{align*}
Isso implica que \(\text{Spec}(T)=\{1,2\}\). Além disso, temos que
\[V_T(1)=\text{Ker}(T-I)=\text{Ker}(\begin{pmatrix}
2 & 1 & -1\\2 & 1 & -1\\ 2 & 2 & -1
\end{pmatrix})=\langle (1,0,2)\rangle.\]
e ainda
\[V_T(2)=\text{Ker}(T-2 I)=\text{Ker}(\begin{pmatrix}
1 & 1 & -1\\2 & 0 & -1\\ 2 & 2 & -2
\end{pmatrix})=\langle (1,1,2)\rangle\]
\end{exemplo}
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que
\[[T]_{\text{can}}=\begin{pmatrix}
1 & 2 & -1\\-2 & -3 & -1\\ 2 & 2 & -2
\end{pmatrix}.\]
Neste caso temos que \begin{align*}
p_T(\lambda)&=\text{det}(\begin{pmatrix}
1- \lambda & 2 & -1\\-2 & -3 - \lambda & -1\\ 2 & 2 & -2-\lambda
\end{pmatrix})\\&=(\lambda+1)^2(\lambda+2).    \end{align*}
Isso implica que \(\text{Spec}(T)=\{-1,-2\}\) e ainda \[V_T(-1)=\langle\{ (1,0,2),(0,1,2)\}\rangle\] e
\[V_T(-2)=\langle (1,-1,1)\rangle\].
Uma vez que os autovetores acima são L.I, eles formam uma base \(B\) de \(\mathbb{R}^3\) e 
\[[T]_B=\begin{pmatrix}
-1 & 0 & 0\\0 & -1 & 0\\ 0 & 0 & -2
\end{pmatrix}\]
é uma matriz diagonal.
\end{exemplo}
\begin{teorema}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que \(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\). São equivalentes:
\begin{enumerate}
    \item \(T\) é diagonalizável.
    \item \(p_T(t)=(t-\lambda_1)^{n_1}\ldots(t-\lambda_k)^{n_k}\) e \(\text{dim}(V_T(\lambda_i))=n_i\) para cada \(i\in [k]\).
    \item\(\text{dim}((V_T(\lambda_1))+\ldots+\text{dim}(V_T(\lambda_k))=\text{dim}(V)=n.\)
\end{enumerate}
\end{teorema}
\begin{lema}
Seja \(\{\lambda_i\}_{i\in [k]}\subseteq\text{Spec}(T).\)
\begin{enumerate}
    \item Se \(v_i \in V_T(\lambda_i)\) para cada \(i\in [k]\) e \(v_1+\ldots+v_k=0\), então \(v_1=\ldots=v_k=0\).
    \item Se \(B_i\subseteq V_T(\lambda_i)\) é L.I para cada \(i\in [k]\), então \(\bigcup_{i\in [k]}B_i\) é L.I.
\end{enumerate}
\end{lema}
\begin{proof}[Demonstração do Lema]\hfill
\begin{enumerate}
\item Vamos provar essa afirmação por indução em \(k\). Primeiro note que o
      resultado é trivial quando \(k=1\). Agora seja \(k\in\mathbb{N}\) e
      assuma  que o resultado vale para cada natural \(i < k\). Sejam
      \(v_1,\ldots,v_k\) tais que \(v_i\in V_T(\lambda_i)\) para cada \(\i\in
      [k]\) e \(v_1+\ldots + v_k=0\). Então temos que 
      \begin{equation}\label{primeira}
      \lambda_1(v_1+\ldots+v_k)=0\lambda_1=0.
    \end{equation}
    Além disso, é claro que
      \begin{equation}\label{segunda}
      T(v_1+\ldots +v_k)=\lambda_1v_1+\ldots +\lambda_kv_k=0.
      \end{equation}
      Subtraindo a Equação \ref{primeira} de \ref{segunda}, obtemos
      \begin{equation}\label{terceira}
       (\lambda_1-\lambda_1)v_1+(\lambda_2-\lambda_1)v_2+\ldots +(\lambda_k-\lambda_1)v_k=0.
     \end{equation}
     Agora notamos que cada termo no lado esquerdo é um autovetor de \(T\)     e
     aplicamos a hipótese de indução para concluir que \(v_2=\ldots =v_k=0
     \). Finalmente, como sabemos que \(v_1+\ldots +v_k=0\) e \(v_2=\ldots =
     v_k=0\), obtemos que \(v_1=0\) também, o que conclui nossa prova.
     \item Seja \(S\subseteq\bigcup_{i\in [k]}B_i\) finito e seja \(\alpha\colon S\to\mathbb{R}\). Note que
       \(V_T(\lambda_i)\cap V_T(\lambda_j)=\{0\}\) sempre que \(i,j\in [k]\) e
       \(i\not=j\) e então podemos escrever
       \[\sum_{v\in S}\alpha_iv_i=\sum_{v\in S_1}\alpha_v v+\ldots+\sum_{v\in
           S_k}\alpha_vv,\]
       onde \(S_i\subseteq B_i\) é finito para cada \(i\in [k]\). Utilizan
       do o fato de que o termo \mbox{\(\sum_{v\in S_i}\alpha_v v\in V_T(\lambda_i)\)}
       para cada \(i\in [k]\) e aplicando o item anterior, obtemos que
       \[\sum_{v\in S_1}\alpha_vv=\ldots=\sum_{v\in S_k}\alpha_vv=0.\]
       Finalmente como como \(S_i\subseteq B_i\) para cada \(i\in [k]\) e
       \(B_i\) é sempre L.I por hipótese segue que a restrição de \(\alpha\) a
       cada \(S_i\) é identicamente nula. Como \(S=\bigcup_{i\in [k]}S_i\) segue
       que \(\alpha\) é indenticamente nula.\qedhere
    \end{enumerate}
\end{proof}

\begin{proof}
\begin{itemize}
\item (i)$\Rightarrow$(ii): Sejam $(v_{i,j})_{j\in n_i}$ autovetores associados
  a $\lambda_i\in\text{Spec}(T)$\textcolor{red}{TERMINAR ESSA IMPLICAÇÃO}  
\item (ii)\(\Rightarrow\)(iii):\[\text{dim}(V_T(\lambda_1))+\ldots +\text{dim}(V_T(\lambda_k))=n_1+\ldots +n_k=\text{deg}(p_T(t))=\text{dim}(V)=n.\]
\item (iii)\(\Rightarrow\)(i): Para cada \(i\in [k]\) considere uma base \(B_i\)
  de \(V_T(\lambda_i)\). Seja \(B=\bigcup_{i\in [k]}B_i\). Pelo lema anterior,
  temos que \(B\) é L.I. Como \(\vert B\vert =n\) segue que \(B\) é uma base de
  \(V\). Além disso, \(B\) é uma base de autovetores de \(T\). Logo, \(T\) é diagonalizável.
\end{itemize}
\end{proof}

\section{Polinômio Minimal}

\begin{definicao}
Seja $V$ um espaço sobre $K$, $\dim V=n<\infty$, $T\in\mathcal{L}(V)$. Definamos por recursão $T^0=I$ e $T^{k+1}=T^k\circ T$. Se $p(t)\in K[t]$, $p(t)=a_0+a_1t+\cdot+a_mt^m$, então está bem definido o operador $p(T)=a_0\cdot I+a_1\cdot I+\dots+a_m\cdot T^m\in\mathcal{L}(V)$.
\end{definicao}

\noindent
Lembremo-nos de que, se $\dim(U)=m$ e $\dim(V)=n$, então $\dim\mathcal{L}(U,V)=mn$. Assim, se $V$ é um espaço vetorial tal que $\dim(V)=n<\infty$, então $\dim\mathcal{L}(V)=n^2$, de modo que existe $m\leq n^2+1$ tal que os operadores $I,T,T^2,\dots,T^m$ sejam linearmente dependentes. Seja $m$ um número minimal tal que $T^m\in\langle I,T,\dots,T^{m-1}\rangle$. Então $T^m=a_0\cdot I+a_1\cdot T+\dots a_{m-1}T^{m-1}$, com $a_i\in K$. Seja $m_T(t)=t^m-a_{m-1}t^{m-1}-\dots-a_1t-a_0$, então $m_T(t)=0$, e $m_T(t)$ é um polinômio de menor grau tal que $m_T(t)=0$.

\begin{definicao}
Um polinômio mônico de grau mínimo tal que $m_T(t)\in K[t]$ tal que $m_T(t)=0$ chama-se um \textbf{polinômio minimal} do operador $T$. Chamemos um polinômio $f(t)\in K[t]$ de um \textbf{polinômio anulador} de $T$ se $f(T)=0$.
\end{definicao}

\begin{lema}
Seja $f(t)\in K[t]$ tal que $f(T)=0$. Então $m(t)\mid f(t)$.
\end{lema}
\begin{proof}
Dividimos $f(t)$ por $m(t)$ (com resto):
\[
f(t)=m_T(t)\cdot q(t)+r(t),\quad\quad\deg(r(t))<\deg(m_T(t))\text{ ou }r(t)=0.
\]
Como $f(T)=0$ e $m_T(t)=0$, então $r(T)=0$, aí $r(t)=0$.
\end{proof}

\begin{corolario}
O polinômio $m_T(t)$ é único.
\end{corolario}

\begin{definicao}
Dado $v\in V$, um polinômio mônico de grau mínimo $m_{T,v}(t)\in K[t]$ tal que $m_{T,v}(T)(v)=0$ chama-se um \textbf{polinômio $T$-minimal} do vetor $v$. Chamemos um polinômio $f(t)\in K[t]$ de um \textbf{polinômio $T$-anulador} de $v$ se $f(T)(v)=0$.
\end{definicao}

\begin{lema}
Seja $f(t)\in K[t]$ tal que $f(T)(v)=0$. Então $m_{T,v}(t)\mid f(t)$.
\end{lema}
\begin{proof}
Dividimos $f(t)$ por $m_{T,v}(t)$ (com resto):
\[
f(t)=m_{T,v}(t)\cdot q(t)+r(t),\quad\quad\deg(r(t))<\deg(m_{T,v}(t))\text{ ou }r(t)=0.
\]
Como $f(T)(v)=0$ e $m_{T,v}(v)=0$, então $r(T)(v)=0$, aí $r(t)=0$.
\end{proof}

\begin{corolario}
O polinômio $m_{T,v}(t)$ é único.
\end{corolario}

\noindent
Se $V$ é um espaço vetorial e $T\in\mathcal{L}(V)$, então $V$ tem uma estrutura de $K[t]$ módulo à esquerda: Se $f(t)\in K[t]$, para $v\in V$ definimos:
\[
f(t)\cdot v=f(T)(v).
\]
Além disso, se considerarmos:
\[
\begin{array}{rcl}
\varphi:K[t]&\rightarrow&\mathrm{End}(V)\\f(t)&\mapsto&f(T),
\end{array}
\]
então $\varphi$ é um homomorfismo de $K$-álgebras e portanto $\mathrm{Ker}(\varphi)$ é um ideal de $K[t]$.

\begin{teorema}
Os polinômios $p_T(t)$ e $m_T(t)$ têm as mesmas raízes em $K$ (a menos de multiplicidade). Em outras palavras, $m_T(\lambda)=0\Leftrightarrow\lambda\in\mathrm{Spec}(T)$.
\end{teorema}
\begin{proof}
Se $m_T(\lambda)=0$, então $m_T(t)=(t-\lambda)q(t)$. Por minimalidade de $m_T(t)$, $q(T)\neq 0$, então existe $w\in V$ tal que $q(T)(w)\neq 0$, aí seja $v=q(T)(w)$, então $v\neq 0$ e:
\[
\begin{array}{rcl}
(T-\lambda I)(v)&=&(T-\lambda I)q(T)(w)\\&=&m_T(t)(w)=0,
\end{array}
\]
aí $T(v)=\lambda v$, aí $\lambda\in\mathrm{Spec}(T)$.

\medskip
\noindent
Por outro lado, se $\lambda\in\mathrm{Spec}(T)$, seja $v\in V$ tal que $v\neq 0$ e $T(v)=\lambda v$, então $T(T(v))=\lambda^2 v,\dots,T^m(v)=\lambda^m v,\dots$, aí para $f(t)\in K[t]$ temos $f(T)(v)=f(\lambda)\cdot v$, aí $0=m_T(T)(v)=m_T(\lambda)\cdot v$, aí $m_T(\lambda)=0$.
\end{proof}

\begin{corolario}
Se $T$ é diagonalizável e $\mathrm{Spec}(T)=\{\lambda_i\}_{i\in r}$, então $m_T(t)=\prod_{i\in r}(t-\lambda_i)$.
\end{corolario}

\noindent
Se $\mathrm{Spec}(T)=\{\lambda_i\}_{i\in r}$, então:
\[
V=\sum_{i\in r}V_T(\lambda_i).
\]

\begin{proof}
Já sabemos que $m_T(t)=\left(\prod_{i\in r}(t-\lambda_i)^{k_i}\right)$ em que $q(t)$ não tem raízes em $K$. Basta provar que $(T-\lambda_0 I)\dots(T-\lambda_{r-1} I)=0$. Seja $v\in V$, $v=v_0+\dots+v_{r-1}$, com $v_i\in V_T(\lambda_i)$, então temos $(T-\lambda_i I)(v_i)=0$, portanto $()(v_i)=0$, e logo $()(v)=0$. Então o polinômio $f(t)=(t-\lambda_0)\dots$ é um polinômio anulador para $T$, aí $m_T(t)\mid f(t)$, aí $m_T(t)=f(t)$.
\end{proof}

\section{Subespaços Invariantes}

\begin{definicao}
Seja $T\in\mathcal{L}(V)$. Um subespaço $W\subseteq V$ chama-se \textbf{$T$-invariante} se $T(W)\subseteq W$.
\end{definicao}

\begin{observacao}
Um subespaço é $T$-invariante se e só se é um $K[t]$-submódulo.
\end{observacao}
\begin{exemplo}
Seja \(V=\mathbb{C}(\mathbb{R})\) e considere o operador \(D\colon f\to
f^\prime\). Então o subespaço\[P_n\coloneqq\{f(t)\in\mathbb{R}[t]\,\colon
  \text{deg}(f)\leq n\}\] é \(D\)-invariante. 
\end{exemplo}
  \noindent
Seja $\dim(V)=n$ e $T\in\mathcal{L}(V)$, $W\subseteq V$ um subespaço
$T$-invariante. Escolhemos uma base $B_1=\{v_i\}_{i\in m}$ de $W$ e
completemo-la até uma base $B=\{v_i\}_{i\in n}$ do espaço $V$. Qual é a matriz
$[T]_B$?

Vamos começar notando que \(T\) é \(W\)-invariante e então 
\begin{align*}
&T(v_1)=\sum_{i\in m}\alpha_{1i}v_i\\&T(v_2)=\sum_{i\in m}\alpha_{2i}v_i\\&\ldots\\&T(v_m)=\sum_{i\in m}\alpha_{mi}v_i\\&T(v_{m+1})=\sum_{i\in n}\alpha_{(m+1)i}v_i\\&\ldots\\&T(v_n)=\sum_{i\in n}\alpha_{ni}v_i.
\end{align*}

Dessa forma segue que
\[[T]_B=\begin{pmatrix}
    \alpha_{11} & \ldots & \alpha_{1m} &\alpha_{1(m+1)}& \ldots & \alpha_{1n}
    \\ \ldots & \ldots & \ldots & \ldots &\ldots & \ldots
    \\\alpha_{m1} & \ldots & \alpha_{mm} & \alpha_{m(m+1)} & \ldots &
    \alpha_{mn}\\ 0 & \ldots & 0 & \alpha_{(m+1)(m+1)}&\ldots &\alpha_{(m+1)n}\\
    \ldots & \ldots & \ldots & \ldots &\ldots & \ldots \\ 0 & 0 & 0
    &\alpha_{n(m+1)} & \ldots & \alpha_{nn}
\end{pmatrix}.\]

Isto é, a matriz de \(T\) na base \(B\) tem a forma

\[[T]_B=\begin{pmatrix}
    A & * \\ 0 & B\end{pmatrix},\]
onde \(A\in M_m(K)\) e \(B\in M_{n-m}(K)\). Note que \(A\) é a matriz da
restrição de \(T\) a \(W\) na base \(B_1\).

Vamos agora considerar a restrição de \(T\) a \(W\), denotada por
\(T\upharpoonright W\). Claramente, temos que \(T\upharpoonright
W\in\mathcal{L}(W)\). Considere \(\bar{V}\coloneqq\frac{V}{W}\) e seja 
\(\pi\colon V\to\bar{V}\) uma projeção. Seja \(\bar{T}\coloneqq \pi\circ T\).
Então \(\bar{T}\in\mathcal{L}(V,\bar{V})\) e \(W\subseteq\text{Ker}(\bar{T})\).
De fato, para cada \(w\in W\) temos \[\pi(T(w))\in \pi(W)=0.\]
Além disso, \(\bar{T}\) induz um operador linear
\(\tilde{T}\in\mathcal{L}(\bar{V})\) definido por \(\tilde{T}(v+W)=\bar{T}(v)\).

\begin{proposicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\), seja \(T\in\mathcal{L}(V)\)
e seja \(W\) um subespaço \(T\)-invariante de \(V\). Seja $B_1$ uma base de $W$ e seja $B$ uma base de $V$ tal que $B_1\subseteq B$ e seja $B_2=B\setminus B_1$. Então $\overline{B}_2=\{\overline{b}:b\in B_2\}$ é uma base de $V/W$ e, sendo $\overline{T}\in L(V/W)$ o operador induzido, temos:
\[
[T]_B=
\begin{pmatrix}
[T\upharpoonright_W]_{B_1}&*\\0&[\overline{T}]_{\overline{B_2}}
\end{pmatrix},
\]
\end{proposicao}
\begin{proof}
\textcolor{red}{COMPLETAR}
\end{proof}

\begin{lema}
Seja $\dim(V)=n$, $T\in\mathcal{L}(V)$ e $W\subseteq V$ um subsepaço $T$-invariante. Então:
\[
p_T(t)=p_{T_1}(t)\cdot p_{T_2}(t)
\]
em que $T_1\in\mathcal{L}(W)$ e $T_2\in\mathcal{L}(W)$ com $T_1(w)=T(w)$ e $T_2(v+W)=T(v)+W$.
\end{lema}
\begin{proof}
Escolhamos $B_1$ e $B$ como bases de $W$ e $V$ tais que $B_1\subseteq B$, então:
\begin{align*}
p_T(t)&=\det[tI-T]_B\\
&=\text{det}\begin{pmatrix}
tI_m-A & * \\ 0 & tI_{n-m}-B\end{pmatrix}\\
&=\text{det}(tI_m-A)\text{det}(tI_{n-m}-B)\\
&=p_A(t)p_B(t)=p_{T_1}(t)p_{T_2}(t)\qedhere
\end{align*}
\end{proof}

\begin{observacao}
O mesmo \textcolor{red}{não} ocorre para polinômios minimais. De fato, seja $T=I_V$ e seja $W$ um subespaço $T$-invariante (De fato, quando \(T\) é a identidade, todo subespaço de \(V\) é \(T\)-invariante), então $T_1=I_W$ e $T_2=I_{V/W}$ e aí $m_T(t)=m_{T_1}(t)=m_{T_2}(t)=t-1$.
\end{observacao}

\begin{teorema}[Teorema da Cayley-Hamilton]\label{cayley-hamilton}
Seja \(V\) um espaço vetorial de dimensão finita sobre um corpo \(K\), e seja \(T\in\mathcal{L}(V)\). Então \(p_T(T)=0\), onde \(p_T(t)\in K[t]\) é um polinômio característico de \(T\). 
\end{teorema}
\begin{proof}
Basta provar que \(\forall v\in V:p_T(T)(v)=0\). Seja $v\in V$. Consideremos:
\[
m_{T,v}(t)=t^m+\alpha_{m-1}t^{m-1}+\dots+\alpha_1 t+\alpha_0,
\]
o polinômio mônico de menor grau tal que \(m_{T,v}(T)(v)=0\). Então \(B_1=\{v,T(v),\ldots, T^{m-1}(v)\}\) é linearmente independente. Seja \(W\) o subespaço gerado por ele. Note que \(W\) é \(T\)-invariante e ainda:
\[[T\upharpoonright_W]_{B_1}=\begin{pmatrix}
0 & 0 & \ldots& 0 & -\alpha_0
\\ 1 & 0 & \ldots & 0 & -\alpha_1
\\ 0 & 1 & \ldots & 0 & -\alpha_2
\\ \ldots & \ldots  & \ldots  & \ldots  & \ldots \
\\ 0 & 0 & \ldots & 1 & -\alpha_{m-1} 
\end{pmatrix}\]
Então, pelo Exercício 18 da lista 1, segue que:
\[
p_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.\]
Aplicando essa função a \(v\) segue:
\[p_{T \upharpoonright_{W}}(T)(v)=m_{T,v}(T)(v)=0.\]
Para concluir que \(p_T(T)(v)=0\), notamos que \(p_{T\upharpoonright_W}(t)\mid p_T(t).\)
\end{proof}
\begin{corolario}
Se \(A\in M_n(K)\) então \(p_A(A)=0\),
onde \(p_A(t)=\text{det}(tI -A)\). \end{corolario}

\begin{exemplo}
Considere a matriz\[\begin{pmatrix}
a & b \\ c & d
\end{pmatrix}.\]
Então temos que \(p_A(t)=t^2-(a+d)t+(ad-bc)\) e também
\begin{align*}
P_A(A)&=A^2-(a+d)A+(ad-bc)I\\
&=
\begin{pmatrix} a^2+bc & ab+ad\\ac+dc & bc+d^2\end{pmatrix}
-\begin{pmatrix} a^2+ad & ab+bd \\ ac+dc & ad+d^2\end{pmatrix}
+ \text{det}(A)I
\\&=\begin{pmatrix}bc-ad & 0 \\ 0 & bc- ad\end{pmatrix}
+ \begin{pmatrix}ad-bc & 0 \\ 0 & ad - bc\end{pmatrix} = 0.
\end{align*}
\end{exemplo}

\begin{teorema}[Teorema da Cayley-Hamilton ao Avesso]\label{cayley-hamilton-avesso}
Se $V$ é espaço de dimensão finita e $T\in L(V)$, então todo polinômio irredutível que divide $p_T$ também divide $m_T$.
\end{teorema}
\begin{proof}
Faremos a demonstração por indução em $V$. Suponhamos o lema válido para $\dim(V)<n$. Seja $V$ espaço vetorial tal que $\dim(V)=n$ e seja $T\in L(V)$ e seja $p$ um polinômio irredutível que divide $p_T$. Se $V=0$, é fácil. Senão, então tome um $v\neq 0$ qualquer. Consideremos:
\[
m_{T,v}(t)=t^m+\alpha_{m-1}t^{m-1}+\dots+\alpha_1 t+\alpha_0,
\]
o polinômio mônico de menor grau tal que \(m_{T,v}(T)(v)=0\). Então \(B_1=\{v,T(v),\ldots, T^{m-1}(v)\}\) é linearmente independente. Seja \(W\) o subespaço gerado por ele. Note que \(W\) é \(T\)-invariante e ainda:
\[[T\upharpoonright_W]_{B_1}=\begin{pmatrix}
0 & 0 & \ldots& 0 & -\alpha_0
\\ 1 & 0 & \ldots & 0 & -\alpha_1
\\ 0 & 1 & \ldots & 0 & -\alpha_2
\\ \ldots & \ldots  & \ldots  & \ldots  & \ldots \
\\ 0 & 0 & \ldots & 1 & -\alpha_{m-1} 
\end{pmatrix}\]
Então, pelo Exercício 18 da lista 1, segue que:
\[
p_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.
\]
Além disso, vendo a definição de $m_{T,v}$, é fácil ver que:
\[
m_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.
\]
Seja $B$ uma base de $V$ tal que $B_1\subseteq B$ e seja $B_2=B\setminus B_1$. Então $\overline{B}_2=\{\overline{b}:b\in B_2\}$ é uma base de $V/W$ e, sendo $\overline{T}\in L(V/W)$ o operador induzido, temos:
\[
[T]_B=
\begin{pmatrix}
[T\upharpoonright_W]_{B_1}&*\\0&[\overline{T}]_{\overline{B_2}}
\end{pmatrix},
\]
aí $p_T=p_{T\upharpoonright_W}p_{\overline{T}}$ e $m_{T\upharpoonright_W}\mid m_T$ e $m_{\overline{T}}\mid m_T$. Assim, como $p\mid p_T$, então $p\mid p_{T\upharpoonright_W}$ ou $p\mid p_{\overline{T}}$.
\begin{itemize}
\item Se $p\mid p_{T\upharpoonright_W}$, como $p_{T\upharpoonright_W}=m_{T\upharpoonright_W}$, então $p\mid m_{T\upharpoonright_W}$, aí $p\mid m_T$.
\item Se $p\mid p_{\overline{T}}$, então, por hipótese de indução, temos $p\mid m_{\overline T}$, aí $p\mid m_T$. \qedhere
\end{itemize}
\end{proof}

\begin{teorema}[Decomposição Primária]
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que
\(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\). Suponhamos que
\(f(T)=0\), onde \[f(t)=p_1^{k_1}(t)\ldots p_r^{k_r}(t)\] e cada
\(p_i(t)\in
K[t]\) é irredutível. Então \(V=V_1\oplus\ldots\oplus V_r\) onde
cada \(V_i\) é \(T\)-invariante e \(p_i^{k_i}(T_{\upharpoonright V_i})=0\).
\end{teorema}
\begin{lema}
Seja \(f(T)=0\) onde \(f(t)=f_1(t)f_2(t)\) com
\(f_1\) e \(f_2\) primas entre si. Então \(V=V_1\oplus V_2\)
onde \(V_1\) e \(V_2\) são \(T\)-invariantes,
\(f_1(T_{\upharpoonright V_1})=0\) e \(f_2(T_{\upharpoonright V_2})=0\).
\end{lema}
\begin{sublema}[Identidade de Bézout]
Se \(\text{m.d.c}(f_1(t),f_2(t))=1\) então existem
\mbox{\(r(t),s(t)\in K[t]\)} tais que
\[f_1(t)r(t)+f_2(t)s(t)=1.\]
\end{sublema}
\begin{proof}[Demonstração do Lema]
Considere \(V_2\coloneqq\text{Im}(f_1(T))\)
e \(V_1\coloneqq\text{Im}(f_1(T))\).
Vamos verificar que que \(V_1\) e \(V_2\) são \(T\)-invariantes.
Seja \(v\in V_1\). Então \(v=f_2(T)(w)\) para algum \(w\in V\).
Dessa forma,
\begin{align*}
T(v) = T f_2(T)(w)=f_2(T)(T(w))=f_2(T)(T(w))\in\text{Im}(f_2(T))=V.
\end{align*}
e analogamente para \(V_2\). Além disso, mostremos que \(V=V_1+V_2\).
De fato, para cada \(v\in V\) temos \[v=f_1(T)r(T)(v)+f_2(T)s(T)(v).\]
Como \(f_1(T)r(T)(v)\in V_2\) e \(f_2(T)s(T)(v)\in V_1\)
concluímos que \(v\in V_1 + V_2\). Agora vamos mostrar que \(V_1\cap
V_2=\{0\}\). Para tal, vamos verificar que
\(V_1\subseteq\text{Ker}(f_1)(T)\)
e \(V_2\subseteq\text{Ker}(f_2(T)).\) Seja \(v\in V_1\). Então
\(v=f_2(T)(w)\) para algum \(w\in V\).
\[f_1(T)(v)=f_1(T)f_2(T)(w)=f(T)(w)=0.\]
Também vejamos que que
\(\text{Ker}(f_1)(T)\cap\text{Ker}(f_2)(T)=\{0\}\). Seja
\(v\in\text{Ker}(f_1(T))\cap\text{Ker}(f_2(T))\).
Então temos por definição que \(f_1(T)(v)=f_2(T)(v)=0\). Segue
\[v=r(T)f_1(T)(v)+s(T)f_2(T)(v)=0.\]
Assim concluímos que \(V=V_1\oplus
V_2\). Finalmente, note que como \(V_1\subseteq\text{Ker}(f_1(T))\) e
\(V_2\subseteq\text{Ker}(f_2(T))\) segue que  \(f_1(T\upharpoonright_{ V_1})=0\) e \(f_2(T\upharpoonright_{ V_2})=0\).
\end{proof}
\begin{proof}[Demonstração do Teorema]
Vamos mostrar este resultado por indução sobre \(r\). Note que o resultado é
óbvio para \(r=1\). Agora suponhamos que o resultado vale para
o caso \(r-1\). Então consideramos \(f(t)=f_1(t)f(t)\),
onde \(f_1(t)=p_1^{k_1}(t)\ldots p_{r-1}^{k_{r-1}}(t)\) e \(f_2(t)=p_r^{k_r}(t)\).
Então \(\text{m.d.c}\{f_1(t),f_2(t)\}=1\).
Aplicando o lema o resultado segue.
\end{proof}
\begin{corolario}
Seja \(V\) um espaço vetorial tal que \(\text{dim}(V)=n < \infty\),
seja \(T\in\mathcal{L}(V)\), e seja
\(m_T(t)=p_1^{k_1}(t)\ldots p_r^{k_r}(t) \) com \(p_i(t)\) irredutíveis e
primos entre si. Então \(V=V_1\oplus\ldots\oplus V_r\) onde \(V_i\)
são \(T\)-invariantes e \(m_{T_{\upharpoonright V_i}}(t)=p_i^{k_i}(t)\). 
\end{corolario}
\begin{proof}
Temos por definição que \(m_T(T)=0\). Portanto, pelo teorema temos
que \(V=V_1\oplus\ldots\oplus V_r\),
onde cada \(V_i\) é \(T\)-invariante.
Considere \(T_i\coloneqq T_{\upharpoonright V_i}\)
para cada \(i\in [r]\).
Temos que \(p_i^{k_i}(T_i)=0\). Então segue que \(m_{T_i}(t)\vert
p_i^{k_i}(t)\), ou seja,
\(m_{T_i}(t)=p_i(t)^{m_i}\), onde \(m_i\leq k_i\). Suponhamos que \(m_i <
k_i\) e consideremos \(g(t)=p_1^{k_1}(t)\ldots p_i^{m_i}(t)\ldots
p_r^{k_r}(t)\), e então \(\text{deg}(g(t))<\text{deg}(m_T(t))\). Provaremos
que \(g(T)=0\), o que irá contradizer a minimalidade do grau de \(m_t(t)\). Se
\(v\in V_j\) com \(j\not= i\) então \(p_j^{k_j}(v)=0\) e portanto
\(g(T)(v)=0\). Se \(v\in V_i\) então \(p_i^{m_i}(T)(v)=0\) e
\(g(T)(v)=0\). Assim concluímos que
\(g(T_k)=0\) para cada \(k=1,\ldots ,r\) e logo \(g(T)=0\).
Isso implica que \(g(T)\) é o polinômio minimal de \(T\), absurdo.
\end{proof}
\begin{corolario}
Seja \(V\) um espaço vetorial tal que \(\text{dim}(V)=n<\infty\), seja
\(T\in\mathcal{L}(V)\), e seja
\(p_T(t)=p_1^{k_1}(t)\ldots p_r^{k_r}(t)\) com
\(p_i(t)\) irredutíveis e primos entre si.
Então \(V=V_1\oplus\ldots\oplus
V_r\), com \(V_i\) \(T\)-invariantes e
\(p_{T_{\upharpoonright V_i}}(t)=p_i(t)^{k_i}\).
\end{corolario}
\begin{proof}
Pelo teorema temos
que \(V=V_1\oplus\ldots\oplus V_r\),
onde cada \(V_i\) é \(T\)-invariante.
Considere \(T_i\coloneqq T_{\upharpoonright V_i}\)
para cada \(i\in [r]\).
Temos que \(p_i^{k_i}(T_i)=0\). Então segue que \(m_{T_i}(t)\vert
p_i^{k_i}(t)\), mas, pelo teorema de Cayley-Hamilton \textcolor{red}{ao avesso} (Teorema \ref{cayley-hamilton-avesso}), todo fator irredutível de $p_{T_i}$ deve dividir $m_{T_i}$, logo ser igual a $p_i$. Assim $p_{T_i}=p_i^{l_i}$ para algum $l_i$. Entretanto, considerando bases $B_i$ de $V_i$, e juntando numa base $B$ de $V$, é fácil ver que $p_T=p_{T_1}\dots p_{T_k}$, assim $p_T=p_1^{l_1}\ldots p_r^{l_r}$, aí pela fatoração única devemos ter $l_i=k_i$ para todo $i$, concluindo a demonstração.
\end{proof}
\begin{corolario}
Um operador \(T\in\mathcal{L}(V)\) é diagonalizável se, e somente se
\(m_T=(t-\lambda_1)\ldots(t-\lambda_r)\) com
\(\lambda_i\not=\lambda_j\) sempre que \(i\not= j\).
\end{corolario}
\begin{proof}
A ida já foi provada em algum momento do passado, então
vamos mostrar apenas a volta. Pelo primeiro Corolário, temos que
\(V=V_1\oplus\ldots\oplus V_r\) com \(m_{T_{\upharpoonright
v_i}}=t-\lambda_i\), ou seja \(T_{\upharpoonright V_i}=\lambda_i
I_{V_i}\) e ainda \(V_i=V_T(\lambda_i)\). 
\end{proof}

\noindent
Considere \(\{T_i\,\colon i\in I\}\subseteq\mathcal{L}(V)\).
Quando os operadores \(T_i\) podem ser diagonalizados simultaneamente?

\begin{teorema}
Um conjunto \(\{T_i\,\colon i \in I\}\) pode ser diagonalizado
simultaneamente se, e somente se cada \(T_i\) é diagonalizável e \(T_iT_j=T_jT_i\) para todo \(i,j\in I\).\end{teorema}
\begin{proof}
Mostraremos por indução na dimensão. Suponhamos que o teorema é válido para espaços de dimensão menor que $n$. Seja $V$ um espaço tal que $\dim(V)=n$ e seja $\mathcal{F}$ um conjunto de operadores que comutam um com outro. Se todo elemento de $\mathcal{F}$ é múltiplo da identidade, então acaba. Caso contrário, existe um $T\in\mathcal{F}$ que não é múltiplo da identidade. Sejam $c_1,\dots,c_k$ os autovalores de $T$. Para $i$ seja $W_i=\Ker(T-c_iI)$. Então, como os elementos de $\mathcal{F}$ comutam um com outro, então $W_i$ é invariante para todo elemento de $\mathcal{F}$. Para cada $U\in\mathcal{F}$, então $m_U$ é produto de fatores lineares distintos, aí, como $m_{U\upharpoonright W_i}\mid m_U$, então $m_{U\upharpoonright W_i}$ é produto de fatores lineares distintos, aí $U\upharpoonright W_I$ é diagonalizável. Como $\dim(W_i)<n$, então existe uma base $B_i$ tal que para todo $U\in\mathcal{F}$ então $[U\upharpoonright W_i]$ seja diagonal. Então $B=B_1\cup\dots\cup B_k$ é a base que buscamos.
\end{proof}

\printindex

\end{document}