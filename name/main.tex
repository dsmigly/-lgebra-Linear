%%% ------- Readme -------%%%

%%-----------------------%%%
\documentclass[11pt,twoside,a4paper]{book}
\usepackage{estilos} 
\externaldocument{listas}
\makeindex
\title{Álgebra Linear \\  Douglas Smigly}
\author{MAT5730}
\date{$2^o$ semestre de 2019}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\chapter*{Informações da Disciplina}
\label{sec:intro}
\addcontentsline{toc}{section}{\nameref{sec:intro}}
\question{Informações Básicas}\label{new-question}

Essas são as notas de aula de Álgebra Linear(MAT5730), as aulas acontecem na sala B-134 às terças 10h e às quintas 8h.

\question{Informações do Professor}

O professor é o Ivan Shestakov, sua sala é a 290-A e o seu e-mail é shestak@ime.usp.br

\question{Bibliografia}
\nocite{*}
\bibliographystyle{plain}
\bibliography{samples}
\question{Avaliação}

A nota final da disciplina será a média aritimética de P1, P2, e P3. Todos os
alunos poderão fazer a prova sub para substituir a menor das suas notas (Sub
aberta). As datas das provas são as seguintes:

\begin{table}[h!]
\begin{center}

\label{tab:table1}
\begin{tabular}{l|r} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
\textbf{Prova} & \textbf{Data}\\
\hline
P1 & 10-09\\
P2 & 15-10\\
P3 & 12-11\\
SUB & 19-11
\end{tabular}
\end{center}
\end{table}

\question{Outras Informações}
\begin{enumerate}[label=(\roman*)]
\item Teremos listas, que não contarão para a nota
\item As listas serão publicadas em 
\item Não haverá monitoria
\end{enumerate}
\newpage 

\chapter{Espaços vetoriais}

\section{Definições Iniciais}

\begin{definicao}
Um \textbf{grupo abeliano} é um conjunto $X$ munido do seguinte:
\begin{itemize}
\item $+:X\times X\rightarrow X$,
\item $0\in X$,
\item $-:X\rightarrow X$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $x,y,z\in X$ então $(x+y)+z=x+(y+z)$,
\item Para $x,y\in X$ então $x+y=y+x$,
\item Para $x\in X$ então $x+0=x$,
\item Para $x\in X$ então $x+(-x)=0$.
\end{itemize}
\end{definicao}

\begin{definicao}
Um \textbf{corpo} é um grupo abeliano $(K,+,0,-)$ munido do seguinte:
\begin{itemize}
\item $\cdot:K\times K\rightarrow K$,
\item $1\in K$,
\item $\cdot^{-1}:K\setminus\{0\}\rightarrow K$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $x,y,z\in K$ então $(x\cdot y)\cdot z=x\cdot(y\cdot z)$,
\item Para $x,y\in K$ então $x\cdot y=y\cdot x$,
\item Para $x\in K$ então $x\cdot 1=x$,
\item Para $x\in K\setminus\{0\}$ então $x\cdot x^{-1}=1$,
\item Para $x,y,z\in K$ então $x\cdot(y+z)=(x\cdot y)+(x\cdot z)$.
\end{itemize}
\end{definicao}

\begin{definicao}
Dado $K$ um corpo, um \textbf{espaço vetorial sobre $K$} é um grupo abeliano $(V,+,0,-)$ munido do seguinte:
\begin{itemize}
\item $\cdot:K\times V\rightarrow V$,
\end{itemize}
satisfazendo as seguintes propriedades:
\begin{itemize}
\item Para $a,b\in K$ e $x\in V$ então $(a\cdot b)\cdot x=a\cdot(b\cdot x)$,
\item Para $x\in V$ então $x\cdot 1=x$,
\item Para $a\in K$ e $x,y\in V$ então $a\cdot(x+y)=(a\cdot x)+(a\cdot y)$,
\item Para $a,b\in K$ e $x\in V$ então $(a+b)\cdot x=(a\cdot x)+(b\cdot x)$.
\end{itemize}
\end{definicao}

\section{Base e Dimensão}

Durante o restante deste capítulo, sempre adotaremos $K$ como sendo um corpo qualquer.

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Seja $I$ um conjunto e $v:I\rightarrow V$ uma função. Uma \textbf{combinação linear} de $v$ é um elemento $u\in V$ tal que existam um conjunto finito $J\subseteq I$ e uma função $\alpha:J\rightarrow K$ tais que:
\[
u=\sum_{i\in J}\alpha_iv_i.
\]
Dizemos que $v$ \textbf{gera} $V$ se e só se todo elemento de $V$ é combinação linear de $v$.
\end{definicao}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Dizemos que um conjunto $S\subseteq V$ \textbf{gera} $V$ se e só se a função $v:S\rightarrow V$ dada por $\forall s\in S:v_s=s$ gera $V$.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e sejam $I$ um conjunto e $v:I\rightarrow V$ uma função. Então $v$ gera $V$ se e somente se a imagem $S=\{v_i:i\in I\}$ gera $V$.
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Se $v$ gera $V$, então para $x\in V$ existem um conjunto finito $J\subseteq I$ e uma função $\alpha:J\rightarrow K$ tais que:
\[
x=\sum_{i\in J}\alpha_iv_i,
\]
aí seja $T=v[J]$, e seja:
\[
\forall s\in T:J_s=\{i\in J:v_i=s\};
\]
e seja $\beta:T\rightarrow K$ a função dada por:
\[
\forall s\in T:\beta_s=\sum_{i\in J_s}\alpha_i,
\]
então $T$ é finito, aí temos:
\[
x=\sum_{i\in J}\alpha_iv_i=\sum_{s\in T}\sum_{i\in J_s}\alpha_iv_i=\sum_{s\in T}\sum_{i\in J_s}\alpha_is=\sum_{s\in T}\beta_ss;
\]
logo $S$ gera $V$.
\item Se $S$ gera $V$, então para $x\in V$ existem um conjunto finito $T\subseteq S$ e uma função $\beta:T\rightarrow K$ tais que:
\[
x=\sum_{s\in T}\beta_ss,
\]
aí existe uma função $i:T\rightarrow I$ tal que:
\[
\forall s\in T:v_{i_s}=s,
\]
aí seja $J=\mathrm{Im}(i)$, então $J$ é finito e $i$ é uma bijeção de $T$ a $J$ e sua inversa é $u=v\upharpoonright J$, aí seja $\alpha=\beta\circ u$, então:
\[
x=\sum_{s\in T}\beta_ss=\sum_{j\in J}\beta_{u_j}u_j=\sum_{j\in J}\alpha_ju_j=\sum_{j\in J}\alpha_jv_j;
\]
logo $v$ gera $V$.\qedhere
\end{itemize}
\end{proof}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $I$ um conjunto e seja $v:I\rightarrow V$ uma função. Dizemos que $v$ é \textbf{linearmente independente} se e só se para todo conjunto finito $J\subseteq I$ e toda função $\alpha:J\rightarrow K$, então temos a implicação:
\[
\sum_{i\in J}\alpha_iv_i=0\quad\Rightarrow\quad\forall i\in J:\alpha_i=0.
\]
Dizemos que $v$ é \textbf{linearmente dependente} se e só se $v$ não é linearmente independente.
\end{definicao}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $S\subseteq V$ um conjunto. Dizemos que $S$ é \textbf{linearmente independente} se e só se a função $v:S\rightarrow V$ dada por $\forall s\in S:v_s=s$ é linearmente independente. Dizemos que $S$ é \textbf{linearmente dependente} se e só se não é linearmente independente.
\end{definicao}

\begin{exemplo}
Se $V$ é um espaço vetorial sobre um corpo $K$ e $u\in V$ é um elemento não nulo, então a função $v:\{0,1\}\rightarrow V$ dada por $v_0=u$ e $v_1=u$ é linearmente \textcolor{red}{dependente}, mas o conjunto $\{v_0,v_1\}=\{u\}$ é linearmente \textcolor{red}{independente}.
\end{exemplo}

\begin{definicao}\label{lideford}\index{Espaço Vetorial!Base ordenada}
Seja $V$ um espaço vetorial sobre um corpo $K$ e seja $I$ um conjunto. Uma \textbf{base de $V$ ordenada por $I$} é uma função $b:I\rightarrow V$ tal que:
\begin{enumerate}[label=(\roman*)]
\item $b$ é linearmente independente.
\item $b$ gera $V$.
\end{enumerate}
\end{definicao}

\begin{definicao}\label{lidef}\index{Espaço Vetorial!Base}
Seja $V$ um espaço vetorial sobre um corpo $K$. Uma \textbf{base} de $V$ é um conjunto $B\subseteq V$ tal que:
\begin{enumerate}[label=(\roman*)]
\item $B$ é linearmente independente.
\item $B$ gera $V$.
\end{enumerate}
\end{definicao}

\begin{teorema}\label{existbase}
Seja $V$ um espaço vetorial e sejam $I\subseteq V$ linearmente independente e $S\subseteq V$ gerador de $V$ tais que $I\subseteq S$. Então existe uma base $B$ de $V$ tal que \[I\subseteq B\subseteq S.\]
\end{teorema}
\begin{proof}
Consideremos o conjunto:
\[
\mathcal{M}\coloneqq\{M\subseteq S\mid M\text{ é linearmente independente e }I\subseteq M\}
\]
Então $(\mathcal{M},\subseteq)$ é um conjunto parcialmente ordenado indutivo (ou seja, todo subconjunto totalmente ordenado possui uma cota superior). De fato, $I\in\mathcal{M}$, o que nos mostra que $\mathcal{M} \neq \emptyset,$ e para subconjunto totalmente ordenado não vazio $\mathcal{C}\subseteq\mathcal{M}$ então $\bigcup\mathcal{C}\in\mathcal{M}.$ 

\medskip
\noindent
Logo, pelo Lema de Zorn, $\mathcal{M}$ possui um elemento maximal $B$. Vamos provar que esse elemento maximal é de fato uma base para $V.$
\begin{enumerate}[label=(\roman*)]
\item $B$ é linearmente independente: segue da definição de $\mathcal{M}.$
\item $B$ gera $V$: Suponha por absurdo que $B$ não gera $V$. Então existe $v\in S$ que não é combinação linear de elementos de $B$, aí $B\cup\{v\}$ é linearmente independente e $I\subseteq B\cup\{v\}\subseteq S$. Então $B\cup\{v\}\in\mathcal{M}$, uma contradição, pois $B$ já é um elemento maximal de $\mathcal{M}$ e obviamente $B\subseteq B\cup\{v\}$. Logo $B$ gera $V$. Portanto, $B$ é uma base de $V$ e $I\subseteq B\subseteq S$.\qedhere
\end{enumerate}
\end{proof}

\noindent
O resultado acima mostra que todo espaço vetorial tem base, bastando para isso tomar $I=\emptyset$ e $S=V.$
\begin{corolario}
Seja $V$ um espaço vetorial sobre um corpo $K$,
seja $I\subseteq V$ um
conjunto linearmente independente e seja $S\subseteq V$
um conjunto que gere $V$. Então
\begin{enumerate}[label=(\roman*)]
\item O espaço $V$ tem uma base;
\item Existe uma base $B$ de $V$ tal que $I\subseteq B$; 
\item Existe uma base $B$ de $V$ tal que $B\subseteq S$.
\end{enumerate}
\end{corolario}

\begin{lema}\label{basefin1} Seja $V$ um espaço vetorial sobre um corpo $K$.
Sejam $(v_1,\dots,v_n)$ uma sequência linearmente independente e
$(u_1,\dots,u_m)$ uma sequência que gera $V$. Então $n\leq m$.
\end{lema}

\begin{sublema} Seja $V$ um espaço vetorial sobre um corpo $K$.
Uma sequência $(v_1,\dots,v_m)$ é linearmente dependente se e somente se existem $i$ e uma sequência $(\alpha_1,\dots,\alpha_{i-1})$ tais que \[v_i=\sum\limits_{j=1}^{i-1}\alpha_jv_j.\]
\end{sublema}
\begin{proof}[Demonstração do Sublema]
Se $(v_1,\dots,v_m)$ é linearmente dependente, então existe uma sequência $(\alpha_1,\dots,\alpha_m)$ não identicamente nula tal que:
\[
\sum_{i=1}^m\alpha_iv_i=0.
\]
Seja $i$ o maior índice tal que $\alpha_i\neq 0$.
Então segue que
\begin{align*}
&\alpha_1 v_1 + \ldots + \alpha_i v_i = 0 \\ \iff & \alpha_1v_1 + \ldots + \alpha_{i-1} v_{i-1} = - \alpha_i v_i \\ \iff &
v_i= - \sum\limits_{j=1}^{i-1}\frac{\alpha_j}{\alpha_i}v_j.\qedhere
\end{align*}
\end{proof}

\begin{proof}[Demonstração do Lema]
Primeiro, listamos os dois conjuntos de vetores: o conjunto gerador seguido do conjunto linearmente independente:
\[
u_1,\dots,u_m;v_1,\dots,v_n
\]
Então movemos o primeiro vetor $v_1$ para a esquerda da primeira lista:
\[
v_1,u_1,\dots,u_m;v_2,\dots,v_n
\]
Como $u_1,\dots,u_m$ gera $V$, $v_1$ é combinação linear dos $u_i$'s. Isso implica que
podemos remover um dos $s_i$'s, que indexando se necessário pode ser $u_1$,
da primeira lista, e ainda temos um conjunto gerador:
\[
v_1,u_2,\dots,u_m;v_2,\dots,v_n
\]
Note que o primeiro conjunto dos vetores ainda gera $V$ e o segundo conjunto ainda é linearmente
independente.

\medskip
\noindent
Agora repetimos o processo, movendo $v_2$ da segunda lista para a primeira lista:
\[
v_1,v_2,u_2,\dots,u_m;v_3,\dots,v_n
\]
Como antes, os vetores na primeira lista são linearmente dependentes, já que eles geravam
$V$ antes da inclusão de $v_2$. Entretanto, como os $v_i$'s são linearmente independentes,
qualquer combinação linear não trivial dos vetores na primeira lista que valha $0$
deve envolver pelo menos um dos $u_i$'s. Portanto, podemos remover este vetor, que
novamente reindexando se necessário pode ser $u_2$ e ainda temos um conjunto gerador:
\[
v_1,v_2,u_3,\dots,u_m;v_3,\dots,v_n
\]
Mais uma vez, o primeiro conjunto dos vetores gera $V$ e o segundo conjunto é linearmente
independente.

\medskip
\noindent
Agora, if $m<n$, então este processo eventualmente esgotará os $u_i$'s e nos levará à lista
\[
v_1,v_2,\dots,v_m;v_{m+1},\dots,v_n
\]
em que $v_1,v_2,\dots,v_m$ geram $V$, o que claramente não é possível pois $v_n$ não é combinação linear
dos $v_1,v_2,\dots,v_m$. Portanto $n\leq m$.
\end{proof}

\begin{observacao}\label{basefin2}
Com o lema, também podemos mostrar que, se existe um conjunto gerador finito, então podemos mostrar que todo conjunto linearmente independente é finito.

\medskip
\noindent
De fato, se existirem uma sequência geradora $(u_1,\dots,u_m)$ e um conjunto linearmente independente infinito $S$, então podemos pegar $m+1$ vetores distintos e assim formar uma sequência linearmente independente $(v_1,\dots,v_{m+1})$, contradizendo o lema \ref{basefin1}.
\end{observacao}

\noindent
Vamos relembrar o que fizemos até aqui com um exemplo:
\begin{exemplo}
Considere $V = \mathbb{R}^4$ um $\mathbb{R}$-espaço vetorial. Sejam os vetores:
\[
\begin{array}{rcl}
v_1 &=& (1,0,0,0) \\
v_2 &=& (0,1,0,-1) \\
v_3 &=& (0,0,1,-1) \\
v_4 &=& (1,-1,0,0) \\
v_5 &=& (1,2,1,0) 
\end{array}
\]
Considere $I = \{ v_1, v_2 \}$ e $S  =\{ v_1,v_2,v_3,v_4,v_5 \}.$ Observe que $I$ é LI; de fato,
\[
\alpha_1v_1 + \alpha_2v_2 = 0 \Rightarrow \alpha_1(1,0,0,0) + \alpha_2 (0,1,0,-1) = 0 \Rightarrow \left\{ \begin{array}{rcl} \alpha_1 &=& 0 \\ \alpha_2 &=& 0 \\ - \alpha_2 &=& 0 \end{array} \right. \Rightarrow \alpha_1 = \alpha_2 = 0
\]
Ademais, tomando $v = (x,y,z,w) \in \mathbb{R}^4,$ temos que
\[
(x-z+w+y)v_1 + (z- w - \varepsilon)v_2 + (z - \varepsilon)v_3 + (z-w-y + \varepsilon)v_4 + \varepsilon_5 = v,
\]
para todo $\varepsilon \in \mathbb{R}.$ Logo, $S$ gera $V.$ 

\medskip
\noindent
Então, existe uma base $B$ de $\mathbb{R}^4$ tal que 
\[
\{ v_1, v_2 \} \subseteq B \subseteq \{v_1,v_2,v_3,v_4,v_5 \}
\]
De fato, esta base é $B=\{v_1, v_2, v_3, v_4 \},$ pois percebe-se que
\[
v_5 = \frac{5}{2}v_1 + \frac{1}{2} v_2 - \frac{1}{2}v_3 - \frac{3}{2} v_4
\]
\end{exemplo}

\noindent
Para trabalhar com a cardinalidade das bases, utilizaremos alguns fatos
conhecidos, enunciados na próxima proposição:
\begin{proposicao}
Se $\lambda$ e $\mu$ são cardinais, então:
\begin{itemize}
\item Se $\lambda\leq\mu$ e $\mu\leq\lambda$, então $\lambda=\mu$. (Teorema de Cantor-Bernstein)\index{Teorema de Cantor-Bernstein}
\item Se $\lambda$ e $\mu$ são infinitos, então \[\lambda+\mu=\lambda\mu=\max\{\lambda,\mu\}.\]
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço vetorial, então duas bases quaisquer têm o mesmo cardinal.
\end{teorema}
\begin{proof}
Sejam $B$ e $C$ bases de $V$.
\begin{itemize}
\item Se $B$ ou $C$ são finitos, então pela observação \ref{basefin2} podemos inferir que $B$ e $C$ são ambos finitos e assim aplicar o lema \ref{basefin1}.
\item Se $B$ e $C$ são infinitos. Para $u\in C$ existem um conjunto finito $I_u\subseteq B$ e uma função $\alpha_u:I_u\rightarrow K$ tais que $u=\sum_{i\in I_u}(\alpha_u)_ii$. Seja $I\subseteq\bigcup_{u\in C}\subseteq B$. Então $I$ gera $V$, assim $I=C$. Desse modo:
\[
\abs{B}=\abs{I}=\abs{\bigcup_{u\in C}I_u}\leq\sum_{u\in C}\abs{I_u}\leq\aleph_0\cdot\abs{C}=\abs{C},
\]
assim $\abs{B}\leq\abs{C}$. Analogamente $\abs{C}\leq\abs{B}$. Portanto $\abs{B}=\abs{C}$.
\end{itemize}
\end{proof}
\begin{definicao}\index{Espaço Vetorial!Dimensão}
Dizemos que a \textbf{dimensão} de um espaço vetorial é a cardinalidade de sua base.
\end{definicao}

\section{Subespaços}

\begin{definicao}
Seja $V$ um espaço vetorial sobre um corpo $K$. Um \textbf{subespaço} de $V$ é um conjunto $W\subseteq V$ tal que:
\begin{itemize}
\item $0\in W$,
\item Para $x,y\in W$ então $x+y\in W$,
\item Para $a\in K$ e $x\in W$ então $ax\in W$.
\end{itemize}
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial e seja $\mathcal{W}$ um conjunto de subespaços. Então $\bigcap\mathcal{W}$ é um subespaço de $V$.
\end{proposicao}

\begin{definicao}
Se $S$ é subconjunto de $V$, definimos:
\[
\langle S\rangle=\left\{\sum\limits_{v\in I}\alpha_vv\mid I\subseteq S\text{ e }I\text{ é finito e }\alpha\in K^I\right\}
\]
e chamamos de \textbf{subespaço gerado} por $S$.
\end{definicao}

\begin{proposicao}
Se $S$ é subconjunto de $V$, então:
\[
\langle S\rangle=\bigcap\{W\mid W\text{ é subespaço de }V\text{ e }S\subseteq W\}.
\]
\end{proposicao}
\begin{proof}
Seja:
\[
T=\bigcap\{W\mid W\text{ é subespaço de }V\text{ e }S\subseteq W\}.
\]
Para $x\in \langle S\rangle$, então existem um conjunto finito $I\subseteq S$ e uma função $\alpha:I\rightarrow V$ tal que:
\[
x=\sum\limits_{v\in I}\alpha_vv,
\]
aí para todo subespaço $W$ tal que $S\subseteq W$, então para todo $v\in I$ temos $v\in S$, aí $v\in W$; aí por indução finita temos $x\in W$; logo $x\in T$. Portanto $\langle S\rangle\subseteq T$.

\medskip
\noindent
Além disso, temos o seguinte:
\begin{itemize}
\item $\emptyset\subseteq S$ e $\emptyset$ é finito e $\emptyset\in K^\emptyset$ e:
\[
0=\sum_{v\in\emptyset}\emptyset_vv,
\]
aí $0\in\langle S\rangle$.
\item Para $x,y\in\langle S\rangle$, então existem conjuntos finitos $I,J\subseteq S$ e funções $\alpha\in K^I$ e $\beta\in K^J$ tais que:
\[
x=\sum\limits_{u\in I}\alpha_uu,\quad y=\sum\limits_{v\in J}\beta_vv,
\]
aí sendo $L=I\cup J$ então $L\subseteq S$ e $L$ é finito, e também sendo $\tilde{\alpha},\tilde{\beta}:L\rightarrow K$ dadas por:
\[
\tilde{\alpha}_l=\left\{\begin{array}{cl}\alpha_l&\text{se }l\in I\\0&\text{se }l\notin I\end{array}\right.,\quad\tilde{\beta}_l=\left\{\begin{array}{cl}\beta_l&\text{se }l\in J\\0&\text{se }l\notin J\end{array}\right.,
\]
e sendo $\gamma:L\rightarrow K$ dada por $\gamma_l=\tilde{\alpha}_l+\tilde{\beta}_l$, então:
\[
x+y=\sum_{l\in L}\gamma_ll,
\]
aí $x+y\in\langle S\rangle$.
\item Para $a\in K$ e $x\in\langle S\rangle$, então existem conjunto finito $I\subseteq S$ e $\alpha\in K^I$ tais que:
\[
x=\sum_{v\in I}\alpha_vv,
\]
aí sendo $\beta:I\rightarrow K$ dada por $\beta_v=a\alpha_v$, então:
\[
ax=\sum_{v\in I}\beta_vv,
\]
aí $ax\in\langle S\rangle$.
\item Para $s\in S$, então $\{s\}\subseteq S$ e $\{s\}$ é finito, e considerando a função $\alpha:\{s\}\rightarrow K$ dada por $\alpha_s=1$, então:
\[
s=\sum_{v\in\{s\}}\alpha_vv,
\]
aí $S\subseteq\langle S\rangle$.
\end{itemize}
Logo $\langle S\rangle$ é um subespaço de $V$ tal que $S\subseteq\langle S\rangle$, aí $T\subseteq\langle S\rangle$.
\end{proof}

\noindent
A intersecção de subsespaços sempre é um subespaço, mas o mesmo não acontece com a união de subespaços.
\begin{proposicao}\label{unotsubsp}
Se $A$ e $B$ são subespaços de $V$ tais que $A\nsubseteq B$ e $B\nsubseteq A$, então $A\cup B$ não é subespaço de $V$.
\end{proposicao}
\begin{proof}
Nesse caso, existe $a\in A$ tal que $a\notin B$ e existe $b\in B$ tal que $b\notin A$. Seja $c=a+b$. Então:
\begin{itemize}
    \item Se $c \in A,$ $b = c - a \in A,$ o que é impossível.
    \item Se $c \in B,$ $a = c - b \in b,$ o que é impossível.
\end{itemize}
Logo, concluímos que $c \notin A \cup B,$ absurdo. \qedhere
\end{proof}

\noindent
Portanto concluímos que $A \cup B$ é um subespaço se e somente se $A \subseteq B$ ou $B \subseteq A.$

\begin{observacao}
Seja $K = F_2 = \{ 0, 1 \},$ e tome $V = K^2.$ Então,
\[
V = \langle (0,1) \rangle \cup \langle (1,0) \rangle \cup \langle (1,1) \rangle
\]
Na verdade, $V$ só pode ser escrito como união de um número finito de subespaços próprios se $K$ for um corpo finito, conforme a seguinte proposição.
\end{observacao}

\begin{proposicao}
Um espaço vetorial $V$ sobre um corpo infinito $K$ não pode ser escrito como união de um número finito de subespaços próprios.
\end{proposicao}
\begin{proof}
Suponhamos que $V=S_1\cup\dots\cup S_n$, em que podemos assumir que:
\[
S_1\nsubseteq S_2\cup\dots\cup S_n,
\]
Seja $w\in S_1 \setminus (S_2\cup\dots\cup S_n)$ e seja $v\notin S_1$. Considere o conjunto infinito:
\[
A=\{rw+v\mid r\in K\},
\]
que é a ``reta'' passando por $v$ e paralela a $w$. Queremos mostrar que cada $S_i$
contém no máximo um vetor do conjunto infinito $A$, o que será uma contradição ao fato de que
$V=S_1\cup\dots\cup S_n$. Isto provará o teorema.

\medskip
\noindent
Se $rw+v\in S_1$ para algum $r\neq 0$, então $w\in S_1$ implicará $v\in S_1$, contrário às hipóteses.
Agora, suponha que $r_1w+v\in S_1$ e $r_2w+v\in S_1$, para algum $i\geq 2$, em que $r_1\neq r_2$.
Então:
\[
(r_1-r_2)w=(r_1w+v)-(r_2w+v)\in S_i,
\]
aí $w\in S_i$, que também contradiz as hipóteses.
\end{proof}

\noindent
Apesar de não podermos trabalhar com a união, podemos realizar a soma de subespaços, e esta sim é um subespaço:

\begin{definicao}
Sejam $W_i \subseteq V$, $i \in I,$ subespaços de $V.$ Definimos:
\[
\sum\limits_{i \in I} W_i = \{ w_{i_1} + \ldots + w_{i_k} \mid k \in \mathbb{N}, w_i \in W_i \}.
\]
\end{definicao}

\noindent
Pode-se mostrar que o conjunto:
\[
\sum\limits_{i \in I} W_i
\]
é subespaço de $V$.

\begin{definicao}\index{Espaço Vetorial!Soma direta}
Uma soma:
\[
\sum\limits_{i \in I} W_i
\]
é dita uma \textbf{soma direta} se para todo $i \in I$ tivermos:
\[
W_i \cap \left( \sum\limits_{j \neq i} W_j \right) = 0.
\]
\end{definicao}

\begin{teorema}
Para subespaço $A$ de $V$, então existe subespaço $B\subseteq V$ tal que $V=A\oplus B$.
\end{teorema}
\begin{proof}
Seja $E$ uma base de $A$. Então existe uma base $G$ de $V$ tal que $E\subseteq G$, aí seja $F=G\setminus E$, e seja $B$ o subespaço gerado por $F$. Então é fácil ver que $V=A\oplus B$.
\end{proof}

\begin{teorema}
\[
\dim(A+B)+\dim(A\cap B)=\dim(A)+\dim(B).
\]
\end{teorema}
\begin{proof}
Seja $E$ base de $A\cap B$. Então existe $F$ tal que $B\cap F=\emptyset$ e $E\cup F$ seja base de $A$ e existe $G$ tal que $A\cap G=\emptyset$ e $E\cup G$ seja base de $B$. Então $E\cup F\cup G$ é base de $A+B$. Daí:
\[
\textcolor{Green}{\dim(A+B)} + \textcolor{Blue}{\dim(A \cap B)} = \textcolor{Green}{\abs{E} + \abs{F} + \abs{G}} + \textcolor{Blue}{\abs{E}} = \textcolor{Red}{\abs{E} + \abs{F}} + \textcolor{Laranja}{\abs{E} + \abs{G}} = \textcolor{Red}{\dim(A)} + \textcolor{Laranja}{\dim(B)}
\]
\end{proof}

\begin{exemplo}
Considere novamente $V = \mathbb{R}^4$. Sejam
\[
W_1 = \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \},
\]
\[
W_2 = \{(x,y,z,t) \in \mathbb{R}^4 | x +y = 0 \mbox{ e } z - 2t = 0 \}.
\]
Então $W_1$ e $W_2$ são subespaços de $V.$ Assim, $W_1 + W_2$ e $W_1 \cap W_2$ são subespaços de $V.$ Vamos encontrar bases para eles.

\medskip
\noindent
Note que:
\[
\begin{array}{lcl}
W_1 &=& \{(x,y,z,t) \in \mathbb{R}^4 | y + z + t = 0 \} \\
&=& \{(x,y,z,-y-z) \in \mathbb{R}^4 | x,y,z \in \mathbb{R} \} \\
&=& \{ (x,0,0,0) + (0,y,0-y) + (0,0,z,-z) : x,y,z \in \mathbb{R} \} \\
&=& \langle (1,0,0,0), (0,1,0-1), (0,0,1,-1) \rangle\\
\end{array}
\]
Verifica-se também que $(1,0,0,0), (0,1,0-1), (0,0,1,-1)$ são linearmente independentes. Logo, $B_1 = \{ (1,0,0,0), (0,1,0-1), (0,0,1,-1) \}$ é base para $W_1.$

\medskip
\noindent
Analogamente, mostra-se que $B_2 = \{ (1,-1,0,0), (0,0,2,1) \}$ é base para $W_2.$

\medskip
\noindent
Agora, para determinar uma base de $W_1 + W_2,$ podemos escalonar a matriz
\[
\left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & -1 \\
1 & -1 & 0 & 0\\
0 & 0 & 2 & 1

\end{array} \right) \rightarrow \cdots \rightarrow \left( \begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0& 0& 0 & 1\\
0 & 0 & 0 & 0

\end{array} \right)
\]
Portanto, o conjunto
\[
\mathcal{B} = \{ (1,0,0,0), (0,1,0,-1),(0,0,1,-1),(1,-1,0,0) \}\]
é base de $W_1 + W_2.$

\medskip
\noindent
Para determinar uma base de $W_1 \cap W_2,$ basta resolver o sistema
\[
\left\{ \begin{array}{l}
y+z+t = 0 \\x+y = 0 \\z - 2t = 0
\end{array} \right.
\]
Assim, $W_1 \cap W_2 = \langle (3,-3,2,1) \rangle.$

\medskip
\noindent
Observe que
\[
\dim(W_1 \cap W_2) + \dim(W_1 + W_2) = 1 + 4 = 5 = 3 + 2 = \dim(W_1) + \dim(W_2).
\]
Como $\dim(W_1 + W_2) = 4,$ temos que $W_1 + W_2 = V = \mathbb{R}^4.$

\medskip
\noindent
Observe também que, como $\dim(W_1 \cap W_2) = 1,$ a soma $W_1 + W_2$ não é direta. 
\end{exemplo}

\section{Coordenadas}

\begin{definicao}
Seja $V$ um espaço vetorial de dimensão finita. Seja $B$ uma base de $V$. Então para $v\in V$ existe um único $\alpha:B\rightarrow K$ tal que \[v=\sum\limits_{b\in B}\alpha_bb,\] e chamamos esse $\alpha$ de $[v]_B$.
\end{definicao}

\chapter{Transformações Lineares}
\index{Espaço Vetorial!Transformações Lineares}

\section{Definições}

\begin{definicao}
Sejam \(U\) e \(V\) espaços vetoriais sobre um corpo \(K\). Uma
\textbf{transformação linear} é uma função $T:U\rightarrow V$ tal que  $$T(\alpha
u+\beta v)=\alpha T(u)+\beta T(v)$$
 para quaisquer $\alpha,\beta\in K$ e $u,v\in V$. Além disso, denotamos o conjunto das transformações lineares de $U$ a $V$ por $\mathcal{L}(U,V).$
\end{definicao}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, seja $B$ uma base de $U$ e $f:B\rightarrow V$ uma função. Então existe uma única transformação linear $T\in\mathcal{L}(U,V)$ tal que $\forall b\in B:T(b)=f(b)$.
\end{teorema}

\begin{definicao}
Seja $T\in\mathcal{L}(U,V)$. Definimos $\mathrm{Ker}(T)=\{u\in U:T(u)=0\}$. Definimos $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$.
\end{definicao}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$. Então:
\begin{itemize}
\item $\mathrm{Ker}(T)$ é um subespaço de $U$.
\item $\mathrm{Im}(T)$ é um subespaço de $V$.
\item $T$ é injetora se e só se $\mathrm{Ker}(T)=0$.
\item Se $T$ é bijetora, então $T^{-1}\in\mathcal{L}(V,U)$.
\end{itemize}
\end{proposicao}

\begin{teorema}
Seja $\mathcal{L}(U,V)$, seja $B$ uma base de $\mathrm{Ker}(T)$, e seja $C$ um conjunto tal que $T[C]$ seja base de $\mathrm{Im}(T)$. Então $B\cup C$ é base $V$.
\end{teorema}
\begin{proof}
Para $v\in V$ então $T(v)\in\mathrm{Im}(T)$, então existem um conjunto finito $F\subseteq C$ e $\alpha:F\rightarrow K$ tais que:
\[
T(v)=\sum\limits_{w\in F}\alpha_wT(w),
\]
assim:
\[
T\left(v-\sum\limits_{w\in F}\alpha_ww\right)=0,
\]
aí:
\[
v-\sum\limits_{w\in F}\alpha_ww\in\mathrm{Ker}(T),
\]
assim existem um conjunto finito $E\subseteq B$ e função $\beta:B\rightarrow K$ tais que:
\[
v-\sum\limits_{w\in F}\alpha_ww=\sum\limits_{u\in E}\beta_uu,
\]
aí:
\[
v=\sum\limits_{u\in E}\beta_uu+\sum\limits_{w\in F}\alpha_ww.
\]
Por outro lado, para subconjunto finito $E\subseteq B\cup C$ e função $\alpha:E\rightarrow K$ tais que:
\[
\sum_{e\in E}\alpha_ee=0,
\]
então:
\[
\sum_{e\in E\cap C}\alpha_eT(e)=0,
\]
aí:
\[
\forall e\in E\cap C:\alpha_e=0,
\]
aí:
\[
\sum_{e\in E\setminus C}\alpha_ee=0,
\]
aí:
\[
\forall e\in E\setminus C:\alpha_e=0,
\]
portanto:
\[
\forall e\in E:\alpha_e=0.
\]
\end{proof}

\begin{teorema}[Teorema do Núcleo-Imagem] \index{Espaço Vetorial!Teorema do Núcleo-Imagem}
Seja $T \in \mathcal{L}(U,V).$ Então
\[
U = \mathrm{Ker}(T) \oplus \mathrm{Im}(T)
\]

\end{teorema}
\begin{corolario}
\[
\dim V=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T)).
\]
\end{corolario}

\begin{definicao}\index{Transformações Lineares!Isomorfismos}
Se $T\in\mathcal{L}(U,V)$ é bijetora, dizemos que $T$ é um \textbf{isomorfismo} de $U$ a $V$.
\end{definicao}
\begin{proposicao}
$T \in \mathcal{L}(U,V)$ é isomorfismo se e somente se $T^{-1}$ também o é.
\end{proposicao}

\begin{proposicao}
Dois espaços vetoriais $U$ e $V$ são isomorfos se e somente se quaisquer duas bases $B$ de $U$ e $C$ de $V$ possuem a mesma cardinalidade.
\end{proposicao}
\begin{teorema}
Para espaços vetoriais $U$ e $V$, então $U$ é isomorfo a $V$ se e só se $\dim(U)=\dim(V)$.
\end{teorema}

\section{Espaço Dual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. Denotamos $V^*=\mathcal{L}(V,K)$. O espaço $V^*$ chama-se o \textbf{espaço dual} de $V$. Os elementos de $V$ chamam-se \textbf{funcionais lineares}.
\end{definicao}

\noindent
Se $\dim(V)=n$, então $\dim(V^*)=n\cdot 1=n$, aí $V$ e $V^*$ são isomorfos.

\begin{teorema}\label{basedual}
Seja $V$ um espaço vetorial com $\dim(V)=n$ e $B=(v_1,\dots,v_n)$ uma base de $V$. Então existe uma base $B^*=(f_1,\dots,f_n)$ de $V^*$ tal que $f_i(v_j)=\delta_{i,j}$ para quaisquer $i,j$. Além disso:
\[
\forall v\in V:v=\sum_{i=1}^nf_i(v)v_i
\]
e:
\[
\forall f\in V^*:f=\sum_{i=1}^nf(v_i)f_i.
\]
\end{teorema}
\begin{proof}
Para $i=1,\dots,n$, existe uma única função linear $f_i:V\rightarrow K$ tal que:
\[
f_i(v_j)=\left\{\begin{array}{rl}0,&i\neq j\\1,&i=j\end{array}\right.
\]
Sejam $\alpha_1,\dots,\alpha_n\in K$ tais que:
\[
\sum_{i=1}^n\alpha_if_i=0.
\]
Para $j=1,\dots,n$, aplicando este funcional para o vetor $v_j\in B$, então:
\[
0=0(v_j)=\sum_{i=1}^n\alpha_if_i(v_j)=\alpha_j,
\]
ou seja, $\alpha_j=0$. Portanto $B^*$ é linearmente independente.

\medskip
\noindent
Além disso, para $v\in V$ existem $\alpha_1,\dots,\alpha_n\in K$ tais que:
\[
v=\sum_{i=1}^n\alpha_iv_i,
\]
aí para $i=1,\dots,n$ temos:
\[
f_i(v)=\alpha_if_i(v_i)=\alpha_i;
\]
logo:
\[
f(v)=\sum_{i=1}^n\alpha_if(v_i)=\sum_{i=1}^nf(v_i)f_i(v). \qedhere
\] 
\end{proof}

\begin{definicao}\index{Espaço Vetorial!Base dual}
A base $B^*$ chama-se a \textbf{base dual} da base $B$.
\end{definicao}

\noindent
Podemos estender o estudo do espaço dual para espaços vetoriais quaisquer.

\begin{definicao}
Seja $B$ uma base de $V$, então para cada $a\in B$ definimos a transformação linear $f_a\in V^*$ por $f_a(b)=\delta_{a,b}$.
\end{definicao}

\noindent
Nesse caso, podemos adaptar facilmente o argumento na demonstração do teorema \ref{basedual} para mostrar que $(f_a)_{a\in B}$ é linearmente independente em $V^*$ e para todo $v\in V$ existe um conjunto finito $F\subseteq B$ tal que:
\[
v=\sum_{b\in F}f_b(v)b.
\]

\section{Espaço Bidual}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $K$. O espaço $V^{**}=(V^*)^*$ chama-se o \textbf{espaço bidual} do espaço $V$.
\end{definicao}

\begin{definicao}
Para $v\in V$, definamos $\varphi_v:V^*\rightarrow K$ assim:
\[
\forall f\in V^*:\varphi_v(f)=f(v).
\]
Então $\varphi_v\in V^{**}$.
\end{definicao}

\begin{proposicao}
$\varphi\in\mathcal{L}(V,V^{**})$ e $\varphi$ é injetora.
\end{proposicao}
\begin{proof}
Seja $B$ uma base de $V$. Para $v\in\mathrm{Ker}(\varphi)$, então $\varphi_v=0$, aí temos $\forall b\in B:f_b(v)=\varphi_v(f_b)=0$, aí existe um conjunto finito $F\subseteq B$ tal que:
\[
v=\sum_{b\in F}f_b(v)b,
\]
aí $v=0$.
\end{proof}

\begin{corolario}
Se $\dim(V)$ é finita, então $\varphi:V\rightarrow V^{**}$ é um isomorfismo.
\end{corolario}
\begin{proof}
\[
\dim(V)=\dim(V^*)=\dim(V^{**}). \qedhere
\]
\end{proof}

\begin{observacao}
Nesse caso $\varphi$ é um isomorfismo natural, ou seja, não depende da escolha de uma base.
\end{observacao}

\begin{corolario}
Se $\dim(V)$ é finita, então toda base de $V^*$ é a base dual para uma base de $V$.
\end{corolario}
\begin{proof}
Seja $C=(f_1,\dots,f_n)$ uma base de $V^*$. Consideremos a base dual $C^*=(g_1,\dots,g_n)$ de $V^{**}$. Mas $\varphi$ é sobrejetora, então existem $v_1,\dots,v_n\in V$ tais que para todo $i$ tenhamos $g_i=\varphi_{v_i}$, assim:
\[
f_i(v_j)=\varphi_{v_j}(f_i)=g_j(f_i)=\delta_{j,i}=\delta_{i,j},
\]
logo $C=(f_1,\dots,f_n)$ é base dual da base $(v_1,\dots,v_n)$ de $V$.
\end{proof}

\section{Anuladores}

\begin{definicao}
Seja $V$ um espaço vetorial e seja $S\subseteq V$ um subconjunto. Então definimos:
\[
S^0=\{f\in V^*\mid\forall s\in S:f(s)=0\}.
\]
O conjunto $S^0$ chama-se o \textbf{anulador} de $S$.
\end{definicao}

\begin{proposicao}
$S^0$ é um subespaço de $V$.
\end{proposicao}

\begin{teorema}
Seja $V$ um espaço de dimensão finita e $W\subseteq V$ um subespaço. Então:
\[
\dim(V)=\dim(W)+\dim(V^0).
\]
\end{teorema}
\begin{proof}
Seja $\dim(V)=n$ e $\dim(W)=m$. Escolhemos uma base $(v_1,\dots,v_m)$ de $W$ e completemo-la até uma base $(v_1,\dots,v_m,v_{m+1},\dots v_n)$ de $V$. Consideremos a base dual $(f_1,\dots,f_n)$ de $V^*$. Mostraremos que $(f_{m+1},\dots,f_n)$ é uma base de $W^0$. É claro que para todo $i=m+1,\dots,n$ temos $f_i\in W^0$. Seja $f\in W^0$, então:
\[
f=\sum_{i=1}^nf(v_i)f_i=\sum_{i=m+1}^nf(v_i)f_i. \qedhere
\]
\end{proof}

\begin{teorema}
Se $\dim(V)$ é finita e $V=U\oplus W$, então $V^*=U^0\oplus W^0$ e $U^0\cong W^*$ e $W_0\cong U^*$.
\end{teorema}
\begin{proof}
Seja $B=B_U\cup B_W$ base de $V$, em que $B_U$ é base de $U$ e $B_W$ é base de $W$. Então a base dual é $B^*=B_U^*\cup B_V^*$, e pelo teorema anterior temos $\langle B_U^*\rangle=W^0$ e $\langle B_V^*\rangle=U^0$.
\end{proof}

\section{Transpostas}

\begin{definicao}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e $T\in\mathcal{L}(U,V)$. Então definimos a \textbf{transposta} de $T$ como a função:
\[
\begin{array}{rcl}
T^t:V^t&\rightarrow&U^t\\f&\mapsto&T^t(f)=f\circ T
\end{array}
\]
\end{definicao}

\begin{proposicao}
Se $\dim(U)$ é finita e $T\in\mathcal{L}(U,V)$, então:
\begin{itemize}
\item[a)] $\mathrm{Ker}(T^t)=(\mathrm{Im}(T))^0$.
\item[b)] $\mathrm{Rank}(T^t)=\mathrm{Rank}(T)$.
\item[c)] $\mathrm{Im}(T^t)=(\mathrm{Ker}(T))^0$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[a)] Temos:
\[
\begin{array}{rcl}
\mathrm{Ker}(T^t)&=&\{f\in V^*\mid T^t(f)=0\}\\&=&\{f\in V^*\mid f\circ T=0\}\\&=&\{f\in V^*\mid \forall u\in U:f(T(u))=0\}\\&=&\{f\in V^*\mid f[\mathrm{Im}(T)]=0\}\\&=&(\mathrm{Im}(T))^0.
\end{array}
\]
\item[b)] Temos $\mathrm{Rank}(T^t)=\dim(\mathrm{Im}(T^t))$ e $\mathrm{Rank}(T)=\dim(\mathrm{Im}(T))$. Além disso:
\[
\dim(V^*)=\dim(\mathrm{Im}(T^t))+\dim(\mathrm{Ker}(T^t))
\]
\[
\dim(V^*)=\dim(\mathrm{Im}(T))+\dim(\mathrm{Im}(T))^0
\]
mas $\dim(V^*)=\dim(V)$ e $\dim(\mathrm{Ker}(T^t))+\dim(\mathrm{Im}(T))^0$.
\item[c)] Temos $\mathrm{Im}(T^t)\subseteq(\mathrm{Ker}(T))^0$. Seja $\varphi\in\mathrm{Im}(T^t)$, então existe $g\in V^*$ tal que $\varphi=T^t(g)$, aí para todo $u\in U$ nós temos $\varphi(u)=T^t(g)(u)=g(T(u))$. Se $u\in\mathrm{Ker}(T)$ então $T(u)=0$, aí $\varphi(u)=0$; logo $\varphi\in(\mathrm{Ker}(T))^0$. Além disso:
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Ker}(T))^0
\]
\[
\dim(U)=\dim(\mathrm{Ker}(T))+\dim(\mathrm{Im}(T))
\]
aí $\dim(\mathrm{Ker}(T))^0=\dim(\mathrm{Im}(T))$, aí $(\mathrm{Ker}(T))^0=\mathrm{Im}(T)$. \qedhere
\end{itemize}
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais de dimensão finita com bases $B$ e $C$ e bases duais $B^*$ e $C^*$. Se $T\in\mathcal{L}(U,V)$, então:
\[
[T]_{B,C}^t=[T^t]_{C^*,B^*}
\]
\end{teorema}

\begin{corolario}
Se $A\in M_{m,n}(K)$, então:
\[
\mathrm{Row Rank}(A)=\mathrm{Column Rank}(A).
\]
\end{corolario}
\begin{proof}
Consideremos $T:K^n\rightarrow K^m$ dada por $T(v)=Av$. Sejam $B$ e $C$ as bases canônicas de $K^n$ e $K^m$, então $[T]_{B,C}=A$. Temos:
\[
\begin{array}{ccccc}
\mathrm{Rank}(T)&=&\mathrm{Column Rank}(A)&&\\
\mathrm{Rank}(T^t)&=&\mathrm{Column Rank}(A^t)&=&\mathrm{Row Rank}(A).
\end{array} \qedhere
\]
\end{proof}

\section{Espaços Quocientes}

\begin{definicao}
Seja $V$ um espaço, $W\subseteq V$ um subespaço. Para $u,v\in V$, digamos que $u\sim v$ se e só se $u-v\in W$. Então $\sim$ é uma relação de equivalência, ou seja:
\begin{itemize}
\item Reflexiva, ou seja, $v\sim v$ sempre.
\item Simétrica, ou seja, se $v\sim u$ então $u\sim v$.
\item Transitiva, ou seja, se $v\sim u$ e $u\sim w$, então $v\sim w$.
\end{itemize}
Seja $V/W$ o conjunto das classes de equivalência relativamente a $\sim$. Para $v\in V$ seja $\overline{v}$ a classe de equivalência de $v$.
\begin{itemize}
\item Definamos em $V/W$ uma estrutura de espaço vetorial. Para $\overline{v},\overline{w}\in V/W$ definamos $\overline{v}+\overline{w}=\overline{v+w}$.
\item Para $\alpha\in K$ e $\overline{v}\in V$ definamos $\alpha\cdot\overline{v}=\overline{\alpha v}$. Então $V/W$ é um espaço vetorial chamado \textbf{espaço quociente}.
\end{itemize}
\end{definicao}

\begin{observacao}
As operações estão ``bem definidas'' pois:
\begin{itemize}
\item Se $\overline{v}=\overline{v'}$ e $\overline{u}=\overline{u'}$, então $v\sim v'$ e $u\sim u'$, aí $v-v',u-u'\in W$, aí $(v+u)-(v'+u')=(v-v')+(u-u')\in W$, aí $\overline{v+u}=\overline{v'+u'}$, aí $\overline{v}+\overline{u}=\overline{v'}+\overline{u'}$.
\item Analogamente para a outra propriedade.
\end{itemize}
Também verificaremos algumas propriedades, deixando o resto ao leitor.
\begin{itemize}
\item Temos a comutatividade da adição, pois $\overline{u}+\overline{v}=\overline{v}+\overline{u}$ equivale a $\overline{u+v}=\overline{v+u}$, que é verdade pois $u+v=v+u$.
\item O que é o $\overline{0}$ de $V/W$? Temos $\overline{0}=W$, e também para todo $w\in W$ temos $w\sim 0$, aí $\overline{w}=\overline{0}=W$.
\end{itemize}
Também temos o seguinte:
\begin{itemize}
\item Se $W=V$, então $V/V=\{\overline{0}\}$.
\item Se $W=\{0\}$, então $V/\{0\}\cong V$.
\end{itemize}
\end{observacao}

\begin{proposicao}
Consideremos a aplicação:
\[
\pi:V\rightarrow V/W,\quad\quad v\mapsto\overline{v}.
\]
Então $\pi\in\mathcal{L}(V,V/W)$, com $\mathrm{Ker}(\pi)=W$.
\end{proposicao}

\begin{notacao}
$\pi$ chama-se a \textbf{projeção canônica} de $V$ para $V/W$.
\end{notacao}

\begin{proof}
Temos o seguinte:
\begin{itemize}
\item $\pi(v+u)=\overline{v+u}=\overline{v}+\overline{u}=\pi(v)+\pi(u)$.
\item $\pi(\alpha v)=\overline{\alpha v}=\alpha\overline{v}=\alpha\pi(v)$.
\end{itemize}
Além disso, se $w\in W$ então $\pi(w)=\overline{w}=W$
\end{proof}

\begin{proposicao}
Seja $T\in\mathcal{L}(U,V)$ e $W\subseteq U$ tal que $W\subseteq\mathrm{Ker}(T)$. Então existe um único $\overline{T}\in\mathcal{L}(U/W,V)$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
\end{proposicao}
\begin{proof}
Temos o seguinte:

\medskip
\noindent
1) Mostraremos que $\overline{T}$ está ``bem definida''. Se $\overline{u}=\overline{v}$, então $u-v\in W\subseteq\mathrm{Ker}(T)$, aí $T(u-v)=0$, aí $T(u)=T(v)$.

\medskip
\noindent
2) Mostraremos que $\overline{T}$ é uma transformação linear.

\begin{itemize}
\item $\overline{T}(\overline{u}+\overline{v})=\overline{T}(\overline{u+v})=T(u+v)=T(u)+T(v)=\overline{T}(\overline{u})+\overline{T}(\overline{v})$.
\item $\overline{T}(\alpha\overline{v})=\overline{T}(\overline{\alpha v})=T(\alpha v)=\alpha T(v)=\alpha\overline{T}(\overline{v})$.
\end{itemize}
Agora, para todo $T'\in\mathcal{L}(U/W,V)$ tal que para todo $u\in U$ tenhamos:
\[
T'(\overline{u})=T(u),
\]
então para todo $v\in U/W$ existe um $u\in U$ tal que $v=\overline{u}$, aí:
\[
T'(v)=T'(\overline{u})=T(u)=\overline{T}(\overline{u})=\overline{T}(v);
\]
logo $T'=\overline{T}$.
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços vetoriais sobre $K$, e seja $T\in\mathcal{L}(U,V)$. Então $U/\mathrm{Ker}(T)\cong\mathrm{Im}(T)$.
\end{teorema}
\begin{proof}
Pela proposição anterior, existe uma única $\overline{T}:U/\mathrm{Ker}(T)\rightarrow V$ tal que para todo $u\in U$ tenhamos:
\[
\overline{T}(\overline{u})=T(u).
\]
Observemos que $\mathrm{Im}(\overline{T})=\mathrm{Im}(T)=\{T(u)\mid u\in U\}$.

\medskip
\noindent
Além disso, para $\overline{u}\in\mathrm{Ker}(\overline{T})$, então $T(u)=\overline{T}(\overline{u})=0$, aí $u\in\mathrm{Ker}(T)$, aí $\overline{u}=\overline{0}$, de modo que $\overline{T}$ é injetora.
\end{proof}

\begin{teorema}
Seja $W$ subespaço de $V$. Então todos os complementos de $W$ em $V$ são isomorfos ao $V/W$.
\end{teorema}
\begin{proof}
Seja $V=W\oplus U$. Consideremos a projeção canônica:
\[
\pi:V\rightarrow V/W.
\]
Seja $\overline{\pi}=\pi\upharpoonright U$. Então $\mathrm{Ker}(\overline{\pi})=U\cap\mathrm{Ker}(\pi)=U\cap W=\{0\}$. Logo $\overline{\pi}$ é injetora.

\medskip
\noindent
Para $\overline{v}\in V/W$, seja $v=w+u$, com $w\in W$ e $u\in U$. Então $\pi(v)=\pi(w)+\pi(u)=\pi(u)=\overline{\pi}(u)$, aí $\overline{v}=\overline{\pi}(u)$, assim $\overline{\pi}$ é sobre $V/W$.
\end{proof}

\begin{corolario}
Seja $W\subseteq V$ um subespaço. Então $\dim V=\dim W+\dim V/W$.
\end{corolario}
\begin{proof}
Seja $V=W\oplus U$, então $\dim V=\dim W+\dim U$, mas $U\cong V/W$, aí $\dim U=\dim V/W$.
\end{proof}

\begin{observacao}
Existem espaços vetoriais $W$ e $U$ e $W'$ e $U'$ tais que $W\oplus U\cong W'\oplus U'$ e $W\cong W'$, mas $U\ncong U'$. De fato podemos tomar:
\[
W=\bigoplus_{i=0}^\infty Ke_{2i},\quad U=\bigoplus_{i=0}^\infty Ke_{2i+1},\quad W'=\bigoplus_{i=0}^\infty Ke_i,\quad U'=\{0\}.
\]
\end{observacao}

\chapter{Determinantes}

\section{Formas Multilineares}

\begin{definicao}
Seja $V$ um espaço vetorial e $V^r=V\times\dots\times V$. Uma \textbf{forma $r$-linear} sobre $V$ é uma função $F:V^r\rightarrow K$ que é linear em cada argumento, ou seja, para cada $i=1,\dots,r$ temos:
\[
F(v_1,\dots,\alpha v_i+\beta v'_i,\dots,v_r)=\alpha F(v_1,\dots,v_i,\dots,v_r)+\beta F(v_1,\dots,v'_i,\dots,v_r).
\]
Denotamos por $L_r(V)$ o conjunto das formas $r$-lineares sobre $V$.
\end{definicao}

\begin{exemplo}
Seja $V=K^2$ e:
\[
F((x_1,y_1),(x_2,y_2),(x_3,y_3))=x_1y_2x_3-x_1x_2x_3.
\]
Então $F$ é uma forma 3-linear.
\end{exemplo}

\begin{definicao}
Uma forma $F\in L_r(V)$ chama-se \textbf{alternada} se e só se para $(v_1,\dots,v_r)\in V^r$ e $i<j$ tais que $v_i=v_j$ então $F(v_1,\dots,v_r)=0$. Denotamos por $A_r(V)$ o conjunto das formas $r$-lineares alternadas.
\end{definicao}

\begin{definicao}
Uma forma $F$ é chamada \textbf{antissimétrica} se para $v\in V^r$ e para $i<j$ temos:
\[
F(v_1,\dots,v_i,\dots,v_j,\dots,v_r)=-F(v_1,\dots,v_j,\dots,v_i,\dots,v_r).
\]
\end{definicao}

\begin{proposicao}
Toda forma alternada é antissimétrica.
\end{proposicao}
\begin{proof}
Seja $F\in A_r(V)$. Sejam $v\in V^r$ e $i<j$, então:
\[
\begin{array}{rcl}
0&=&F(v_1,\dots,v_i+v_j,\dots,v_i+v_j,\dots,v_r)\\&=&F(v_1,\dots,v_i,\dots,v_i,\dots,v_r)+F(v_1,\dots,v_i,\dots,v_j,\dots,v_r)\\&+&F(v_1,\dots,v_j,\dots,v_i,\dots,v_r)+F(v_1,\dots,v_j,\dots,v_j,\dots,v_r)\\&=&F(v_1,\dots,v_i,\dots,v_j,\dots,v_r)+F(v_1,\dots,v_j,\dots,v_i,\dots,v_r)
\end{array}
\]
\end{proof}

\begin{proposicao}
Se a característica do corpo é $\neq 2$, então toda forma antissimétrica é reflexiva.
\end{proposicao}
\begin{proof}
Para $F$ antissimétrica e $v\in V^r$ e $i<j$, se $v_i=v_j$, sendo $v=v_i$, então:
\[
F(v_1,\dots,v,\dots,v,\dots,v_1)=-F(v_1,\dots,v,\dots,v,\dots,v_r),
\]
aí:
\[
2F(v_1,\dots,v,\dots,v,\dots,v_r)=0,
\]
aí:
\[
F(v_1,\dots,v,\dots,v,\dots,v_r)=0.
\]
\end{proof}

\begin{definicao}
Seja $F\in L_r(V)$ e $\sigma\in S_r$ uma permutação. Para $(v_1,\dots,v_r)\in V^r$ definimos:
\[
(\sigma F)\left(v_1,\dots,v_r\right)=F\left(v_{\sigma(1)},\dots,v_{\sigma(r)}\right).
\]
É fácil ver que $\sigma F\in L_r(V)$.
\end{definicao}

\begin{observacao}
Para $F\in L_r(V)$, então $F$ é antissimétrica se e somente se para toda transposição $\tau\in S_r$ tivermos $\tau F=-F$.
\end{observacao}

\begin{proposicao}
Seja $F\in L_r(V)$ uma forma antissimétrica. Então para $\sigma\in S_r$, temos:
\[
\sigma F=(\sgn \ \sigma)F.
\]
\end{proposicao}
\begin{proof}
Para $\sigma\in S_r$, então $\sigma$ pode ser escrita como um produto de transposições:
\[
\sigma=\tau_1\dots\tau_k,
\]
aí $\sigma$ é par se e só se $k$ é par. Portanto:
\[
\sigma F=(\tau_1\dots\tau_k)F=(-1)^k F=(\sgn \ \sigma)F,
\]
pois $\sgn \ \sigma=(-1)^k$.
\end{proof}

\begin{proposicao}
Toda forma $r$-linear determina uma forma $r$-linear alternada da seguinte maneira:
\[
F\mapsto\varphi(F)=\sum_{\sigma\in S_r}(\sgn \ \sigma)(\sigma F).
\]
\end{proposicao}
\begin{proof}
Seja $v_i=v_j=v$ com $i<j$. Precisamos provar que $\varphi(F)(v)=0$. Seja $\tau$ a transposição $(i,j)$, então $ S_r=A_r\cup A_r\tau$ e $A_r\cap A_r\tau=\emptyset$. Então temos o seguinte:
\[
\begin{array}{rcl}
\varphi(F)(v)&=&\sum_{\sigma\in S_r}(\sgn \ \sigma)(\sigma F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma\tau F(v))\\&=&\sum_{\sigma\in A_r}(\sigma F(v))-\sum_{\sigma\in A_r}(\sigma F(v))\\&=&0.
\end{array}
\]
\end{proof}

\begin{observacao}
Se $F\in A_r(V)$ e $v\in V^r$ é linearmente dependente, então:
\[
F(v)=0.
\]
\end{observacao}

\begin{lema}
Seja $\dim V=n$ e $F\in A_n(V)$. Seja $(e_1,\dots,e_n)$ uma base de $V$, então $F$ é completamente determinada pelo valor $F(e)$.
\end{lema}
\begin{proof}
Seja $(v_1,\dots,v_n)\in V^n$. Então existe $(\alpha_{i,j})\in M_n(K)$ tal que:
\[
v_i=\sum_{j=1}^n\alpha_{i,j}e_j.
\]
Assim:
\[
\begin{array}{rcl}
F(v_1,\dots,v_n)&=&F\left(\sum\limits_{j_1=1}^n\alpha_{1,j_1}e_{j_1},\dots,\sum\limits_{j_n=1}^n\alpha_{n,j_n}e_{j_n}\right)\\&=&\sum\limits_{j_1,\dots,j_n=1}^n\alpha_{1,j_1}\dots\alpha_{n,j_n}F\left(e_{j_1},\dots,e_{j_n}\right)\\&=&\sum\limits_{\sigma\in S_n}\alpha_{1,\sigma_1}\dots\alpha_{n,\sigma_n}F\left(e_{\sigma_1},\dots,e_{\sigma_n}\right)\\&=&\textcolor{red}{\left(\sum\limits_{\sigma\in S_n}\alpha_{1,\sigma_1}\dots\alpha_{n,\sigma_n} \sgn \ \sigma \right)} F(e_1,\dots,e_n).
\end{array}
\]
\end{proof}

\noindent
Note então que o valor
\[
\sum\limits_{\sigma\in S_n}\alpha_{1,\sigma_1}\dots\alpha_{n,\sigma_n} \sgn \ \sigma 
\]
\emph{determina} $F$ para qualquer $v \in V^n.$ Chamaremos este valor de \textbf{determinante} de $F.$

\begin{exemplo}

\end{exemplo}

\section{Determinantes}

Seja $K$ um corpo e consideremos o anel das matrizes $M_n(K)$. Identificaremos os elementos de $M_n(K)$ com os elementos de $(K^n)^n$ assim:
\[
\begin{pmatrix}
a_{1,1}&&a_{1,n}\\&\ddots&\\a_{n,1}&&a_{n,n}
\end{pmatrix} \longleftrightarrow
\left((a_{1,1},\dots,a_{1,n}),\dots,(a_{n,1},\dots,a_{n,n})\right).
\]
Portanto, uma função $n$-linear aqui é uma função $n$-linear nas linhas da matriz.

\begin{definicao}
Uma função $\det:M_n(K)\rightarrow K$ é dita uma função \textbf{determinante} se e só se $\det$ é $n$-linear alternada e $\det(I)=1$.
\end{definicao}

\noindent
Pelo que vimos, existe e é única a função determinante: É a forma $n$-linear alternada que vale $1$ na base canônica de $K^n$.

\medskip
\noindent
Logo, se $A=(a_{i,j})\in M_n(K)$, então:
\[
\det(A)=\sum_{\sigma\in S_n}\sgn(\sigma)a_{1,\sigma_1}\dots a_{n,\sigma_n}.
\]

\begin{exemplo}
Para $n=2$, temos $S_2=\{I,(1,2)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{1,1}&a_{1,2}\\a_{2,1}&a_{2,2}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{1,1}a_{2,2}-a_{1,2}a_{2,1}.
\]
\end{exemplo}

\begin{exemplo}
Agora, se $n=3$, então $S_3=\{I,(1,2,3),(1,3,2),(1,2),(1,3),(2,3)\}$, e assim, sendo:
\[
A=\begin{pmatrix}
a_{1,1}&a_{1,2}&a_{1,3}\\a_{2,1}&a_{2,2}&a_{2,3}\\a_{3,1}&a_{3,2}&a_{3,3}
\end{pmatrix},
\]
então temos:
\[
\det(A)=a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3}a_{2,2}a_{3,1}-a_{1,1}a_{2,3}a_{3,2}.
\]
\end{exemplo}

\begin{proposicao}
Temos as seguintes propriedades:
\begin{itemize}
\item[1)] Para todo $A\in M_n(K)$ temos $\det(A)=\det(A^t)$.
\item[2)] Para $A,B\in M_n(K)$ vale $\det(AB)=\det(A)\det(B)$.
\item[3)] Para $A\in M_n(K)$, então $A$ é inversível se e só se $\det(A)\neq 0$. Neste caso, temos $\det(A^{-1})=(\det(A))^{-1}$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Sendo $A=(a_{i,j})\in M_n(K)$, então temos:
\[
\begin{array}{rccll}
\det(A)&=&\sum\limits_{\sigma\in S_n}&\sgn(\sigma)&a_{1,\sigma_1}\dots a_{n,\sigma_n}\\&=&\sum\limits_{\sigma\in S_n}&\sgn(\sigma)&a_{\sigma^{-1}_1,1}\dots a_{\sigma^{-1}_n,n}\\&=&\sum\limits_{\tau\in S_n}&\sgn(\tau^{-1})&a_{\tau_1,1}\dots a_{\tau_n,n}\\&=&\sum\limits_{\tau\in S_n}&\sgn(\tau)&a^t_{1,\tau_1}\dots,a^t_{n,\tau_n}\\&=&\det(A^t).&&
\end{array}
\]
\item[2)] Seja $F_A:M_n(K)\rightarrow K$ tal que $\forall X\in M_n(K):F_A(X)=\det(AX)$. Então a função $F_A$ é uma função $n$-linear alternada sobre as colunas, mas também $F_A(I)=\det(A)$, aí $F_A(B)=\det(A)\det(B)$, assim $\det(AB)=\det(A)\det(B)$
\item[3)] Se $A$ é inversível, então existe a inversa $A^{-1}$, assim:
\[
1=\det(I)=\det(AA^{-1})=\det(A)\det(A)^{-1},
\]
aí $\det(A)\neq 0$ e $\det(A^{-1})=(\det(A))^{-1}$. Por outro lado, se $\det(A)\neq 0$, então $\det(A^t)\neq 0$, aí as colunas de $A$ são linearmente independentes, aí consideremos $T:K^n\rightarrow K^n$ tal que $[T]_{\mathrm{can}}=A$, então $T$ é inversível, assim $A=[T]_{\mathrm{can}}$ é inversível.
\end{itemize}
\end{proof}

\noindent
Assim lembremo-nos do seguinte: a função $\det$ é uma função $n$-linear e alternada nas linhas (ou nas colunas) da matriz, logo:
\begin{itemize}
\item[1)] Trocar duas linhas (ou colunas) da matriz muda o sinal do determinante.
\item[2)] Somar a uma linha (ou coluna) uma combinação linear das demais linhas (colunas) não altera o valor do determinante.
\item[3)] Ao multiplicar uma linha (ou coluna) por um escalar, o determinante fica multiplicado por esse escalar.
\end{itemize}

\begin{proposicao}
Temos o seguinte:
\begin{itemize}
\item[1)] O determinante de uma matriz triangular é o produto dos elementos da diagonal da matriz.
\item[2)] Se:
\[
A=\begin{pmatrix}
B&0\\C&D
\end{pmatrix}
\]
em que $B\in M_r(K)$ e $D\in M_{n-r}(K)$ e $C\in M_{n-r,r}(K)$ e $0\in M_{r,n-r}(K)$, então:
\[
\det(A)=\det(B)\det(D).
\]
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Seja $A=(a_{i,j})\in M_n(K)$ uma matriz triangular inferior, então para $i<j$ temos $a_{i,j}=0$, mas a única permutação $\sigma\in S_n$ tal que para todo $i=1,\dots,n$ tenhamos $i\geq\sigma_i$ é a permutação identidade $I$, assim temos:
\[
\begin{array}{rcl}
\det(A)&=&\sum_{\sigma\in S_n}\sgn(\sigma)a_{1,\sigma_1}\dots a_{n,\sigma_n}\\
&=&\sgn(I)a_{1,I_1}\dots a_{n,I_n}\\
&=&a_{1,1}\dots a_{n,n}.
\end{array}
\]
\item[2)] Seja $F:M_r(K)\rightarrow K$ tal que:
\[
F(X)=\det\begin{pmatrix}
X&0\\C&D
\end{pmatrix}.
\]
Então $F$ é $r$-linear alternada nas linhas de $X$, assim $F(X)=F(I)\det(X)$.

\medskip
\noindent
Agora consideremos $G:M_{n-r}(K)\rightarrow K$ tal que:
\[
G(Y)=\det\begin{pmatrix}
I&0\\C&Y
\end{pmatrix}
\]
Então $G$ é $(n-r)$-linear alternada nas colunas de $Y$, logo $G(Y)=G(I)\det(Y)$. Mas:
\[
G(I)=\det\begin{pmatrix}
I&0\\C&I
\end{pmatrix}=1,
\]
assim $G(Y)=\det(Y)$, aí $F(I)=G(D)=\det(D)$, assim $F(X)=F(I)\det(X)=\det(X)\det(D)$, aí acaba.
\end{itemize}
\end{proof}

\noindent
Agora temos a \textbf{regra de Laplace}:

\begin{teorema}
Dada $A=(a_{i,j})\in M_n(K)$, indicaremos por $M_{i,j}$ a matriz quadrada de tamanho $n-1$ obtida a partir de $A$ eliminando a linha $i$ e a coluna $j$.

\smallskip
\noindent
Para cada $i=1,\dots,n$, então vale:
\[
\det(A)=\sum_{j=1}^n(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
Para cada $j=1,\dots,n$, então vale:
\[
\det(A)=\sum_{i=1}^n(-1)^{i+j}a_{i,j}\det(M_{i,j}).
\]
\end{teorema}
\begin{proof}
Provaremos a primeira afirmação pois a segunda é análoga.
\end{proof}

\medskip
\noindent
\textcolor{red}{AULA DE 19 DE AGOSTO (COLOCAREI ASSIM QUE CONSEGUIR)}

\medskip
\noindent
\textcolor{red}{FICOU FALTANDO A PROVA DA REGRA DE LAPLACE E A PARTE DE MATRIZES SOBRE ANEIS COMUTATIVOS}

\medskip
\noindent
Bláa blá blá

\chapter{Formas Canônicas}

\section{Espectro de um Operador}

\subsection{Autovalores e Autovetores}

\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\).
\begin{itemize}
\item Para $\lambda\in K$, dizemos que $\lambda$ é um \textbf{autovalor} de $T$ se existe um $v\neq 0$ tal que $T(v)=\lambda v$.\index{Autovalor}
\item Para $\lambda\in K$, um \textbf{autovetor} associado a $\lambda$ é um $v\in V$ tal que $T(v)=\lambda v$.\index{Autovetor}
\item Para $\lambda\in K$, chamamos de \textbf{autoespaço} associado a $\lambda$ o conjunto $V_T(\lambda)$ dos autovetores associados a $\lambda$.\index{Autoespaço}
\end{itemize}
\end{definicao}

\begin{exemplo}
Seja \(V=\mathbb{C}^1(\mathbb{R})\) e considere o operador linear \(T\in\mathcal{L}(V)\) tal que \(T(v)=v^\prime\) para cada \(v\in V\). Considere \(v=e^{\lambda x}\) com \(\lambda\in K\). Então \(T(v)=\lambda e^{\lambda x}=\lambda v\). Ou seja \(v\) é um autovetor associado a \(\lambda\).
\end{exemplo}

\begin{definicao}\index{Spectrum}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \textbf{spectrum} do operador \(T\) é o conjunto:
\[\text{Spec}(T)\coloneqq\{\lambda\in K\,\colon \lambda \text{ é autovalor de } T\}.\]
\end{definicao}

\noindent
No contexto da definição anterior, considere
\(\lambda\in\text{Spec}(T)\). Então
\begin{align*}
v\in V_T(\lambda)&\iff T(v)=\lambda v\\&\iff (T-\lambda I)(v)=0\\&\iff v\in\text{Ker}(T-\lambda I).
\end{align*}

\noindent
Ainda no mesmo contexto, vamos assumir agora que \(\text{dim}(V)=n<\infty\). Então temos 
que \[\lambda\in\text{Spec}(T)\implies\text{Ker}(T-\lambda I)\not =\{0\}\implies\text{det}(T-\lambda I)=0.\]
Reciprocamente, se \(\text{det}(T-\lambda I)=0\) então \(V_T(\lambda)=\text{Ker}(T-\lambda I)\not=\{0\}\).

\subsection{Polinômio Característico}

\begin{definicao}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) e seja \(T\in\mathcal{L}(V)\). O \textbf{polinômio característico} de \(T\) é o polinômio:
\[p_T(t)\coloneqq\text{det}(tI-T).\]
\end{definicao}

\noindent
Note que \(\lambda\in\text{Spec}(T)\) se e só se \(\lambda\) é raiz de \(p_T(\lambda)\). Além disso, note que se  \(B\) e \(B^\prime\) são bases \(V\), então \(p_T(\lambda)=p_{[T]_B}(\lambda)\). De fato, se \(P\) é a matriz de mudança da base \(B\) para a base \(B^\prime\), então
\begin{align*}
[\lambda I - T]_{B^\prime}&=P^{-1}[\lambda I - T]_{B}P
\end{align*}
Isso implica que \[\text{det}([\lambda I- T]_{B^\prime})=\text{det}(P^{-1})\text{det}([\lambda I - T]_B)\text{det}(P).\]
Ou seja,
\[\text{det}([\lambda I- T]_{B^\prime})=\text{det}([\lambda I - T]_B).\]
\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^2)\) tal que \[[T]_{\text{can}}=\begin{pmatrix}
0 & -1\\1 & 0
\end{pmatrix}.\]
Isto é, \(T(x,y)=(-y,x)\) para cada \((x,y)\in\mathbb{R}^2\). Então 
\begin{align*}
p_T(x)&=\text{det}\begin{pmatrix}
x & 1\\-1 & x
\end{pmatrix}\\&=x^2+1.
\end{align*}
Dessa forma, \(\text{Spec}(T)=\varnothing\) pois \(p_T(x)\) não possui raízes em \(K=\mathbb{R}\).
\end{exemplo}

\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que\[[T]_{\text{can}}=\begin{pmatrix}
3 & 1 & -1\\2 & 2 & -1\\ 2 & 2 & 0
\end{pmatrix}.\]
Então:
\begin{align*}
p_T(x)&=\text{det}\begin{pmatrix}
x-3 & -1 & 1 \\ -2 & x-2 & 1 \\ -2 & -2 & x
\end{pmatrix}\\&=(x-1)^2(x-2).
\end{align*}
Isso implica que \(\text{Spec}(T)=\{1,2\}\). Além disso, temos que
\[V_T(1)=\text{Ker}(T-I)=\text{Ker}\begin{pmatrix}
2 & 1 & -1\\2 & 1 & -1\\ 2 & 2 & -1
\end{pmatrix}=\langle (1,0,2)\rangle.\]
e ainda
\[V_T(2)=\text{Ker}(T-2 I)=\text{Ker}\begin{pmatrix}
1 & 1 & -1\\2 & 0 & -1\\ 2 & 2 & -2
\end{pmatrix}=\langle (1,1,2)\rangle\]
\end{exemplo}

\begin{exemplo}
Seja \(T\in\mathcal{L}(\mathbb{R}^3)\) tal que
\[[T]_{\text{can}}=\begin{pmatrix}
1 & 2 & -1\\-2 & -3 & -1\\ 2 & 2 & -2
\end{pmatrix}.\]
Neste caso temos que
\begin{align*}
p_T(x)&=\text{det}\begin{pmatrix}
x-1 & -2 & 1\\2 & x+3 & 1\\ -2 & -2 & x+2
\end{pmatrix}\\&=(x+1)^2(x+2).
\end{align*}
Isso implica que \(\text{Spec}(T)=\{-1,-2\}\) e ainda \[V_T(-1)=\langle(1,0,2),(0,1,2)\rangle\] e
\[V_T(-2)=\langle (1,-1,1)\rangle.\]
Uma vez que os autovetores acima são L.I, eles formam uma base \(B\) de \(\mathbb{R}^3\) e 
\[[T]_B=\begin{pmatrix}
-1 & 0 & 0\\0 & -1 & 0\\ 0 & 0 & -2
\end{pmatrix}\]
é uma matriz diagonal.
\end{exemplo}
\begin{teorema}\label{diagonal}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que \(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\), e sejam $\lambda_1,\dots,\lambda_k$ os autovalores distintos, e para $i=1,\dots,k$ seja $n_k=\text{dim} \ V_T(\lambda_i)$. São equivalentes:
\begin{enumerate}
\item \(T\) é diagonalizável.
\item \(p_T(t)=(t-\lambda_1)^{n_1}\ldots(t-\lambda_k)^{n_k}\).
\item \(n_1+\ldots+n_k=n.\)
\end{enumerate}
\end{teorema}
\begin{lema}\label{autoespaco}
Sejam \(\lambda_1,\dots,\lambda_k\in K\) distintos. Então:
\begin{enumerate}
\item Se \(v_i \in V_T(\lambda_i)\) para cada \(i=1,\dots,k\) e \(v_1+\ldots+v_k=0\), então \(v_1=\ldots=v_k=0\).
\item Se \(B_i\subseteq V_T(\lambda_i)\) é L.I para cada \(i=1,\dots,k\), então $B_1\cup\dots\cup B_k$ é L.I.
\end{enumerate}
\end{lema}
\begin{proof}[Demonstração do Lema]\hfill
\begin{enumerate}
\item Vamos provar essa afirmação por indução em \(k\). Primeiro note que o
resultado é trivial quando \(k=1\). Agora seja \(k\in\mathbb{N}\) e
assuma  que o resultado vale para cada natural \(i < k\). Sejam
\(v_1,\ldots,v_k\) tais que \(v_i\in V_T(\lambda_i)\) para cada \(i=1,\dots,k\) 
e \(v_1+\ldots + v_k=0\). Então:
\begin{equation}\label{primeira}
0=\lambda_10=\lambda_1(v_1+v_2+\ldots+v_k)=\lambda_1v_1+\lambda_1v_2+\ldots+\lambda_1v_k.
\end{equation}
Além disso, é claro que:
\begin{equation}\label{segunda}
0=T(0)=T(v_1+v_2+\ldots+v_k)=\lambda_1v_1+\lambda_2v_2+\ldots+\lambda_kv_k.
\end{equation}
Subtraindo a Equação \ref{primeira} de \ref{segunda}, obtemos:
\begin{equation}\label{terceira}
(\lambda_2-\lambda_1)v_2+\ldots +(\lambda_k-\lambda_1)v_k=0.
\end{equation}
Agora notamos que cada termo $(\lambda_i-\lambda_1)v_i$ no lado esquerdo é um autovetor de \(T\) associado a $\lambda_i$ e 
aplicamos a hipótese de indução para concluir que \(v_2=\ldots =v_k=0\). 
Finalmente, como sabemos que \(v_1+\ldots +v_k=0\) e \(v_2=\ldots =
v_k=0\), obtemos que \(v_1=0\) também, o que conclui nossa prova.
\item Seja \(S\subseteq B_1\cup\dots\cup B_k\) finito e seja \(\alpha\colon S\to\mathbb{R}\) tal que:
\[
\sum_{v\in S}\alpha_vv=0.
\]
Note que
\(V_T(\lambda_i)\cap V_T(\lambda_j)=\{0\}\) sempre que e
\(i\not=j\) e então podemos escrever
\[\sum_{v\in S}\alpha_vv=\sum_{v\in S_1}\alpha_v v+\ldots+\sum_{v\in
S_k}\alpha_vv,\]
onde \(S_i\subseteq B_i\) é finito para cada \(i=1,\dots,k\). Utilizando o fato de que:
\[
\sum_{v\in S_i}\alpha_v v\in V_T(\lambda_i)
\]
para cada \(i=1,\dots,k\) e aplicando o item anterior, obtemos que
\[\sum_{v\in S_1}\alpha_vv=\ldots=\sum_{v\in S_k}\alpha_vv=0.\]
Finalmente como como \(S_i\subseteq B_i\) para cada \(i=1,\dots,k\) e
\(B_i\) é sempre L.I por hipótese segue que a restrição de \(\alpha\) a
cada \(S_i\) é identicamente nula. Como \(S=S_1\cup\dots\cup S_k\), segue
que \(\alpha\) é indenticamente nula. \qedhere
\end{enumerate}
\end{proof}

\begin{proof}
Temos o seguinte:
\begin{itemize}
\item (i)$\Rightarrow$(ii): Seja $B$ uma base tal que:
\[
[T]_B=\begin{pmatrix}
\lambda_1I_{m_1}&&\\&\ddots&\\&&\lambda_kI_{m_k}
\end{pmatrix},
\]
em que $m_1,\dots,m_k$ são inteiros positivos. Então o polinômio característico de $T$ é:
\[
p_T(t)=(t-\lambda_1)^{m_1}\ldots(t-\lambda_k)^{m_k}.
\]
Além disso, para $i=1,\dots,k$, então a matriz de $T-\lambda_iI$ é igual a:
\[
\begin{pmatrix}
(\lambda_1-\lambda_i)I_{m_1}&&&&\\&\ddots&&&\\&&0_{m_i}&&\\&&&\ddots&\\&&&&(\lambda_k-\lambda_i)I_{m_k}
\end{pmatrix},
\]
aí é fácil ver que:
\[
n_i=\mathrm{dim} \ V_T(\lambda_i)=\mathrm{dim} \ \mathrm{Ker}(T-\lambda_iI)=m_i,
\]
ou seja, $n_i=m_i$; assim:
\[
p_T(t)=(t-\lambda_1)^{n_1}\ldots(t-\lambda_k)^{n_k}.
\]
\item (ii)\(\Rightarrow\)(iii): O polinômio característico de $T$ tem grau $n$, aí:
\[
n_1+\ldots+n_k=\text{deg}(p_T(t))=n.
\]
\item (iii)\(\Rightarrow\)(i): Para cada \(i=1,\dots,k\) considere uma base \(B_i\)
de \(V_T(\lambda_i)\). Seja \(B=B_1\cup\dots\cup B_k\). Pelo Lema \ref{autoespaco},
temos que \(B\) é L.I. Como \(\vert B\vert =n\) segue que \(B\) é uma base de
\(V\). Além disso, \(B\) é uma base de autovetores de \(T\). Logo, \(T\) é diagonalizável.
\end{itemize}
\end{proof}

\subsection{Polinômio Minimal}

\begin{definicao}
Seja $V$ um espaço sobre $K$, $\dim V=n<\infty$, $T\in\mathcal{L}(V)$. Definamos por recursão $T^0=I$ e $T^{k+1}=T^k\circ T$. Se $p(t)\in K[t]$, $p(t)=a_0+a_1t+\dots+a_mt^m$, então está bem definido o operador $p(T)=a_0\cdot I+a_1\cdot I+\dots+a_m\cdot T^m\in\mathcal{L}(V)$.
\end{definicao}

\noindent
Lembremo-nos de que, se $\dim(U)=m$ e $\dim(V)=n$, então $\dim\mathcal{L}(U,V)=mn$. Assim, se $V$ é um espaço vetorial tal que $\dim(V)=n<\infty$, então $\dim\mathcal{L}(V)=n^2$, de modo que existe $m\leq n^2+1$ tal que os operadores $I,T,T^2,\dots,T^m$ sejam linearmente dependentes. Seja $m$ o menor deles. Então existem $a_0,\dots,a_{m-1}\in K$ tais que:
\[
T^m+a_{m-1}T^{m-1}+\dots+a_1T+a_0I=0.
\]
Seja:
\[
m_T(t)=t^m+a_{m-1}t^{m-1}+\dots+a_1t+a_0,
\]
então $m_T(T)=0$, e $m_T(t)$ é um polinômio mônico de grau mínimo tal que $m_T(T)=0$.

\begin{definicao}
Um polinômio mônico de grau mínimo tal que $m_T(t)\in K[t]$ tal que $m_T(t)=0$ chama-se um \textbf{polinômio minimal} do operador $T$.
\end{definicao}

\begin{lema}
Seja $f(t)\in K[t]$ tal que $f(T)=0$. Então $m(t)\mid f(t)$.
\end{lema}
\begin{proof}
Dividimos $f(t)$ por $m(t)$ (com resto):
\[
f(t)=m_T(t)\cdot q(t)+r(t),\quad\quad\deg(r(t))<\deg(m_T(t))\text{ ou }r(t)=0.
\]
Como $f(T)=0$ e $m_T(t)=0$, então $r(T)=0$, aí $r(t)=0$.
\end{proof}

\begin{corolario}
O polinômio $m_T(t)$ é único.
\end{corolario}

\noindent
Se $V$ é um espaço vetorial e $T\in\mathcal{L}(V)$, então $V$ tem uma estrutura de $K[t]$ módulo à esquerda: Se $f(t)\in K[t]$, para $v\in V$ definimos:
\[
f(t)\cdot v=f(T)(v).
\]
Além disso, se considerarmos:
\[
\begin{array}{rcl}
\varphi:K[t]&\rightarrow&\mathrm{End}(V)\\f(t)&\mapsto&f(T),
\end{array}
\]
então $\varphi$ é um homomorfismo de $K$-álgebras e portanto $\mathrm{Ker}(\varphi)$ é um ideal de $K[t]$.

\begin{teorema}
Os polinômios $p_T(t)$ e $m_T(t)$ têm as mesmas raízes em $K$ (a menos de multiplicidade). Em outras palavras, $m_T(\lambda)=0\Leftrightarrow\lambda\in\mathrm{Spec}(T)$.
\end{teorema}
\begin{proof}
Se $m_T(\lambda)=0$, então $m_T(t)=(t-\lambda)q(t)$. Por minimalidade de $m_T(t)$, $q(T)\neq 0$, então existe $w\in V$ tal que $q(T)(w)\neq 0$, aí seja $v=q(T)(w)$, então $v\neq 0$ e:
\[
\begin{array}{rcl}
(T-\lambda I)(v)&=&(T-\lambda I)q(T)(w)\\&=&m_T(t)(w)=0,
\end{array}
\]
aí $T(v)=\lambda v$, aí $\lambda\in\mathrm{Spec}(T)$.

\medskip
\noindent
Por outro lado, se $\lambda\in\mathrm{Spec}(T)$, seja $v\in V$ tal que $v\neq 0$ e $T(v)=\lambda v$, então:
\[
T(T(v))=\lambda^2 v,\dots,T^m(v)=\lambda^m v,\dots,
\]
aí para $f(t)\in K[t]$ temos $f(T)(v)=f(\lambda)\cdot v$, aí $0=m_T(T)(v)=m_T(\lambda)\cdot v$, aí $m_T(\lambda)=0$.
\end{proof}

\begin{corolario}\label{diagonal1}
Se $T$ é diagonalizável e $\mathrm{Spec}(T)=\{\lambda_1,\dots,\lambda_r\}$, então:
\[
m_T(t)=(t-\lambda_1)\dots(t-\lambda_r).
\]
\end{corolario}

\begin{proof}
Já sabemos que:
\[
m_T(t)=(t-\lambda_1)^{k_1}\dots(t-\lambda_r)^{k_r}q(t)
\]
em que $q(t)$ não tem raízes em $K$. Seja
\[
f(t)=(t-\lambda_1I)\dots(t-\lambda_rI).
\]
Seja $v\in V$, então $v=v_1+\dots+v_r$ para alguns $v_i\in V_T(\lambda_i)$, aí temos $(T-\lambda_iI)(v_i)=0$, aí $f(T)(v_i)=0$; logo $f(T)(v)=0$. Portanto $f(T)=0$, aí $m_T\mid f$, aí $m_T=f$.
\end{proof}

\section{Restrições de Operadores}

\subsection{Subespaços Invariantes}

\begin{definicao}
Seja $T\in\mathcal{L}(V)$. Um subespaço $W\subseteq V$ chama-se \textbf{$T$-invariante} se $T(W)\subseteq W$.
\end{definicao}

\begin{observacao}
Um subespaço é $T$-invariante se e só se é um $K[t]$-submódulo.
\end{observacao}
\begin{exemplo}
Seja \(V=\mathbb{C}(\mathbb{R})\) e considere o operador \(D\colon f\to
f^\prime\). Então o subespaço\[P_n\coloneqq\{f(t)\in\mathbb{R}[t]\,\colon
\text{deg}(f)\leq n\}\] é \(D\)-invariante. 
\end{exemplo}

\begin{proposicao}
Seja \(V\) um espaço vetorial de dimensão finita sobre um corpo \(K\), seja \(T\in\mathcal{L}(V)\)
e seja \(W\) um subespaço \(T\)-invariante de \(V\). Seja $B_1$ uma base de $W$ e seja $B$ uma base de $V$ tal que $B_1\subseteq B$. Então $B_2=\{\overline{b}:b\in B\setminus B_1\}$ é uma base de $V/W$ e, sendo $\overline{T}\in L(V/W)$ o operador induzido, temos:
\[
[T]_B=
\begin{pmatrix}
[T\upharpoonright_W]_{B_1}&*\\0&[\overline{T}]_{B_2}
\end{pmatrix},
\]
\end{proposicao}
\begin{proof}
Seja $\dim(V)=\textcolor{blue}{n}$ e $T\in\mathcal{L}(V)$, e seja $W\subseteq V$ um subespaço
$T$-invariante. Escolhemos uma base $B_1=\{v_1,\dots,v_{\textcolor{red}{m}}\}$ de $W$ e
completemos uma base $B=\{v_1,\dots,v_{\textcolor{red}{m}},v_{m+1},\dots,v_{\textcolor{blue}{n}}\}$ do espaço $V$. Qual é a matriz
$[T]_B$?

\medskip
\noindent
Vamos começar notando que \(T\) é \(W\)-invariante e então:
\[
\begin{array}{lcl}
T(v_1)&=&\sum\limits_{i=1}^{\textcolor{red}{m}}\alpha_{1,i}v_i\\T(v_2)&=&\sum\limits_{i=1}^{\textcolor{red}{m}}\alpha_{2,i}v_i\\&\vdots&\\T(v_m)&=&\sum\limits_{i=1}^{\textcolor{red}{m}}\alpha_{m,i}v_i\\T(v_{m+1})&=&\sum\limits_{i=1}^{\textcolor{blue}{n}}\alpha_{m+1,i}v_i\\&\vdots&\\T(v_n)&=&\sum\limits_{i=1}^{\textcolor{blue}{n}}\alpha_{n,i}v_i.
\end{array}
\]
Dessa forma segue que:
\[[T]_B=\begin{pmatrix}
\alpha_{1,1} & \ldots & \alpha_{1,m} &\alpha_{1,m+1}& \ldots & \alpha_{1,n}
\\\ldots & \ldots & \ldots & \ldots &\ldots & \ldots
\\\alpha_{m,1} & \ldots & \alpha_{m,m} & \alpha_{m,m+1} & \ldots &
\alpha_{m,n}\\ 0 & \ldots & 0 & \alpha_{m+1,m+1}&\ldots &\alpha_{m+1,n}\\
\ldots & \ldots & \ldots & \ldots &\ldots & \ldots \\ 0 & \ldots & 0
&\alpha_{n,m+1} & \ldots & \alpha_{n,n}
\end{pmatrix}.\]

\noindent
Isto é, a matriz de \(T\) na base \(B\) tem a forma:
\[[T]_B=\begin{pmatrix}
X & * \\ 0 & Y\end{pmatrix},\]
onde \(X\in M_m(K)\) e \(Y\in M_{n-m}(K)\). Note que \(X\) é a matriz da
restrição de \(T\) a \(W\) na base \(B_1\). Além disso, temos o seguinte:
\[
\begin{array}{lcl}
\bar{T}(\bar{v}_{m+1})&=&\sum\limits_{i={\textcolor{red}{m}}+1}^{\textcolor{blue}{n}}\alpha_{m+1,i}\bar{v}_i\\&\vdots&\\\bar{T}(\bar{v}_n)&=&\sum\limits_{i={\textcolor{red}{m}}+1}^{\textcolor{blue}{n}}\alpha_{n,i}\bar{v}_i.
\end{array}
\]
Portanto $Y$ é a matriz da transformação $\bar{T}$ na base $\{\bar{v}_{m+1},\dots,\bar{v}_n\}$.
\end{proof}

\begin{lema}
Seja $\dim(V)=n$, $T\in\mathcal{L}(V)$ e $W\subseteq V$ um subsepaço $T$-invariante. Então:
\[
p_T(t)=p_{T\upharpoonright_W}(t)\cdot p_{\bar{T}}(t)
\]
em que $\bar{T}$ é o operador induzido em $V/W$.
\end{lema}
\begin{proof}
Escolhamos $B_1$ e $B$ como bases de $W$ e $V$ tais que $B_1\subseteq B$, então, considerando $B_2=\{\bar{b}:b\in B\setminus B_1\}$, a matriz de \(T\) na base \(B\) tem a forma:
\[Z=\begin{pmatrix}
X & * \\ 0 & Y\end{pmatrix},\]
onde $X$ é a matriz de $T\upharpoonright_W$ em relação a $B_1$ e $Y$ é a matriz de $\bar{T}$ em relação a $B_2$, assim:
\begin{align*}
p_T(t)&=p_Z(t)\\
&=\det(tI-Z)\\
&=\text{det}\begin{pmatrix}
tI_m-X & * \\ 0 & tI_{n-m}-Y\end{pmatrix}\\
&=\text{det}(tI_m-X)\text{det}(tI_{n-m}-Y)\\
&=p_X(t)p_Y(t)\\
&=p_{T\upharpoonright_W}(t)p_{\bar{T}}(t)\qedhere
\end{align*}
\end{proof}

\begin{observacao}
O mesmo \textcolor{red}{não} ocorre para polinômios minimais. De fato, seja $T=I_V$ e seja $W$ um subespaço $T$-invariante (De fato, quando \(T\) é a identidade, todo subespaço de \(V\) é \(T\)-invariante), então $T_1=I_W$ e $T_2=I_{V/W}$ e aí $m_T(t)=m_{T_1}(t)=m_{T_2}(t)=t-1$. Não obstante, ainda temos um resultado interessante para isso.
\end{observacao}

\begin{lema}
Seja $\dim(V)=n$, $T\in\mathcal{L}(V)$ e $W\subseteq V$ um subsepaço $T$-invariante. Então:
\[
m_{T\upharpoonright_W}(t)\mid m_T(t),\quad m_{\bar{T}}(t)\mid m_T(t),
\]
em que $\bar{T}$ é o operador induzido em $V/W$.
\end{lema}
\begin{proof}
Escolhamos $B_1$ e $B$ como bases de $W$ e $V$ tais que $B_1\subseteq B$, então, considerando $B_2=\{\bar{b}:b\in B\setminus B_1\}$, a matriz de \(T\) na base \(B\) tem a forma:
\[Z=\begin{pmatrix}
X & * \\ 0 & Y\end{pmatrix},\]
onde $X$ é a matriz de $T\upharpoonright_W$ em relação a $B_1$ e $Y$ é a matriz de $\bar{T}$ em relação a $B_2$, assim é fácil de mostrar por indução que para todo $k\geq 0$ então temos uma matriz da forma:
\[
Z^k=
\begin{pmatrix}
X^k & * \\ 0 & Y^k
\end{pmatrix},
\]
assim temos uma matriz é da forma:
\[
m_T(Z)=
\begin{pmatrix}
m_T(X) & * \\ 0 & m_T(Y)
\end{pmatrix},
\]
mas sabemos que $m_T(Z)=0$, aí $m_T(X)=0$ e $m_T(Y)=0$, aí a conclusão segue.
\end{proof}

\subsection{Subespaços Cíclicos}

\begin{definicao}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$. Para $v\in V$, definimos o \textbf{subespaço $T$-cíclico gerado por $v$} como o conjunto $Z(v,T)$ de todos os vetores da forma $p(T)(v)$ em que $p\in K[x]$. Dizemos que $v$ é um \textbf{vetor cíclico} para $T$ se e só se $Z(v,T)=V$.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Para $v\in V$, então $Z(v,T)$ é o subespaço gerado pelo conjunto $\{T^n(v):n\in\mathbb{N}\}$.
\end{proposicao}

\begin{definicao}
Dado $v\in V$, um polinômio mônico de grau mínimo $m_{T,v}(t)\in K[t]$ tal que $m_{T,v}(T)(v)=0$ chama-se um \textbf{polinômio $T$-anulador} do vetor $v$.
\end{definicao}

\begin{lema}
Seja $f(t)\in K[t]$ tal que $f(T)(v)=0$. Então $m_{T,v}(t)\mid f(t)$.
\end{lema}
\begin{proof}
Dividimos $f(t)$ por $m_{T,v}(t)$ (com resto):
\[
f(t)=m_{T,v}(t)\cdot q(t)+r(t),\quad\quad\deg(r(t))<\deg(m_{T,v}(t))\text{ ou }r(t)=0.
\]
Como $f(T)(v)=0$ e $m_{T,v}(T)(v)=0$, então $r(T)(v)=0$, aí $r(t)=0$.
\end{proof}

\begin{corolario}
O polinômio $m_{T,v}(t)$ é único.
\end{corolario}

\begin{teorema}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$. Seja $v\in V$ que possua um polinômio $T$-anulador, e consideremos o polinômio $m_{T,v}$. Então:
\begin{itemize}
\item O grau de $m_{T,v}$ é igual à dimensão de $Z(v,T)$.
\item Se o grau de $m_{T,v}$ é $k$, então $v,T(v),\dots,T^{k-1}(v)$ formam uma base de $Z(v,T)$.
\item Se $W=Z(v,T)$, então $m_{T\upharpoonright_W}=m_{T,v}$.
\end{itemize}
\end{teorema}

\noindent
O teorema seguinte mostra como subespaços cíclicos podem ser compostos e decompostos.

\begin{teorema}\label{ciclico}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$.
\begin{itemize}
\item (Compondo subespaços cíclicos) Se $u_1,\dots,u_n\in V$ tem $T$-anuladores mutuamente primos entre si, então o $T$-anulador de $u=u_1+\dots+u_n$ é:
\[
m_{T,u}=m_{T,u_1}\dots m_{T,u_n}
\]
e também:
\[
Z(u,T)=Z(u_1,T)\oplus\dots\oplus Z(u_n,T).
\]
\item (Decompondo subespaços cíclicos) Se $m_{T,u}=f_1\dots f_n$ com $f_1,\dots,f_n$ mutuamente primos entre si, então $u$ tem a forma:
\[
u=u_1+\dots+u_n,
\]
em que $m_{T,u_i}=f_i$, e também:
\[
Z(u,T)=Z(u_1,T)\oplus\dots\oplus Z(u_n,T).
\]
\end{itemize}
\end{teorema}

\section{Teoremas de Cayley-Hamilton}

\subsection{Teorema de Cayley-Hamilton Usual}

\begin{teorema}[Teorema da Cayley-Hamilton]\label{cayley-hamilton}
Seja \(V\) um espaço vetorial de dimensão finita sobre um corpo \(K\), e seja \(T\in\mathcal{L}(V)\). Então \(p_T(T)=0\), onde \(p_T(t)\in K[t]\) é um polinômio característico de \(T\). 
\end{teorema}
\begin{proof}
Basta provar que \(\forall v\in V:p_T(T)(v)=0\). Seja $v\in V$. Consideremos:
\[
m_{T,v}(t)=t^m+\alpha_{m-1}t^{m-1}+\dots+\alpha_1 t+\alpha_0,
\]
o polinômio mônico de menor grau tal que \(m_{T,v}(T)(v)=0\). Então \(B_1=\{v,T(v),\ldots, T^{m-1}(v)\}\) é linearmente independente. Seja \(W\) o subespaço gerado por ele. Note que \(W\) é \(T\)-invariante e ainda:
\[[T\upharpoonright_W]_{B_1}=\begin{pmatrix}
0 & 0 & \ldots& 0 & -\alpha_0
\\ 1 & 0 & \ldots & 0 & -\alpha_1
\\ 0 & 1 & \ldots & 0 & -\alpha_2
\\ \ldots & \ldots  & \ldots  & \ldots  & \ldots \
\\ 0 & 0 & \ldots & 1 & -\alpha_{m-1} 
\end{pmatrix}\]
Então, pelo Exercício 18 da lista 1, segue que:
\[
p_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.\]
Aplicando essa função a \(v\) segue:
\[p_{T \upharpoonright_{W}}(T)(v)=m_{T,v}(T)(v)=0.\]
Para concluir que \(p_T(T)(v)=0\), notamos que \(p_T(t)=p_{T\upharpoonright_W}(t)\cdot p_{\bar{T}}(t),\) em que $\bar{T}$ é o operador induzido em $V/W$.
\end{proof}
\begin{corolario}
Se \(A\in M_n(K)\) então \(p_A(A)=0\),
onde \(p_A(t)=\text{det}(tI -A)\). \end{corolario}

\begin{exemplo}
Considere a matriz\[\begin{pmatrix}
a & b \\ c & d
\end{pmatrix}.\]
Então temos que \(p_A(t)=t^2-(a+d)t+(ad-bc)\) e também
\begin{align*}
P_A(A)&=A^2-(a+d)A+(ad-bc)I\\
&=
\begin{pmatrix} a^2+bc & ab+ad\\ac+dc & bc+d^2\end{pmatrix}
-\begin{pmatrix} a^2+ad & ab+bd \\ ac+dc & ad+d^2\end{pmatrix}
+ \text{det}(A)I
\\&=\begin{pmatrix}bc-ad & 0 \\ 0 & bc- ad\end{pmatrix}
+ \begin{pmatrix}ad-bc & 0 \\ 0 & ad - bc\end{pmatrix} = 0.
\end{align*}
\end{exemplo}

\subsection{Teorema de Cayley Hamilton ao Avesso}

\begin{teorema}[Teorema da Cayley-Hamilton ao Avesso]\label{cayley-hamilton-avesso}
Se $V$ é espaço de dimensão finita e $T\in L(V)$, então todo polinômio irredutível que divide $p_T$ também divide $m_T$.
\end{teorema}
\begin{proof}
Faremos a demonstração por indução na dimensão de $V$. Suponhamos o lema válido para $\dim(V)<n$. Seja $V$ espaço vetorial tal que $\dim(V)=n$ e seja $T\in L(V)$ e seja $p$ um polinômio irredutível que divide $p_T$. Se $V=0$, é fácil. Senão, então tome um $v\neq 0$ qualquer. Consideremos:
\[
m_{T,v}(t)=t^m+\alpha_{m-1}t^{m-1}+\dots+\alpha_1 t+\alpha_0,
\]
o polinômio mônico de menor grau tal que \(m_{T,v}(T)(v)=0\). Então \(B_1=\{v,T(v),\ldots, T^{m-1}(v)\}\) é linearmente independente. Seja \(W\) o subespaço gerado por ele. Note que \(W\) é \(T\)-invariante e ainda:
\[[T\upharpoonright_W]_{B_1}=\begin{pmatrix}
0 & 0 & \ldots& 0 & -\alpha_0
\\ 1 & 0 & \ldots & 0 & -\alpha_1
\\ 0 & 1 & \ldots & 0 & -\alpha_2
\\ \ldots & \ldots  & \ldots  & \ldots  & \ldots \
\\ 0 & 0 & \ldots & 1 & -\alpha_{m-1} 
\end{pmatrix}\]
Então, pelo Exercício 18 da lista 1, segue que:
\[
p_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.
\]
Além disso, vendo a definição de $m_{T,v}$, é fácil ver que:
\[
m_{T \upharpoonright_{W}}(t)=
t^m + \alpha_{m-1}t^{m-1} + \ldots + \alpha_1t+\alpha_0.
\]
Sendo $\overline{T}\in L(V/W)$ o operador induzido, temos $p_T=p_{T\upharpoonright_W}p_{\overline{T}}$ e $m_{T\upharpoonright_W}\mid m_T$ e $m_{\overline{T}}\mid m_T$. Assim, como $p\mid p_T$, então $p\mid p_{T\upharpoonright_W}$ ou $p\mid p_{\overline{T}}$.
\begin{itemize}
\item Se $p\mid p_{T\upharpoonright_W}$, como $p_{T\upharpoonright_W}=m_{T\upharpoonright_W}$, então $p\mid m_{T\upharpoonright_W}$, aí $p\mid m_T$.
\item Se $p\mid p_{\overline{T}}$, então, por hipótese de indução, temos $p\mid m_{\overline T}$, aí $p\mid m_T$. \qedhere
\end{itemize}
\end{proof}

\section{Decomposições Primárias}

\subsection{Decomposição Primária Geral}

\begin{teorema}[Decomposição Primária Geral]
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que
\(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\). Suponhamos que
\(f(T)=0\) e também:
\[
f=f_1\ldots f_r,
\]
com $f_1,\dots,f_r$ mutuamente primos entre si. Para cada $i$ seja $V_i=\Ker \ f_i(T)$. Então:
\begin{itemize}
\item Cada $V_i$ é $T$-invariante.
\item $V=V_1\oplus\dots\oplus V_r$.
\item Cada projeção canônica $P_i:V\rightarrow V_i$ é um polinômio de $T$.
\end{itemize}
\end{teorema}
\begin{lema}[Identidade de Bézout]
Se \(\mathrm{mdc}(f_1,\dots,f_r)=1\) então existem $g_1,\dots,g_r\in K[t]$ tais que:
\[f_1g_1+\dots+f_rg_r=1.\]
\end{lema}
\begin{proof}[Demonstração do Teorema]
Para cada $i$, então para $v\in V_i$ temos $f_i(T)(v)=0$, aí $Tf_i(T)(v)=0$, aí $f_i(T)T(v)=0$, aí $T(v)\in V_i$; logo $V_i$ é $T$-invariante. Agora para cada $i$ seja:
\[
h_i=\frac{f}{f_i}.
\]
Então $\mathrm{mdc}(h_1,\dots,h_r)=1$, assim existem $g_1,\dots,g_r\in K[t]$ tais que:
\[h_1g_1+\dots+h_rg_r=1,\]
aí:
\[
h_1(T)g_1(T)+\dots+h_r(T)g_r(T)=I.
\]
Para $v_1,\dots,v_r\in V$, se para todo $i$ tivermos $v_i\in V_i$ e:
\[
v_1+\dots+v_r=0,
\]
então para cada $i$ temos:
\[
h_i(T)(v_1)+\dots+h_i(T)(v_r)=0,
\]
mas para cada $j\neq i$ então $f_j\mid h_i$, aí $h_i(T)(v_j)=0$; assim:
\[
h_i(T)(v_i)=0,
\]
mas:
\[
v_i=h_1(T)g_1(T)(v_i)+\dots+h_r(T)g_r(T)(v_i),
\]
e para cada $j\neq i$ temos $f_i\mid h_j$, aí $h_j(T)(v_i)=0$; assim:
\[
v_i=h_i(T)g_i(T)(v_i)=0;
\]
portanto:
\[
v_1=\dots=v_r=0.
\]
Para todo $v\in V$ então:
\[
v=h_1(T)g_1(T)(v)+\dots+h_r(T)g_r(T)(v),
\]
e para cada $i$ temos:
\[
f_i(T)h_i(T)g_i(T)(v)=f(T)g_i(T)(v)=0,
\]
aí:
\[
h_i(T)g_i(T)(v)\in V_i;
\]
logo:
\[
V=V_1\oplus\dots\oplus V_r,
\]
e, para cada $i$, a função $h_i(T)g_i(T)$ é a projeção canônica de $V$ em $V_i$.
\end{proof}

\subsection{Decomposição Primária para Minimais}

\begin{teorema}[Decomposição Primária para Minimais]\label{primaria minimal}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que
\(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\), e seja:
\[
m_T=p_1^{k_1}\ldots p_r^{k_r},
\]
com \(p_1\dots,p_r\) irredutíveis e mutuamente
primos entre si. Para cada $i$ seja $V_i=\Ker \ p_i(T)^{k_i}$. Então:
\begin{itemize}
\item Cada $V_i$ é $T$-invariante.
\item $V=V_1\oplus\dots\oplus V_r$.
\item Cada projeção canônica $P_i:V\rightarrow V_i$ é um polinômio de $T$.
\item Para cada $i$ então \(m_{T\upharpoonright_{V_i}}=p_i^{k_i}\).
\end{itemize}
\end{teorema}
\begin{proof}
Por definição temos \(m_T(T)=0\). Logo, pelo teorema da decomposição primária geral, temos os três primeiros itens. Agora considere \(T_i\coloneqq T\upharpoonright_{V_i}\)
para cada \(i=1,\dots,r\).
Temos que \(p_i(T_i)^{k_i}=0\). Então segue que \(m_{T_i}\mid
p_i^{k_i}\), ou seja,
\(m_{T_i}=p_i^{m_i}\), onde \(m_i\leq k_i\). Consideremos:
\[
g=p_1^{k_1}\ldots p_i^{m_i}\ldots
p_r^{k_r}.
\]
Para $j\neq i$ e para \(v\in V_j\), então \(p_j^{k_j}(v)=0\) e portanto
\(g(T)(v)=0\). Se \(v\in V_i\) então \(p_i^{m_i}(T)(v)=0\) e
\(g(T)(v)=0\). Assim, como $V=V_1\oplus\dots\oplus V_r$, concluímos que \(g(T)=0\).
Isso implica que \(m_T\mid g\), aí \(p_i^{k_i}\mid p_i^{m_i}\), aí $k_i\leq m_i$, assim $m_i=k_i$, aí \(m_{T_i}=p_i^{k_i}.\)
\end{proof}

\noindent
A decomposição primária para minimais também goza de uma propriedade muito importante para o estudo da álgebra linear.

\begin{teorema}[Unicidade da Decomposição Primária para Minimais]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que:
\[
V=U_1\oplus\dots\oplus U_m
\]
em que $U_i$ é um subespaço $T$-invariante tal que $m_{T\upharpoonright_{U_i}}=p_i^{e_i}$ e $p_1,\dots,p_m$ são polinômios mônicos irredutíveis distintos, e suponhamos que:
\[
V=W_1\oplus\dots\oplus W_n
\]
em que $W_j$ é um subespaço $T$-invariante tal que $m_{T\upharpoonright_{W_j}}=q_j^{f_j}$ e $q_1,\dots,q_n$ são polinômios mônicos irredutíveis distintos. Então $m=n$ e, depois de uma reindexação adequada, $U_k=W_k$ para todo $k$. Portanto $p_k=q_k$ e $e_k=f_k$ para todo $k$.
\end{teorema}
\begin{proof}
Para todo $i$, então $U_i$ contém um elemento $u_i$ tal que $m_{T,u_i}=p_i^{e_i}$; assim, definindo a soma $u=u_1+\dots+u_m$, então $m_{T,u}=p_1^{e_1}\dots p_m^{e_m}$. Logo $m_T=p_1^{e_1}\dots p_m^{e_m}$. Analogamente temos $m_T=q_1^{f_1}\dots q_n^{f_n}$.

\medskip
\noindent
Portanto, pela fatoração única em $K[t]$, então $m=n$ e, depois de uma reindexação apropriada, temos $p_k=q_k$ e $e_k=f_k$ para todo $k$. Para todo $k$, temos:
\[
U_k\subseteq\Ker \ p_k(T)^{e_k},
\]
mas pela decomposição primária geral temos:
\[
U_1\oplus\dots\oplus U_m=\left(\Ker \ p_1(T)^{e_1}\right)\oplus\dots\oplus\left(\Ker \ p_m(T)^{e_m}\right),
\]
aí para todo $k$ temos $U_k=\Ker \ p_k(T)^{e_k}$. Analogamente temos $W_k=\Ker \ q_k(T)^{f_k}$ para todo $k$.
\end{proof}

\subsection{Decomposição Primária para Característicos}

\begin{teorema}[Decomposição Primária para Característicos]\label{primaria caracteristico}
Seja \(V\) um espaço vetorial sobre um corpo \(K\) tal que
\(\text{dim}(V)=n<\infty\) e seja \(T\in\mathcal{L}(V)\), e seja:
\[
p_T=p_1^{k_1}\ldots p_r^{k_r},
\]
com \(p_1\dots,p_r\) irredutíveis e mutuamente
primos entre si. Para cada $i$ seja $V_i=\Ker \ p_i(T)^{k_i}$. Então:
\begin{itemize}
\item Cada $V_i$ é $T$-invariante.
\item $V=V_1\oplus\dots\oplus V_r$.
\item Cada projeção canônica $P_i:V\rightarrow V_i$ é um polinômio de $T$.
\item Para cada $i$ então \(p_{T\upharpoonright_{V_i}}=p_i^{k_i}\).
\end{itemize}
\end{teorema}
\begin{proof}
Pelo teorema de Cayley-Hamilton temos \(p_T(T)=0\). Portanto, pelo teorema da decomposição primária geral, temos os três primeiros itens. Agora considere \(T_i\coloneqq T\upharpoonright_{V_i}\)
para cada \(i=1,\dots,r\).
Temos que \(p_i(T_i)^{k_i}=0\). Então segue que \(m_{T_i}\mid
p_i^{k_i}\), aí, pelo teorema de Cayley-Hamilton \textcolor{red}{ao avesso} (Teorema \ref{cayley-hamilton-avesso}), todo fator irredutível de $p_{T_i}$ deve dividir $m_{T_i}$, logo ser igual a $p_i$. Assim $p_{T_i}=p_i^{l_i}$ para algum $l_i$. Entretanto, considerando bases $B_i$ de $V_i$, e juntando numa base $B$ de $V$, é fácil ver que $p_T=p_{T_1}\dots p_{T_k}$, assim $p_T=p_1^{l_1}\ldots p_r^{l_r}$, aí pela fatoração única devemos ter $l_i=k_i$ para todo $i$, concluindo a demonstração.
\end{proof}

\section{Critérios de Diagonalização}

\subsection{Diagonalização Usual}

\begin{teorema}\label{diagonal2}
Um operador \(T\in\mathcal{L}(V)\) é diagonalizável se, e somente se:
\[
m_T(t)=(t-\lambda_1)\ldots(t-\lambda_r)
\]
com \(\lambda_i\not=\lambda_j\) sempre que \(i\not= j\).
\end{teorema}
\begin{proof}
A ida já foi provada no corolário \ref{diagonal1}, então
vamos mostrar apenas a volta. Pelo teorema da decomposição primária geral, sendo $V_i=\mathrm{Ker} \ (T-\lambda_iI)$ para todo $i$, então cada $V_i$ é $T$-invariante e \(V=V_1\oplus\ldots\oplus V_r\), aí para todo $i$ temos \(T\upharpoonright_{V_i}=\lambda_iI\), aí \(V_i\subseteq V_T(\lambda_i)\); logo $T$ é diagonalizável. 
\end{proof}

\subsection{Diagonalização Simultânea de Vários Operadores}

Considere \(\mathcal{F}\subseteq\mathcal{L}(V)\). Em que condições os operadores $T\in\mathcal{F}$ podem ser diagonalizados simultaneamente, ou seja, existe base $B$ de $V$ tal que para todo $T\in\mathcal{F}$ a matriz $[T]_B$ seja diagonal?

\begin{teorema}
Um conjunto \(\mathcal{F}\subseteq\mathcal{L}(V)\) pode ser diagonalizado
simultaneamente se, e somente se cada \(T\in\mathcal{F}\) é diagonalizável e \(TU=UT\) para quaisquer \(T,U\in\mathcal{F}\).
\end{teorema}
\begin{proof}
Mostraremos por indução na dimensão. Suponhamos que o teorema é válido para espaços de dimensão menor que $n$. Seja $V$ um espaço tal que $\dim(V)=n$ e seja $\mathcal{F}$ um conjunto de operadores que comutam um com outro. Se todo elemento de $\mathcal{F}$ é múltiplo da identidade, então acaba. Caso contrário, existe um $T\in\mathcal{F}$ que não é múltiplo da identidade. Sejam $c_1,\dots,c_k$ os autovalores de $T$. Para $i$ seja $W_i=\Ker(T-c_iI)$. Então, como os elementos de $\mathcal{F}$ comutam um com outro, então $W_i$ é invariante para todo elemento de $\mathcal{F}$. Para cada $U\in\mathcal{F}$, então $m_U$ é produto de fatores lineares distintos, aí, como $m_{U\upharpoonright_{W_i}}\mid m_U$, então $m_{U\upharpoonright_{W_i}}$ é produto de fatores lineares distintos, aí $U\upharpoonright_{W_i}$ é diagonalizável. Como $\dim(W_i)<n$, então existe uma base $B_i$ tal que para todo $U\in\mathcal{F}$ a matriz $[U\upharpoonright_{W_i}]_{B_i}$ seja diagonal. Portanto $B=B_1\cup\dots\cup B_k$ é a base que buscamos.
\end{proof}

\section{Triangularização de Matrizes}

\subsection{Triangularização Usual}

\begin{definicao}
Um operador linear $T\in\mathcal{L}(V)$ é dito \textbf{triangularizável} se existe uma
base ordenada $B=(v_1,\dots,v_n)$ de $V$ tal que a matriz $[T]_B$ seja triangular por cima, ou equivalentemente, se:
\[
T(v_i)\in\langle v_1\dots,v_i\rangle
\]
para todo $i=1,\dots,n$.
\end{definicao}

\begin{teorema}[Teorema de Schur]
Seja $V$ um espaço vetorial de dimensão finita sobre um corpo $K$ e seja $T\in\mathcal{L}(V)$. Se o polinômio característico (ou o polinômio minimal) de $T$ se decompõe em fatores lineares sobre $K$, então $T$ é triangularizável.
\end{teorema}
\begin{proof}
Provaremos por indução em $n$ que cada matriz $A\in\mathcal{M}_n(K)$
cujo polinômio característico se decomponha em fatores lineares sobre $K$ é similar
a uma matriz triangular por cima. Se $n=1$, então é fácil, já que todas as matrizes quadradas de tamanho $1$ são triangulares por cima. Assumamos o resultado para $n-1$ e seja $A\in M_n(K)$ tal que o polinômio característico se decomponha em fatores lineares sobre $K$.

\medskip
\noindent
Seja $\lambda$ um autovalor de $T$ e seja $e_1\neq 0$ tal que $T(e_1)=\lambda e_1$, e estenda $(e_1)$ a uma base ordenada $B=(e_1,\dots,e_n)$ de $K^n$. A matriz de $A$ em relação a $B$ tem a forma:
\[
[A]_B=\begin{pmatrix}
\lambda&*\\0&A'
\end{pmatrix}
\]
para alguma matriz $A'\in M_{n-1}(K)$. Como $[A]_B$ e $A$ são similares, temos:
\[
\det(xI-A)=\det(xI-[A]_B)=(x-\lambda)\det(xI-A').
\]
Portanto o polinômio característico de $A'$ também se decompõe em fatores lineares sobre $K$ a hipótese de indução implica que exista uma matriz inversível $P\in M_{n-1}(K)$ tal que:
\[
U=PA'P^{-1}
\]
seja triangular por cima. Assim, se:
\[
Q=\begin{pmatrix}
1&0\\0&P
\end{pmatrix},
\]
então $Q$ é inversível e:
\[
Q[A]_BQ^{-1}=\begin{pmatrix}
1&0\\0&P
\end{pmatrix}\begin{pmatrix}
\lambda&*\\0&A'
\end{pmatrix}\begin{pmatrix}
1&0\\0&P^{-1}
\end{pmatrix}=
\begin{pmatrix}
\lambda&*\\0&U
\end{pmatrix}
\]
é triangular por cima.
\end{proof}

\begin{corolario}
Num corpo algebricamente fechado $K$, então toda matriz sobre $K$ é semelhante sobre $K$ a uma matriz triangular superior.
\end{corolario}

\begin{observacao}
Por outro lado, se $T\in\mathcal{L}(V)$ é triangularizável, então é evidente que o polinômio característico de $T$ se decompõe em fatores lineares sobre $K$.
\end{observacao}
\begin{proof}
Para isto, apenas considere a matriz triangular superior:
\[
[T]_B=\begin{pmatrix}
a_{1,1}&a_{1,2}&a_{1,3}&\ldots&a_{1,n}\\0&a_{2,2}&a_{2,3}&\ldots&a_{2,n}\\0&0&a_{3,3}&\ldots&a_{3,n}\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\ldots&a_{n,n}
\end{pmatrix}.
\]
Então é fácil ver que:
\[
p_T(x)=(x-a_{1,1})\dots(x-a_{n,n}),
\]
ou seja, o polinômio característico se decompõe em fatores lineares sobre $K$.
\end{proof}

\subsection{Triangularização Simultânea de Vários Operadores}

Considere \(\mathcal{F}\subseteq\mathcal{L}(V)\). Em que condições os operadores \(T\in\mathcal{F}\) podem ser triangularizados simultaneamente, ou seja, existe base $B$ de $V$ tal que para todo $T\in\mathcal{F}$ a matriz $[T]_B$ seja triangular por cima?

\medskip
\noindent
Infelizmente não responderemos a esta pergunta, mas mostraremos uma condição suficiente para a triangularização simultânea.

\begin{teorema}
Seja \(\mathcal{F}\subseteq\mathcal{L}(V)\) um conjunto tal que cada $T\in\mathcal{F}$ seja triangularizável e \(TU=UT\) para quaisquer \(T,U\in\mathcal{F}\). Então $\mathcal{F}$ pode ser triangularizado simultaneamente.
\end{teorema}

\begin{lema}
Para todo espaço vetorial $V$ de dimensão finita tal que $V\neq 0$ e conjunto $\mathcal{F}\subseteq\mathcal{L}(V)$ de operadores triangularizáveis que comutam um com outro, então existe um $v\in V$ tal que $v\neq 0$ e para todo $T\in\mathcal{F}$ exista $\lambda\in K$ tal que $T(v)=\lambda v$.
\end{lema}

\begin{proof}
Suponhamos a afirmação para espaços vetoriais de dimensão menor que $n$. Seja $V$ um espaço com $\dim(V)=n$. Seja $\mathcal{F}$ conjunto de operadores triangularizáveis que comutam. Se todo elemento de $\mathcal{F}$ é múltiplo da identidade, então acaba. Senão, então existe $T\in\mathcal{F}$ que não é múltiplo da identidade, então, como $T$ é triangularizável, então existe um autovalor $c$, aí, sendo $W=\Ker(T-cI)$, então $0<\dim(W)<n$, aí, como os elementos de $\mathcal{F}$ se comutam, então $W$ é invariante para todo elemento de $\mathcal{F}$. Para $U\in\mathcal{F}$, então $m_U$ é produto de fatores lineares, aí, como $m_{U\upharpoonright_W}\mid m_U$, então $m_{U\upharpoonright_W}$ é produto de fatores lineares, aí $U\upharpoonright_W$ é triangularizável. Como $\dim(W)<n$, então existe um $v\in W$ não nulo tal que para cada $U\in\mathcal{F}$ exista um $\lambda\in K$ tal que $(U\upharpoonright_W)(v)=\lambda v$, aí $U(v)=\lambda v$; aí acaba.
\end{proof}

\begin{proof}[Demonstração do Teorema]

\end{proof}

\subsection{Quase Triangularização}

\section{Decomposições Cíclicas}

\subsection{Espaços Primários}

\begin{definicao}
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Dizemos que $V$ é \textbf{primário} em relação a $T$ se e só se existe um polinômio irredutível $p(t)\in K[t]$ e um natural $e$ tal que $p(T)^e=0$.
\end{definicao}

\noindent
O seguinte teorema mostra como podemos decompor um espaço vetorial anulável por uma potência de polinômio irredutível em subespaços cíclicos.

\begin{teorema}[Decomposição Cíclica de Espaços Primários]
Seja $V$ um espaço vetorial com \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$ tal que $m_T=p^e$, em que $p$ é um polinômio irredutível. Então $V$ é uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_n,T)
\]
de subespaços cíclicos com anuladores $m_{T,v_i}=p^{e_i}$, que podem ser arranjados em ordem decrescente:
\[
e=e_1\geq e_2\geq\dots\geq e_n.
\]
\end{teorema}
\begin{proof}
Seja $v_1\in V$ um vetor com anulador igual a polinômio minimal de $T$, ou seja:
\[
m_{T,v_1}=m_T=p^e.
\]
Tal elemento deve existir pois $m_{T,v}\mid m_T$ para todo $v\in V$ e se ninguém tiver anulador igual a $p(t)^e$, então $p(t)^{e-1}$ anulará $V$.

\medskip
\noindent
Se mostrarmos que $Z(v_1,T)$ possui complemento $T$-invariante, ou seja, $V=Z(v_1,T)\oplus S_1$ para algum subespaço $T$-invariante $S_1$, então, como $S_1$ tem dimensão finita sobre $K$, ao considerarmos $T\upharpoonright_{S_1}$, podemos repetir o processo para obter:
\[
V=Z(v_1,T)\oplus Z(v_2,T)\oplus S_2
\]
em que $m_{T,v_i}=p^{e_i}$. Podemos continuar esta decomposição:
\[
V=Z(v_1,T)\oplus Z(v_2,T)\oplus\dots\oplus Z(v_n,T)\oplus S_n
\]
enquanto $S_n\neq 0$. Mas a sequência ascendente de subespaços $T$-invariantes:
\[
Z(v_1,T)\subseteq Z(v_1,T)\oplus Z(v_2,T)\subseteq\dots
\]
deve terminar pois a sequência das dimensões é estritamente crescente e $V$ tem dimensão finita, assim existe um inteiro $n$ tal que $S_n=0$, fornecendo-nos a decomposição buscada.

\medskip
\noindent
Seja $v=v_1$. A soma direta de subespaços $T$-invariantes $V_1=Z(v,T)\oplus 0$ claramente existe. Suponhamos que a soma direta de subespaços $T$-invariantes:
\[
V_k=Z(v,T)\oplus W_k
\]
exista. Afirmamos que, se $V_k\neq V$, então é possível encontrar um subespaço $T$-invariante $W_{k+1}$ que contenha propriamente $W_k$ e para o qual a soma direta $V_{k+1}=Z(v,T)\oplus W_{k+1}$ exista. Esta processo deve também terminar após um número finito de passos, fornecendo-nos uma decomposição em soma direta de subespaços $T$-invariantes:
\[
V=Z(v,T)\oplus W
\]
como desejado.

\medskip
\noindent
Se $V_k\neq V$, então seja $u\in V\setminus V_k$, aí o polinômio de menor grau $r$ tal que $r(T)(u)\in V_k$ deve ser $p^f$ para algum $f\leq e$. Além disso, como $u\notin V_k$, então $f>0$. Assim existem $a(t)\in K[t]$ e $w\in W_k$ tais que:
\[
p(T)^f(u)=a(T)(v)+w.
\]
Logo:
\[
0=p(T)^e(u)=p(T)^{e-f}p(T)^f(u)=p(T)^{e-f}(a(T)(v)+w)=p(T)^{e-f}a(T)(v)+p(T)^{e-f}(w).
\]
Como $Z(v,T)\cap W_k=0$ então $p(T)^{e-f}a(T)(v)=0$, aí $p^e\mid p^{e-f}a$, aí $p^f\mid a$, aí existe $\alpha(t)\in K[t]$ tal que $a=p^f\alpha$, assim:
\[
p(T)^f(u)=a(T)(v)+w=p(T)^f\alpha(T)(v)+w,
\]
aí:
\[
p(T)^f(u-\alpha(T)(v))\in W_k.
\]
Assim seja:
\[
W_{k+1}=W_k+Z\left(u-\alpha(T)(v),T\right).
\]
Para $x\in Z(v,T)\cap W_{k+1}$, então existem $f(t)\in K[t]$ e $g(t)\in K[t]$ e $w\in W_k$ tais que:
\[
x=f(T)(v)=w+g(T)(u-\alpha(T)(v)),
\]
aí:
\[
g(T)(u)=(f-g\alpha)(T)(v)-w\in V_k,
\]
aí $p^f\mid g$, aí:
\[
g(T)(u-\alpha(T)(v))\in W_k,
\]
aí:
\[
x\in Z(v,T)\cap W_k,
\]
aí $x=0$. Logo a soma direta de subespaços $T$-invariantes:
\[
V_{k+1}=Z(v,T)\oplus W_{k+1}
\]
existe.
\end{proof}

\noindent
A decomposição cíclica de espaços primários, assim como a decomposição primária para minimais, goza da propriedade da unicidade.

\begin{teorema}[Unicidade da Decomposição Cíclica de Espaços Primários]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que $V$ é uma soma direta:
\[
V=Z(u_1,T)\oplus\dots\oplus Z(u_m,T),
\]
em que $m_{T,u_i}=p_i^{e_i}$ e também:
\[
e_1\geq e_2\geq\dots\geq e_m,
\]
e também suponhamos que $V$ é uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_n,T),
\]
em que $m_{T,v_j}=q_j^{f_j}$ e também:
\[
f_1\geq f_2\geq\dots\geq f_n.
\]
Então $m=n$ e $p=q$ e $e_k=f_k$ para todo $k$.
\end{teorema}

\begin{lema}
Seja $V$ um espaço vetorial e $T\in\mathcal{L}(V)$ e seja $p(t)$ um polinômio irredutível.
\begin{itemize}
\item Se $p(T)=0$, então $V$ é um espaço vetorial sobre o corpo $K[t]/p(t)K[t]$ com a
multiplicação definida por:
\[
\overline{r}(v)=r(T)v
\]
para quaisquer $r(t)\in K[t]$ e $v\in V$.
\item Para qualquer subespaço $T$-invariante $W$ de $V$ o conjunto:
\[
W\cap \Ker \ p(T)=\{v\in W:p(T)(v)=0\}
\]
é um subespaço $T$-invariante de $V$ e se $V=U\oplus W$, então:
\[
\Ker \ p(T)=\left(U\cap \Ker \ p(T)\right)\oplus\left(W\cap \Ker \ p(T)\right).
\]
\end{itemize}
\end{lema}

\begin{proof}[Demonstração do Teorema]
Primeiro notemos que $m_T=p_1^{e_1}$ e $m_T=q_1^{f_1}$. Assim $p=q$ e $e_1=f_1$. Agora mostraremos que $m=n$. De acordo com o lema anterior, sendo $W=Ker p(T)$, então:
\[
W=\left(W\cap Z(u_1,T)\right)\oplus\dots\oplus\left(W\cap Z(u_m,T)\right)
\]
e também:
\[
W=\left(W\cap Z(v_1,T)\right)\oplus\dots\oplus\left(W\cap Z(v_n,T)\right).
\]
Como $p(T)[W]=0$, então $W$ é um espaço vetorial sobre o corpo $L=K[t]/p(t)K[t]$ e aí cada uma das duas decomposições expressa $W$ como uma soma direta de subespaços de dimensão $1$. Logo $m=\dim_L(W)=n$.

\medskip
\noindent
Finalmente mostraremos que os expoentes $e_i$ e $f_i$ são iguais usando indução em $e_1$. Se $e_1=1$, então $e_i=1$ para todo $i$ e como $f_1=e_1$, temos também $f_i=1$ para todo $i$. Suponhamos que o resultado seja válido para $e_1\leq k-1$ e seja $e_1=k$. Escreva:
\[
(e_1,\dots,e_n)=(e_1,\dots,e_s,1,\dots,1),\quad e_s>1
\]
e
\[
(f_1,\dots,f_n)=(f_1,\dots,f_t,1,\dots,1),\quad f_t>1.
\]
Então:
\[
p(T)[V]=p(T)[Z(u_1,T)]\oplus\dots\oplus p(T)[Z(u_m,T)]
\]
e
\[
p(T)[V]=p(T)[Z(v_1,T)]\oplus\dots\oplus p(T)[Z(v_n,T)],
\]
mas $p(T)[Z(v_1,T)]$ é um subespaço cíclico anulável por $p(T)^{e_1-1}$, aí pela hipótese de indução temos:
\[
s=t\quad\text{e}\quad e_1=f_1,\dots,e_s=f_s,
\]
concluindo assim a demonstração da unicidade.
\end{proof}

\subsection{Decomposição Cíclica em Divisores Elementares}

Agora podemos juntar a decomposição primária para minimais e a decomposição cíclica para espaços primários e obter a decomposição cíclica em divisores elementares.

\begin{teorema}[Decomposição Cíclica em Divisores Elementares]
Seja $V$ um espaço vetorial de \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$. Se o polinômio minimal é dado por:
\[
m_T=p_1^{e_1}\dots p_n^{e_n},
\]
em que os $p_i$ são polinômios irredutíveis mônicos distintos sobre $K$, então $V$ pode ser decomposto em uma soma direta:
\[
V=V_1\oplus\dots\oplus V_n,
\]
em que:
\[
V_i=\Ker \ p_i(T)^{e_i}
\]
é um subespaço $T$-invariante tal que $m_{T\upharpoonright_{V_i}}=p_i^{e_i}$. Finalmente, cada subespaço $T$-invariante $V_i$ pode ser escrito como uma soma direta de submódulos cíclicos, de modo que:
\[
V=\left(Z(v_{1,1},T)\oplus\dots\oplus Z(v_{1,k_1},T)\right)\oplus\left(Z(v_{n,1},T)\oplus\dots\oplus Z(v_{n,k_n},T)\right),
\]
em que $m_{T,v_{i,j}}=p_i^{e_{i,j}}$ e os termos de cada decomposição cíclica podem ser arranjados de modo que para cada $i$ tenhamos:
\[
e_i=e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i}.
\]

\end{teorema}

\noindent
Além disso, juntando os teoremas das unicidades da decomposição primária para minimais e da decomposição cíclica para espaços primários, então temos a unicidade da decomposição cíclica em divisores elementares.

\begin{teorema}[Unicidade da Decomposição Cíclica em Divisores Elementares]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que $V$ possa ser escrito como uma soma direta:
\[
V=\left(Z(u_{1,1},T)\oplus\dots\oplus Z(u_{1,k_1},T)\right)\oplus\dots\oplus\left(Z(u_{m,1},T)\oplus\dots\oplus Z(u_{m,k_m},T)\right)
\]
em que $m_{T,u_{i,j}}=p_i^{e_{i,j}}$ e:
\[
e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i},
\]
e suponhamos que $V$ possa ser escrito como uma soma direta:
\[
V=\left(Z(v_{1,1},T)\oplus\dots\oplus Z(v_{1,l_1},T)\right)\oplus\dots\oplus\left(Z(v_{n,1},T)\oplus\dots\oplus Z(v_{n,l_n},T)\right)
\]
em que $m_{T,v_{i,j}}=q_i^{f_{i,j}}$ e:
\[
f_{i,1}\geq f_{i,2}\geq\dots\geq f_{i,l_i}.
\]
Então:
\begin{itemize}
\item O número de somandos é o mesmo em ambas as decomposições; de fato,
$m=n$ e, depois de uma reindexação apropriada, $k_u=l_u$ para todo $u$.
\item Os subespaços primários são os mesmos; isto é, depois de uma reindexação apropriada, para todo $i$ temos $p_i=q_i$ e:
\[
Z(u_{i,1},T)\oplus\dots\oplus Z(u_{i,k_i},T)=Z(v_{i,1},T)\oplus\dots\oplus Z(v_{i,l_i},T).
\]
\end{itemize}
\end{teorema}

\begin{definicao}
O multiconjunto dos polinômios $p_i^{e_{i,j}}$ é unicamente determinado pela decomposição cíclica em divisores elementares, assim ele é chamado o multiconjunto dos \textbf{divisores elementares}.
\end{definicao}

\subsection{Decomposição Cíclica em Fatores Invariantes}

A forma mais usual de se apresentar uma decomposição em subespaços cíclicos é a decomposição cíclica em fatores invariantes.

\begin{teorema}[Decomposição Cíclica em Fatores Invariantes]
Seja $V$ um espaço vetorial de \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$. Então $V$ pode ser escrito como uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_r,T)
\]
em que $m_{T,v_i}=p_i$ e também:
\[
p_r\mid p_{r-1}\mid\dots\mid p_2\mid p_1.
\]
\end{teorema}

\begin{proof}
De acordo com a decomposição cíclica em divisores elementares, $V$ pode ser escrito como soma direta:
\[
V=\left(Z(u_{1,1},T)\oplus\dots\oplus Z(u_{1,k_1},T)\right)\oplus\dots\oplus\left(Z(u_{m,1},T)\oplus\dots\oplus Z(u_{m,k_m},T)\right)
\]
em que $m_{T,u_{i,j}}=p_i^{e_{i,j}}$ e:
\[
e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i},
\]
assim podemos combinar somandos cíclicos com polinômios minimais relativamente primos. Uma maneira de fazer isto é pegar o subespaço cíclico $Z(u_{i,1},T)$ de cada coleção:
\[
Z(u_{i,1},T)\oplus\dots\oplus Z(u_{i,k_i},T)
\]
para obtermos o seguinte:
\[
Z(v_1,T)=Z(u_{1,1},T)\oplus\dots\oplus Z(u_{m,1},T)
\]
e repetir o processo:
\[
Z(v_2,T)=Z(u_{1,2},T)\oplus\dots\oplus Z(u_{m,2},T)
\]
\[
Z(v_3,T)=Z(u_{1,3},T)\oplus\dots\oplus Z(u_{m,3},T)
\]
\[
\vdots
\]
É claro que alguns somandos podem estar faltando aqui pois os diferentes subespaços primários:
\[
Z(u_{i,1},T)\oplus\dots\oplus Z(u_{i,k_i},T)
\]
não necessariamente têm o mesmo número de somandos. De qualquer modo o resultado de rearranjarmos e combinarmos os somandos cíclicos é uma decomposição da forma:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_r,T).
\]
\end{proof}

\noindent
Por causa da unicidade da decomposição cíclica em divisores elementares, então temos a unicidade da decomposição cíclica em fatores invariantes

\begin{teorema}[Unicidade da Decomposição Cíclica em Fatores Invariantes]
Seja $V$ um espaço vetorial e seja $T\in\mathcal{L}(V)$. Suponhamos que $V$ possa ser escrito como uma soma direta:
\[
V=Z(u_1,T)\oplus\dots\oplus Z(u_r,T)
\]
em que $m_{T,u_i}=p_i$ e também:
\[
p_r\mid p_{r-1}\mid\dots\mid p_2\mid p_1,
\]
e que $V$ possa ser escrito como uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_s,T)
\]
em que $m_{T,v_j}=q_j$ e também:
\[
q_s\mid q_{s-1}\mid\dots\mid q_2\mid q_1.
\]
Então $m=n$ e também $p_k=q_k$ para todo $k$.
\end{teorema}

\begin{definicao}
A sequência dos polinômios $p_i$ é unicamente determinado pela decomposição cíclica em fatores invariantes, assim ela é chamada a sequência dos \textbf{fatores invariantes}.
\end{definicao}

\section{Formas Racionais de Matrizes}

\subsection{Matrizes Companheiras}

\begin{definicao}
Definimos a \textbf{matriz companheira} do polinômio mônico:
\[
p(x)=x^n+a_{n-1}x^{n-1}+\dots+a_1x+a_0
\]
como a matriz:
\[
C[p(x)]=\begin{pmatrix}
0&0&\ldots&0&-a_0\\1&0&\ldots&0&-a_1\\0&1&\ddots&&\vdots\\\vdots&\vdots&\ddots&0&-a_{n-2}\\0&0&\ldots&1&-a_{n-1}
\end{pmatrix}
\]
\end{definicao}

\begin{teorema}
Dado polinômio $p(x)\in K[x]$, então temos:
\[
p_{C[p(x)]}(x)=m_{C[p(x)]}=p(x).
\]
Além disso, um espaço vetorial $V$ possui um vetor cíclico para $T\in\mathcal{L}(V)$ se e somente se $T$ pode ser representado por uma matriz companheira, e nesse caso a base representante é $T$-cíclica.
\end{teorema}
\begin{proof}
Veja o Exercício 15 da lista 2.
\end{proof}

\subsection{Forma Racional para Divisores Elementares}

Agora estamos prontos para determinar um conjunto de formas canônicas para similaridade entre matrizes. Faremos isto através das formas racionais. Há pelo menos dois tipos de formas racionais: a forma racional para divisores elementares e a forma racional para fatores invariantes. Primeiro apresentaremos as formas racionais para divisores elementares.

\begin{definicao}
Dizemos que uma matriz $A$ está na \textbf{forma racional para divisores elementares} se e só se $A$ é da forma:
\[
\begin{pmatrix}
C[r_1^{e_1}(x)]&&\\&\ddots&\\&&C[r_n^{e_n}(x)]
\end{pmatrix},
\]
em que os $r_i(x)$ são polinômios mônicos irredutíveis.
\end{definicao}

\begin{teorema}[Forma Racional para Divisores Elementares]
Cada classe de similaridade $S$ de matrizes contém uma matriz $A$ na forma racional para divisores elementares. Além disso, o conjunto das matrizes em $S$ que estão nessa forma
é o conjunto de matrizes obtidas de $A$ através de reordenação das matrizes companheiras no bloco diagonal.
\end{teorema}
\begin{proof}
Seja $V$ um espaço vetorial de \emph{dimensão finita} e seja $T\in\mathcal{L}(V)$. Pela decomposição cíclica em divisores elementares, o espaço $V$ pode ser expressado pela seguinte soma direta:
\[
V=\left(Z(v_{1,1},T)\oplus\dots\oplus Z(v_{1,k_1},T)\right)\oplus\left(Z(v_{n,1},T)\oplus\dots\oplus Z(v_{n,k_n},T)\right),
\]
em que $m_{T,v_{i,j}}=p_i^{e_{i,j}}$ e também:
\[
e_{i,1}\geq e_{i,2}\geq\dots\geq e_{i,k_i}.
\]
A concatenação $B$ das bases ordenadas:
\[
B_{i,j}=\left(v_{i,j},T(v_{i,j}),\dots,T^{d_{i,j}-1}(v_{i,j})\right),
\]
em que $d_{i,j}=\mathrm{deg}(p_i)\cdot e_{i,j}$, é uma base ordenada tal que:
\[
[T]_B=\begin{pmatrix}
C[p_1^{e_{1,1}}(x)]&&&&&&\\&\ddots&&&&&\\&&C[p_1^{e_{1,k_1}}(x)]&&&&\\&&&\ddots&&&\\&&&&C[p_n^{e_{n,1}}(x)]&&\\&&&&&\ddots&\\&&&&&&C[p_n^{e_{n,k_n}}(x)].
\end{pmatrix}
\]
O restante do enunciado pode ser demonstrada tendo em mente a unicidade da decomposição cíclica em divisores elementares.
\end{proof}

\subsection{Forma Racional para Fatores Invariantes}

Agora apresentaremos a forma racional para fatores invariantes, que é a forma racional usual da álgebra linear. Portanto, se alguém pedir que representemos uma matriz na forma racional, muito provavelmente está pedindo a forma racional para fatores invariantes.

\begin{definicao}
Dizemos que uma matriz $A$ está na \textbf{forma racional para fatores invariantes} se e só se $A$ é uma matriz da forma:
\[
\begin{pmatrix}
C[s_1(x)]&&\\&\ddots&\\&&C[s_n(x)]
\end{pmatrix},
\]
em que $s_{k+1}(x)\mid s_k(x)$ para $k=1,\dots,n-1$.
\end{definicao}

\begin{teorema}[Forma Racional para Fatores Invariantes]
Cada classe de similaridade $S$ de matrizes contém uma matriz $A$ na forma racional para fatores invariantes. Além disso, o conjunto das matrizes em $S$ que estão nessa forma
é o conjunto de matrizes obtidas de $A$ através de reordenação das matrizes companheiras no bloco diagonal.
\end{teorema}

\begin{lema}
Se $p(x),q(x)$ são polinômios mônicos primos entre si, então:
\[
C[p(x)q(x)]\sim\begin{pmatrix}
C[p(x)]&\\&C[q(x)]
\end{pmatrix}.
\]
\end{lema}
\begin{proof}[Demonstração do Lema]
Se uma matriz $A$ tem polinômio minimal:
\[
m_T=p_1^{e_1}\dots p_n^{e_n}
\]
de grau igual ao tamanho da matria, então o teorema das formas racionais para divisores elementares implica que os divisores elementares de $A$ são precisamente:
\[
p_1^{e_1},\dots,p_n^{e_n}.
\]
Como as matrizes $C[p(x)q(x)]$ e $C[p(x)]\oplus C[q(x)]$ têm o mesmo tamanho $m$ e o mesmo polinômio minimal $p(x)q(x)$ de grau $m$, segue que eles têm o mesmo multiconjunto dos divisores elementares e assim são similares.
\end{proof}

\begin{proof}[Demonstração do Teorema]
O lema pode ser usado para rearranjar e combinar as matrizes companheiras numa matriz $A$ em forma racional para divisores elementares para produzir uma matriz em forma racional para fatores invariantes que seja similar a $A$. Além disso, é fácil ver que este processo é reversível.
\end{proof}

\noindent
Finalizaremos esta seção mostrando uma aplicação das formas racionais para divisores elementares.

\begin{teorema}
O polinômio característico de uma matriz é igual ao produto de seus divisores elementares.
\end{teorema}
\begin{proof}
Seja $A$ uma matriz, então ela é similar a uma matriz na forma racional para divisores elementares:
\[
\begin{pmatrix}
C[r_1(x)]&&\\&\ddots&\\&&C[r_n(x)]
\end{pmatrix}.
\]
Sabemos que o polinômio característico de $C[r_i(x)]$ é igual a $r_i(x)$, assim o resultado segue.
\end{proof}

\section{Formas de Jordan de Matrizes}

\subsection{Forma de Jordan Usual}

As formas racionais, seja para divisores elementares ou para fatores invariantes, têm a façanha de que todo operador linear num espaço vetorial de dimensão finita tem uma forma racional canônica, seja para divisores elementares ou para fatores invariantes. Porém, as formas racionais podem estar longes do ideal de simplicidade que tínhamos em mente para um conjunto de formas canônicas fáceis de apresentar, assim servem mais como ferramentas teóricas do que práticas.

\medskip
\noindent
Quando $V$ tem dimensão finita e o polinômio minimal $m_T(x)$ de $T$ se decompõe sobre $K$, ou seja:
\[
m_T(x)=(x-c_1)^{r_1}\dots(x-c_k)^{r_k},
\]
então existe uma outra forma canônica que é bem mais fácil de apresentar do que a forma racional.

\medskip
\noindent
De algum modo, a complexidade das formas racionais vem da escolha da base para os subespaços $T$-invariantes cíclicos $Z(v_{i,j},T)$. Recordemos que as bases $T$-cíclicas
têm a forma:
\[
B_{i,j}=(v_{i,j},T(v_{i,j}),\dots,T^{d_{i,j}-1}(v_{i,j})),
\]
em que $d_{i,j}=\mathrm{deg}(p_i)\cdot e_{i,j}$. Com essa base, toda complexidade no final quando tentamos expressar:
\[
T(T^{d_{i,j}-1}(v_{i,j}))=T^{d_{i,j}}(v_{i,j})
\]
como combinação linear dos vetores da base.

\medskip
\noindent
No entanto, como $B_{i,j}$ tem a forma:
\[
(v,T(v),T^2(v),\dots,T^{d-1}(v)),
\]
então qualquer sequência da forma:
\[
(p_0(T)(v),p_1(T)(v),\dots,p_{d-1}(T)(v))
\]
em que $\mathrm{deg}(p_k)=k$ será uma base de $Z(v_{i,j},T)$. Em particular, quando $m_T(x)$ se decompõe sobre $K$, os divisores elementares são:
\[
p_i^{e_{i,j}}(x)=(x-c_i)^{e_{i,j}}
\]
e assim a sequência:
\[
C_{i,j}=(v_{i,j},(T-c_iI)(v_{i,j}),\dots,(T-c_iI)^{e_{i,j}-1}(v_{i,j}))
\]
é também uma base de $Z(v_{i,j},T)$.

\medskip
\noindent
Para $0\leq k<e_{i,j}$ temos:
\[
\begin{array}{rcl}
T((T-c_iI)^k(v_{i,j}))
&=&(T-c_iI+c_iI)((T-c_iI)^k(v_{i,j}))\\
&=&(T-c_iI)^{k+1}(v_{i,j})+c_i(T-c_i)^k(v_{i,j}).
\end{array}
\]
Logo, para esta base, a complexidade se espalha melhor, e a matriz de $T\upharpoonright_{Z(v_{i,j},T)}$ em relação a $C_{i,j}$ é a matriz de tamanho $e_{i,j}$:
\[
\begin{pmatrix}
c_i&0&\ldots&\ldots&0\\1&c_i&\ddots&&\vdots\\0&1&\ddots&\ddots&\vdots\\\vdots&\ddots&\ddots&\ddots&0\\0&\ldots&0&1&c_i
\end{pmatrix}.
\]
Assim apresentaremos o conceito das formas de Jordan.

\begin{definicao}
Um \textbf{bloco de Jordan} é uma matriz do tipo:
\[
J_{c,n}=\begin{pmatrix}
c&0&\ldots&\ldots&0\\1&c&\ddots&&\vdots\\0&1&\ddots&\ddots&\vdots\\\vdots&\ddots&\ddots&\ddots&0\\0&\ldots&0&1&c
\end{pmatrix},
\]
em que $c\in K$ e a matriz tem tamanho $n$. Dizemos que uma matriz está na \textbf{forma de Jordan} se e só se é uma soma direta de blocos de Jordan.
\end{definicao}

\noindent
Com a discussão anterior, então temos as conhecidas formas de Jordan.

\begin{teorema}[Forma de Jordan]
Seja $V$ um espaço vetorial de dimensão finita sobre um corpo $K$ e seja $T\in\mathcal{L}(V)$. Suponhamos que o polinômio minimal de $T\in\mathcal{L}(V)$ se decomponha sobre o corpo $K$, ou seja:
\[
m_T(x)=(x-c_1)^{e_1}\dots(x-c_n)^{e_n},
\]
em que $c_i\in K$. Então existe uma base $B$ tal que a matriz de $T$ seja:
\[
\begin{pmatrix}
J_{c_1,e_{1,1}}&&&&&&\\&\ddots&&&&&\\&&J_{c_1,e_{1,k_1}}&&&&\\&&&\ddots&&&\\&&&&J_{c_n,e_{n,1}}&&\\&&&&&\ddots&\\&&&&&&J_{c_n,e_{n,k_n}}
\end{pmatrix},
\]
em que os polinômios $(x-c_i)^{e_{i,j}}$ são os divisores elementares de $T$.
\end{teorema}

\begin{corolario}
Se $K$ é algebricamente fechado, então, a menos de ordem dos blocos na diagonal, o conjunto de matrizes na forma de Jordan constitui um conjunto de formas canônicas para a similaridade de matrizes.
\end{corolario}

\subsection{Forma de Jordan Generalizada}

Vimos que, para $T$ possuir uma forma canônica de Jordan, o polinômio minimal $m_T(x)$ deve ser um produto de fatores lineares. Agora apresentaremos uma forma canônica de Jordan generalizada para lidarmos com as demais situações. Começaremos com o caso de um espaço vetorial de dimensão finita primário para um operador linear $T$.

\begin{definicao}
Seja $p$ polinômio mônico de grau $d$. Um
\textbf{bloco de Jordan generalizado} é uma matriz do tipo:
\[
\mathfrak{J}_{p,n} =
\begin{pmatrix}
C[p]&0&\ldots&0&0\\N&C[p]&\ldots&0&0\\0&N&\ldots&0&0\\\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&\ldots&C[p]&0\\0&0&\ldots&N&C[p]
\end{pmatrix},
\]
em que:
\[
N=\begin{pmatrix}
0&0&\ldots&0&1\\0&0&\ldots&0&0\\\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&\ldots&0&0\\0&0&\ldots&0&0\\
\end{pmatrix},
\]
que é a matriz valendo $1$ na entrada $(1,d)$ e $0$ nas outras entradas.
Dizemos que $\mathfrak{J}_{p,n}$ é \textbf{irredutível} se e só se $p$ é um polinômio irredutível. Dizemos que uma matriz está na \textbf{forma de Jordan generalizada} se e só se é uma soma direta de blocos de Jordan generalizados \emph{irredutíveis}. Note, em particular, que o bloco de Jordan usual $J_{c,n}$ é a mesma coisa que o bloco de Jordan generalizado irredutível $\mathfrak{J}_{x-c,n}$.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial de dimensão finita sobre um corpo $K$ e seja $T\in\mathcal{L}(V)$, e assuma
que $m_T=p^n$, em que $p$ é um polinômio de grau $d$, e que $T$ tenha um vetor cíclico $v$. Defina $B$ assim:
\[
B = \left(v,\dots,T^{d-1}(v), p(T)(v),...,T^{d-1}p(T)(v),\dots,p(T)^{n-1}(v),\dots,T^{d-1}p(T)^{n-1}(v)\right).
\]
Então $B$ é uma base de $V$ sobre $K$ e $[T]_B=\mathfrak{J}_{p,n}$.
\end{proposicao}
\begin{proof}
Seja:
\[
p(x)=x^d+a_{d-1}x^{d-1}+\dots+a_1x+a_0.
\]
Para $0\leq k<n$ temos:
\[
\begin{array}{rcl}
T(T^{d-1}p(T)^k(v))
&=&T^dp(T)^k(v)\\
&=&\left(p(T)-\left(a_{d-1}T^{d-1}+\dots+a_1T+a_0\right)\right)p(T)^k(v)\\
&=&\left(p(T)\right)^{k+1}(v)-\left(a_{d-1}T^{d-1}p(T)^k(v)+\dots+a_1Tp(T)^k(v)+a_0p(T)^k(v)\right).
\end{array}
\]
Logo é fácil ver que $[T]_B=\mathfrak{J}_{p,n}$.
\end{proof}

\begin{teorema}[Forma de Jordan Generalizada]
Seja $V$ um espaço vetorial de dimensão finita sobre um corpo $K$ e seja $T\in\mathcal{L}(V)$.
Então existe uma base $B$ tal que a matriz de $T$ seja:
\[
\mathfrak{J}=\begin{pmatrix}
\mathfrak{J}_{p_1,e_{1,1}}&&&&&&\\&\ddots&&&&&\\&&\mathfrak{J}_{p_1,e_{1,k_1}}&&&&\\&&&\ddots&&&\\&&&&\mathfrak{J}_{p_n,e_{n,1}}&&\\&&&&&\ddots&\\&&&&&&\mathfrak{J}_{p_n,e_{n,k_n}}
\end{pmatrix},
\]
em que os polinômios $p_i^{e_{i,j}}$ são os divisores elementares de $T$.
\end{teorema}

\subsection{Forma de Jordan Real}

Agora seja $K=\mathbb{R}$ o corpo dos números reais. Produziremos uma outra versão
da forma canônica de Jordan que é válida para todas as matrizes
com entradas em $\mathbb{R}$. Essa forma canônica será de alguma forma diferente da
forma de Jordan generalizada.

\medskip
\noindent
Lembremo-nos de que um operador linear $T\in\mathcal{L}(V)$, em que
$V$ é um espaço vetorial de dimensão finita sobre um corpo $K$, tem uma forma canônica de Jordan se e somente se o polinômio minimal $m_T$ é um produto de fatores lineares, não necessariamente distintas. Essa condição
será satisfeita para todos os polinômios sobre $K$ se e somente se o corpo $K$ for
algebricamente fechado. O corpo $\mathbb{C}$ dos números complexos é algebricamente fechado
(embora não apresentaremos uma prova desse fato nestas notas de aula), mas o corpo
$\mathbb{R}$ dos números reais não é. De fato, $x^2+1$ é um polinômio sobre $\mathbb{R}$ que não possui raízes em $\mathbb{R}$. Portanto, nenhum operador linear $T\in\mathcal{L}_{\mathbb{R}}(\mathbb{R}^2)$
tal que $m_T(x) = x^2 + 1$ pode possuir uma forma canônica de Jordan. Não obstante, há uma variante bem simples da forma canônica de Jordan que é válida sobre $\mathbb{R}$ e dá conta da natureza especial de $\mathbb{R}$ e de sua relação com os números complexos.

\begin{definicao}
Um
\textbf{bloco de Jordan real} é um bloco de Jordan usual ou uma matriz do tipo:
\[
J_{a+bi,n} =
\begin{pmatrix}
C_{a+bi}&0&\ldots&0&0\\I&C_{a+bi}&\ldots&0&0\\0&I&\ldots&0&0\\\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&\ldots&C_{a+bi}&0\\0&0&\ldots&I&C_{a+bi}
\end{pmatrix},
\]
em que $a$ e $b$ são reais com $b\neq 0$ e:
\[
C_{a+bi}=\begin{pmatrix}
a&-b\\b&a
\end{pmatrix}.
\]
Dizemos que uma matriz está na \textbf{forma de Jordan real} se e só se é uma soma direta de blocos de Jordan reais.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço vetorial de dimensão finita sobre $\mathbb{R}$ e seja $T\in\mathcal{L}(V)$, e assuma
que:
\[
m_T(x)=\left((x-a)^2+b^2\right)^n,
\]
em que $a$ e $b$ são reais e $b\neq 0$, e que $T$ tenha um vetor cíclico $v$. Então existe uma base $B$ de $V$ sobre $\mathbb{R}$ tal que $[T]_B=J_{a+bi,n}$.
\end{proposicao}

\begin{teorema}[Forma de Jordan Real]
Seja $V$ um espaço vetorial de dimensão finita sobre $\mathbb{R}$ e seja $T\in\mathcal{L}(V)$.
Então existe uma base $B$ tal que a matriz de $T$ seja uma soma direta de blocos de Jordan reais.
\end{teorema}

\section{Espaços Indecomponíveis}

\subsection{Espaços Cíclicos}

A decomposição cíclica em divisores elementares pode ser usada para caracterizar espaços vetoriais de dimensão finita com vetores cíclicos através de seus divisores elementares.

\begin{teorema}[Caracterização de Espaços Cíclicos]
Seja $V$ um espaço vetorial de dimensão finita e seja $T\in\mathcal{L}(V)$. Consideremos o polinômio minimal:
\[
m_T=p_1^{e_1}\dots p_n^{e_n}.
\]
As seguintes propriedades são equivalentes:
\begin{itemize}
\item[1)] $V$ possui um vetor cíclico para $T$.
\item[2)] $V$ é uma soma direta:
\[
V=Z(v_1,T)\oplus\dots\oplus Z(v_n,T)
\]
de subespaços cíclicos primários $V_i=Z(v_i,T)$ tais que $m_{T\upharpoonright{V_i}}=p_i^{e_i}$.
\item[3)] Os divisores elementares de $V$ são precisamente:
\[
p_1^{e_1},\dots,p_n^{e_n}.
\]
\end{itemize}
\end{teorema}
\begin{proof}
Suponhamos que $V$ tenha vetor cíclico para $T$. Então a decomposição primária para minimais de $V$ é uma decomposição cíclica, pois pelo Exercício 1 da Lista 4 todo subespaço $T$-invariante de um espaço com vetor cíclico também tem um vetor cíclico. Logo, (1) implica (2). Reciprocamente, se (2) ocorre, então como os polinômios minimais são relativamente primos, o Teorema \ref{ciclico} implica que $V$ tem um vetor cíclico. Deixamos o restante da prova para o leitor.
\end{proof}

\subsection{Espaços Indecomponíveis}

A decomposição cíclica em divisores elementares é uma decomposição de $V$ numa soma dureta de subespaços $T$-invariantes que não podem ser decompostos em pedaços menores. De fato, isto caracteriza a decomposição cíclica em divisores elementares de $V$. Antes de justificar estas afirmações, apresentamos a seguinte definição.

\begin{definicao}
Um espaço vetorial $V$ é dito \textbf{indecomponível} para um $T\in\mathcal{L}(V)$ se e só se não puder ser escrito como uma soma direta de dois subespaços $T$-invariantes próprios.
\end{definicao}

\begin{teorema}[Caracterização de Espaços Indecomponíveis]
Seja $V$ um espaço vetorial de dimensão finita e seja $T\in\mathcal{L}(V)$. Então as seguinte propriedades são equivalentes:
\begin{itemize}
\item[1)] $V$ é indecomponível para $T$.
\item[2)] $V$ é um espaço primário e tem um vetor cíclico para $T$.
\item[3)] $V$ tem apenas um divisor elementar para $T$.
\end{itemize}
\end{teorema}

\noindent
Portanto, a decomposição cíclica em divisores elementares é uma decomposição de $V$ em uma soma direta de subespaços $T$-invariantes indecomponíveis para $T$. Reciprocamente, se:
\[
V=V_1\oplus\dots\oplus V_n
\]
é uma decomposição de $V$ numa soma direta de subespaços $T$-invariantes indecomponíveis para $T$, então cada subespaço $T$-invariante $V_i$ é primário e possui vetor cíclico, aí esta decomposição é a decomposição cíclica em divisores elementares de $V$.

\subsection{Teorema de Cauchy}

Quem já está familiarizado com teoria dos grupos sabem que todo grupo de ordem prima é cíclico. Entretanto, existem espaços com polinômio minimal irredutível que no entanto não possuem vetores cíclicos. Não obstante, espaços com polinômio minimal irredutível com vetores cíclicos são importantes.

\medskip
\noindent
De fato, se $V$ é um espaço vetorial de dimensão finita e $T\in\mathcal{L}(V)$, com polinômio minimal $m_T$, então cada fator primo de $p$ de $m_T$ nos oferece um subespaço $T$-invariante $W$ de $V$ com vetor cíclico cujo polinômio minimal é $p$ e aí $W$ é também indecomponível para $T$.
Infelizmente, $W$ não necessariamente tem um complemento $T$-invariante e aí não podemos usá-lo
para decompor $V$. Não obstante, o teorema ainda é útil.

\begin{teorema}[Teorema de ``Cauchy'' para Álgebra Linear]
Seja $V$ um espaço vetorial de dimensão finita e $T\in\mathcal{L}(V)$, com polinômio minimal $m_T$. Se $p$ é um divisor primo de $m_T$, então $V$ tem um subespaço $T$-invariante $W$ com vetor cíclico com $T$-anulador $p$.
\end{teorema}
\begin{proof}
Se $m_T=pq$, então existe um $v\in V$ tal que $w\coloneqq q(T)(v)\neq 0$ mas $p(T)(w)=0$.
Assim $W=Z(w,T)$ é anulado por $p(T)$ e assim $m_{T,w}\mid p$. Mas $p$ é primo e
$m_{T,w}\neq 1$, aí $m_{T,w}=p$.
\end{proof}

\chapter{Espaços com Produto Interno}

Neste capítulo, convencionaremos que $K$ é $\mathbb{R}$ ou $\mathbb{C}$.

\section{Definições e Exemplos}

\subsection{Definições}

\begin{definicao}
Para espaço vetorial $V$, um \textbf{produto interno} é uma função:
\[
\begin{array}{rcl}
V\times V&\rightarrow&K\\
(u,v)&\mapsto&\langle u,v\rangle
\end{array}
\]
que satisfaz as seguintes propriedades:
\begin{itemize}
\item $\langle\alpha u+\beta v,w\rangle=\alpha\langle u,w\rangle+\beta\langle v,w\rangle$ para quaisquer $\alpha,\beta\in K$ e $u,v,w\in V$.
\item $\overline{\langle u,v\rangle}=\langle v,u\rangle$ para quaisquer $u,v\in V$.
\item $\langle v,v\rangle\geq 0$ para $v\in V$.
\item $\langle v,v\rangle=0\Rightarrow v=0$ para $v\in V$.
\end{itemize}
\end{definicao}

\begin{definicao}
Um \textbf{espaço com produto interno} é um espaço vetorial munido de um produto interno.
\end{definicao}

\begin{definicao}
Definimos o seguinte:
\begin{itemize}
\item Um \textbf{espaço euclidiano} é um espaço com produto interno sobre $\mathbb{R}$ de dimensão finita.
\item Um \textbf{espaço unitário} é um espaço com produto interno sobre $\mathbb{C}$ de dimensão finita.
\end{itemize}
\end{definicao}

\subsection{Exemplos}

\begin{exemplo}
O exemplo principal é $V=K^n$, em que, para quaisquer:
\[
u=(\alpha_1,\dots,\alpha_n),\quad\quad v=(\beta_1,\dots,\beta_n),
\]
definimos:
\[
\langle u,v\rangle_{\text{usual}}=\alpha_1\overline{\beta_1}+\dots+\alpha_n\overline{\beta_n}.
\]
Este é o \textbf{produto interno usual}.
\end{exemplo}

\begin{exemplo}
Podemos considerar $V=\m{n}{K}\cong K^{n^2}$. Com base no exemplo anterior, para quaisquer matrizes:
\[
A=(a_{i,j}),\quad\quad B=(b_{i,j}),
\]
definimos:
\[
\langle A,B\rangle=\sum_{i,j}a_{i,j}\overline{b_{i,j}}.
\]
\end{exemplo}

\begin{exemplo}
Consideremos $V=\mathcal{C}[a,b]$, o conjunto das funções contínuas de $[a,b]$ em $\mathbb{C}$, e para $f,g\in V$ definamos:
\[
\langle f,g\rangle=\int_a^bf(t)\overline{g(t)}\dif t.
\]
\end{exemplo}

\begin{exemplo}
Seja $T\in\mathcal{L}(U,V)$ bijetora. Seja $\langle\cdot,\cdot\rangle_V$ um produto interno em $V$. Para $u,v\in U$, definimos:
\[
\langle u,v\rangle_U=\langle T(u),T(v)\rangle_V.
\]
\end{exemplo}

\begin{exemplo}
Seja $U$ um espaço de dimensão finita e seja $B=(e_1,\dots,e_n)$ uma base de $U$. Então existe uma única transformação linear $T:U\rightarrow K^n$ tal que:
\[
T(e_i)=(0,\dots,1,\dots,0),
\]
em que a entrada $i$ vale $1$ e as outras valem $0$. Defininamos:
\[
\langle u,v\rangle_U=\langle T(u),T(v)\rangle_{\text{usual}}.
\]
Para quaisquer vetores em $U$:
\[
u=\alpha_1e_1+\dots+\alpha_ne_n,\quad v=\beta_1e_1+\dots+\beta_ne_n,
\]
então temos:
\[
\langle u,v\rangle_U=\langle T(u),T(v)\rangle_{\text{usual}}=\sum_{i=1}^n\alpha_i\overline{\beta_i}.
\]
\end{exemplo}

\subsection{Propriedades Iniciais}

\begin{proposicao}
Para espaço vetorial $V$ e produto interno $\langle\cdot,\cdot\rangle$, então temos:
\[
\langle u,\alpha v+\beta w\rangle=\overline{\alpha}\langle u,v\rangle+\overline{\beta}\langle u,w\rangle
\]
para quaisquer $\alpha,\beta\in K$ e $u,v,w\in V$.
\end{proposicao}
\begin{proof}
Temos o seguinte:
\[
\begin{array}{rcl}
\langle u,\alpha v+\beta w\rangle&=&\overline{\langle \alpha v+\beta w,u\rangle}\\
&=&\overline{\alpha\langle v,u\rangle+\beta\langle w,u\rangle}\\
&=&\overline{\alpha}\overline{\langle v,u\rangle}+\overline{\beta}\overline{\langle w,u\rangle}\\
&=&\overline{\alpha}\langle u,v\rangle+\overline{\beta}\langle u,w\rangle.
\end{array}
\]
\end{proof}

\newpage

\section{Geometria de Vetores}

\subsection{Norma de Vetores}

\begin{definicao}
Para vetor $x$, definimos a \textbf{norma} de $x$ como:
\[
\norm{x}=\sqrt{\langle x,x\rangle}.
\]
\end{definicao}

\begin{proposicao}\label{norma1}
A função $\norm{\cdot}$ satisfaz as seguintes propriedades:
\begin{itemize}
\item $\norm{v}\geq 0$ para $v\in V$. 
\item $\norm{v}=0\Rightarrow v=0$ para $v\in V$.
\item $\norm{\alpha v}=\abs{\alpha}\norm{v}$ para $\alpha\in K$ e $v\in V$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Para $v$ temos $\langle v,v\rangle\geq 0$, aí $\sqrt{\langle v,v\rangle}\geq 0$, aí $\norm{v}\geq 0.$
\item Para $v$, se $\norm{v}=0$, então $\sqrt{\langle v,v\rangle}=0$, aí $\langle v,v\rangle=0$, aí $v=0.$
\item Para $v\in V$ e $\alpha\in K$ temos:
\[
\norm{\alpha v}^2=\langle\alpha v,\alpha v\rangle
=\alpha\overline{\alpha}\langle v,v\rangle
=\abs{\alpha}^2\norm{v}^2
=\left(\abs{\alpha}\norm{v}\right)^2,
\]
mas $\forall x\in V:\norm{x}\geq 0$ e também $\forall\gamma\in K:\abs{\gamma}\geq 0$,
assim $\norm{\alpha v}=\abs{\alpha}\norm{v}.$
\end{itemize}
\end{proof}

\subsection{Teorema de Pitágoras}

\begin{definicao}
Dois vetores $u,v\in V$ são ditos \textbf{ortogonais} se e só se $\langle u,v\rangle=0$.
\end{definicao}

\begin{proposicao}[Teorema de Pitágoras]
Para $u,v\in V$, se $u$ e $v$ são ortogonais, então:
\[
\norm{u+v}^2=\norm{u}^2+\norm{v}^2.
\]
\end{proposicao}
\begin{proof}
Temos o seguinte:
\[
\begin{array}{rcl}
\norm{u+v}^2&=&\langle u+v,u+v\rangle\\
&=&\langle u,u\rangle+\langle u,v\rangle+\langle v,u\rangle+\langle v,v\rangle\\
&=&\norm{u}^2+\langle u,v\rangle+\overline{\langle u,v\rangle}+\norm{v}^2\\
&=&\norm{u}^2+\norm{v}^2.
\end{array}
\]
\end{proof}

\begin{definicao}
Para vetores $u$ e $v$ tais que $v\neq 0$, definimos a \textbf{projeção ortogonal} de $u$ sobre $v$ como o vetor:
\[
w=\frac{\langle u,v\rangle}{\norm{v}^2}v.
\]
\end{definicao}

\begin{proposicao}
Se $u$ e $v$ são vetores tais que $v\neq 0$, sendo $w$ a projeção ortogonal de $u$ sobre $v$ então $u-w$ e $v$ são ortogonais.
\end{proposicao}
\begin{proof}
Temos o seguinte:
\[
\begin{array}{rcl}
\langle w,v\rangle&=&\left\langle \frac{\langle u,v\rangle}{\norm{v}^2}v,v\right\rangle\\
&=&\frac{\langle u,v\rangle}{\norm{v}^2}\left\langle v,v\right\rangle\\
&=&\frac{\langle u,v\rangle}{\norm{v}^2}\norm{v}^2\\
&=&\langle u,v\rangle,
\end{array}
\]
assim:
\[
\langle u-w,v\rangle=\langle u,v\rangle-\langle w,v\rangle=0.
\]
\end{proof}

\begin{proposicao}[Desigualdade de Cauchy-Bunyakovski-Schwarz]
Seja $V$ um espaço com produto interno. Então para quaisquer $u,v\in V$ temos:
\[
\abs{\langle u,v\rangle}\leq\norm{u}\cdot\norm{v}.
\]
\end{proposicao}
\begin{proof}
Suponhamos sem perda de generalidade que $v\neq 0.$ Seja $w$ a projeção ortogonal de $u$ sobre $v.$ Então $u-w$ e $v$ são ortogonais, mas $w$ é múltiplo de $v,$ aí $u-w$ e $w$ são ortogonais, aí, pelo teorema de Pitágoras, temos:
\[
\norm{u-w}^2+\norm{w}^2=\norm{(u-w)+w}^2=\norm{u}^2,
\]
assim temos $\norm{w}^2\leq\norm{u}^2$, aí $\norm{w}\leq\norm{u}$, mas:
\[
\norm{w}=\norm{\frac{\langle u,v\rangle}{\norm{v}^2}v}=\frac{\abs{\langle u,v\rangle}}{\norm{v}^2}\norm{v}=\frac{\abs{\langle u,v\rangle}}{\norm{v}},
\]
aí temos:
\[
\frac{\abs{\langle u,v\rangle}}{\norm{v}}\leq\norm{u},
\]
aí concluímos que:
\[
\abs{\langle u,v\rangle}\leq\norm{u}\cdot\norm{v}.
\]
\end{proof}

\begin{proposicao}[Desigualdade Triangular]\label{norma2}
Para vetores $u$ e $v$, então:
\[
\norm{u+v}\leq\norm{u}+\norm{v}.
\]
\end{proposicao}
\begin{proof}
Sabemos que para $z\in\mathbb{C}$ então $z+\overline{z}\leq 2\abs{z}$. De fato, sendo $z=a+bi$ com $a,b\in\mathbb{R}$, então $\abs{z}=\sqrt{a^2+b^2}\geq a$, aí $z+\overline{z}=2a\leq 2\abs{z}$. Agora, para $u,v\in V$ temos:
\[
\begin{array}{rcl}
\norm{u+v}^2&=&\langle u+v,u+v\rangle\\
&=&\norm{u}^2+\norm{v}^2+\left(\langle u,v\rangle+\overline{\langle u,v\rangle}\right)\\
&\leq&\norm{u}^2+\norm{v}^2+2\abs{\langle u,v\rangle}\\
&\leq&\norm{u}^2+\norm{v}^2+2\norm{u}\cdot\norm{v}\\
&=&\left(\norm{u}+\norm{v}\right)^2,
\end{array}
\]
portanto $\norm{u+v}\leq\norm{u}+\norm{v}.$
\end{proof}

\subsection{Lei dos Cossenos}

Com a desigualdade de Cauchy-Bunyakovski-Schwarz, podemos definir o seguinte:

\begin{definicao}
Para vetores não nulos $u$ e $v$, definimos o \textbf{ângulo} entre $u$ e $v$ como o único $\theta$ tal que $0\leq\theta\leq\pi$ tal que:
\[
\cos(\theta)=\frac{\mbox{Re}{\langle u,v\rangle}}{\norm{u}\cdot\norm{v}}.
\]
\end{definicao}

\begin{proposicao}[Lei dos Cossenos]
Para vetores não nulos $u$ e $v$, sendo $\theta$ o ângulo entre $u$ e $v$, então:
\[
\norm{u-v}^2=\norm{u}^2+\norm{v}^2-2\norm{u}\norm{v}\cos(\theta).
\]
\end{proposicao}

\subsection{Espaços Normados}

\begin{definicao}
Para espaço vetorial $V$, uma \textbf{norma} é uma função:
\[
\begin{array}{rcl}
V&\rightarrow&\mathbb{R}\\
v&\mapsto&\norm{v}
\end{array}
\]
que satisfaça as seguintes propriedades:
\begin{itemize}
\item $\norm{v}\geq 0$ para $v\in V$. 
\item $\norm{v}=0\Rightarrow v=0$ para $v\in V$.
\item $\norm{\alpha v}=\abs{\alpha}\norm{v}$ para $\alpha\in K$ e $v\in V$.
\item $\norm{u+v}\leq\norm{u}+\norm{v}$ para $u,v\in V$.
\end{itemize}
\end{definicao}

\begin{definicao}
Um \textbf{espaço normado} é um espaço vetorial munido de uma norma.
\end{definicao}

\begin{proposicao}
Seja $V$ um espaço com produto interno. Então $\norm{\cdot}$ é uma norma.
\end{proposicao}
\begin{proof}
Segue das Proposições \ref{norma1} e \ref{norma2}.
\end{proof}

\begin{proposicao}
Seja $V$ um espaço vetorial, e seja $\norm{\cdot}\colon V\rightarrow\mathbb{R}$ uma função satisfazendo as seguintes propriedades:
\begin{enumerate}
    \item $\norm{ax+by}\leq\abs{a}\norm{x}+\abs{b}\norm{y}$.
    \item Se $\norm{x}=0$ então $x=0$.
\end{enumerate}
Então $\norm{\cdot}$ é uma norma.
\end{proposicao}
\begin{proof}[Demonstração]
Vamos dividir em etapas:
\begin{enumerate}
    \item Mostraremos que $\norm{x+y}\leq\norm{x}+\norm{y}$. De fato temos $\norm{1x+1y}\leq\abs{1}\norm{x}+\abs{1}\norm{y}$, aí $\norm{x+y}\leq\norm{x}+\norm{y}$.
    \item Mostraremos que $\norm{0}=0$. Temos $\norm{0\cdot0+0\cdot0}\leq\abs{0}\norm{0}+\abs{0}\norm{0}$, aí $\norm{0}\leq 0$, além disso, por (1) temos $\norm{0+0}\leq\norm{0}+\norm{0}$, aí $\norm{0}\leq2\norm{0}$, aí $0\leq\norm{0}$, assim $\norm{0}=0$.
    \item Mostraremos que $\norm{x}\geq 0$. Por (1) temos $\norm{1x+(-1)x}\leq\abs{1}\norm{x}+\abs{-1}\norm{x}$, aí $\norm{0}\leq 2\norm{x}$, aí por (2) temos $2\norm{x}\geq 0$, aí $\norm{x}\geq 0$.
    \item Mostraremos que se $k\neq 0$ então $\norm{kx}=\abs{k}\norm{x}$. Para $k$ e $x$ então $\norm{kx+0\cdot0}\leq\abs{k}\norm{x}+\abs{0}\norm{0}$, aí $\norm{kx}\leq\abs{k}\norm{x}$. Logo, para $k$ e $x$, se $k\neq 0$ então $\norm{\frac{1}{k}\cdot kx}\leq\abs{\frac{1}{k}}\norm{kx}$, aí $\norm{x}\leq\frac{1}{\abs{k}}\norm{kx}$, aí $\abs{k}\norm{x}\leq\norm{kx}$, mas também $\norm{kx}\leq\abs{k}\norm{x}$, aí $\norm{kx}=\abs{k}\norm{x}$.
\end{enumerate}
\end{proof}

\begin{definicao}
Um \textbf{espaço de Hilbert} é um espaço com produto interno $V$ que é completo em relação à norma induzida.
\end{definicao}

\subsection{Bases Ortonormais}

\begin{definicao}
Seja $(v_i)_{i\in I}$ família de vetores. Dizemos que:
\begin{itemize}
\item A família é \textbf{ortogonal} se e só se para quaisquer $i,j\in I$ tais que $i\neq j$ os vetores $v_i$ e $v_j$ forem ortogonais.
\item A família é \textbf{ortonormal} se e só se é ortogonal e para todo $i\in I$ temos $\norm{v_i}=1$.
\end{itemize}
\end{definicao}

\begin{proposicao}
Se $(v_i)_{i\in I}$ é uma família ortogonal de vetores \emph{não nulos}, então a família é L.I.
\end{proposicao}
\begin{proof}
Para conjunto finito $J\subseteq I$ e para $\alpha:J\rightarrow I$, se:
\[
\sum_{i\in J}\alpha_iv_i=0,
\]
então para $j\in J$ temos:
\[
0=\left\langle\sum_{i\in J}\alpha_iv_i,v_j\right\rangle=\sum_{i\in J}\alpha_i\langle v_i,v_j\rangle=\alpha_j\norm{v_j}^2,
\]
mas $v_j\neq 0$, aí $\norm{v_j}^2\neq 0$, aí $\alpha_j=0$.
\end{proof}

\begin{teorema}[Ortogonalização de Gram-Schmidt]
Para toda sequência linearmente independente $(u_1,\dots,u_k)$ de vetores, existe uma sequência ortogonal $(v_1,\dots,v_k)$ tal que $\langle u_1,\dots,u_k\rangle=\langle v_1,\dots,v_k\rangle$.
\end{teorema}
\begin{proof}
Indução sobre $k$. O caso $k=1$ é fácil. Agora suponhamos que $k>1$ e o teorema seja válido para $k-1.$ Seja $(u_1,\dots,u_k)$ uma sequência linearmente independente. Então existe uma sequência ortogonal $(v_1,\dots,v_{k-1})$ tal que $\langle u_1,\dots,u_{k-1}\rangle=\langle v_1,\dots,v_{k-1}\rangle.$ Agora seja:
\[
\tilde{u}_k=\frac{\langle u_k,v_1\rangle}{\norm{v_1}^2}v_1+\dots+\frac{\langle u_k,v_{k-1}\rangle}{\norm{v_{k-1}}^2}v_{k-1},
\]
a soma das projeções de $u_k$ sobre os vetores $v_1,\dots,v_{k-1},$ e seja $v_k=u_k-\tilde{u}_k.$ Então, para $j=1,\dots,k-1,$ temos:
\[
\langle\tilde{u}_k,v_j\rangle=\left\langle\sum_{i=1}^{k-1}\frac{\langle u_k,v_i\rangle}{\norm{v_i}^2}v_i,v_j\right\rangle=\sum_{i=1}^{k-1}\frac{\langle u_k,v_i\rangle}{\norm{v_i}^2}\langle v_i,v_j\rangle=\frac{\langle u_k,v_j\rangle}{\norm{v_j}^2}\langle v_j,v_j\rangle=\langle u_k,v_j\rangle,
\]
assim:
\[
\langle v_k,v_i\rangle=\langle u_k,v_i\rangle-\langle \tilde{u}_k,v_i\rangle=0;
\]
portanto $(v_1,\dots,v_{k-1},v_k)$ é ortogonal. Agora, $\tilde{u}_k$ é gerado pelos vetores $v_1,\dots,v_{k-1},$ de modo que $u_k=\tilde{u}_k+v_k$ é gerado pelos vetores $v_1,\dots,v_{k-1},v_k.$ Além disso, como $\langle u_1,\dots,u_{k-1}\rangle=\langle v_1,\dots,v_{k-1}\rangle,$ então $\tilde{u}_k$ é gerado pelos vetores $u_1,\dots,u_{k-1},$ de modo que $v_k=u_k-\tilde{u}_k$ é gerado pelos vetores $u_1,\dots,u_{k-1},u_k.$ Portanto temos $\langle u_1,\dots,u_{k-1},u_k\rangle=\langle v_1,\dots,v_{k-1},v_k\rangle.$ 
\end{proof}

\begin{corolario}
Todo espaço com produto interno de dimensão finita tem uma base ortonormal.
\end{corolario}
\begin{proof}
Basta pegar uma base $(v_1,\dots,v_n),$ aplicar a ortogonalização de Gram-Schmidt, e para cada $i$ pegar $e_i=\frac{v_i}{\norm{v_i}}.$
\end{proof}

\begin{proposicao}\label{matriz}
Sejam $U$ e $V$ espaços com produto interno de dimensão finita e seja $T\in\mathcal{L}(U,V).$ Se $B=(e_1,\dots,e_n)$ e $C=(f_1,\dots,f_n)$ são bases \emph{ortonormais} de $U$ e $V,$ então:
\[
[T]_{B,C}=\begin{pmatrix}
\langle T(e_1),f_1\rangle&&\langle T(e_n),f_1\rangle\\&\ddots&\\\langle T(e_1),f_n\rangle&&\langle T(e_n),f_n\rangle
\end{pmatrix}.
\]
\end{proposicao}
\begin{proof}
Seja:
\[
[T]_{B,C}=\begin{pmatrix}
a_{1,1}&&a_{1,n}\\&\ddots&\\a_{n,1}&&a_{n,n}
\end{pmatrix}.
\]
Para todo $j$ então:
\[
T(e_j)=\sum_{k=1}^na_{k,j}f_k,
\]
assim para $i$ temos:
\[
\langle T(e_j),f_i\rangle=\left\langle \sum_{k=1}^na_{k,j}f_k,f_i\right\rangle=\sum_{k=1}^na_{k,j}\langle f_k,f_i\rangle=a_{i,j}\langle f_i,f_i\rangle=a_{i,j}.
\]
\end{proof}

\subsection{Complemento Ortogonal}

\begin{definicao}
Agora seja $V$ um espaço com produto interno e $S$ um subconjunto de $V$. Definimos o conjunto:
\[
S^\perp=\{v\in V\mid \forall s\in S :\langle v,s\rangle=0\},
\]
e chamamos de \textbf{complemento ortogonal} de $S$.
\end{definicao}

\begin{proposicao}
Para qualquer subconjunto $S$ então:
\begin{itemize}
\item[1)] $S^\perp$ é um subespaço de $V.$
\item[2)] $S^\perp=\langle S\rangle^\perp.$
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Para $s\in S$ temos $\langle 0,s\rangle=0$; aí $0\in S^\perp$. Para $a,b\in K$ e $x,y\in S^\perp$, então para $s\in S$ temos $\langle x,s\rangle=0$ e $\langle y,s\rangle=0,$ assim:
\[
\langle ax+by,s\rangle=a\langle x,s\rangle+b\langle y,s\rangle=0;
\]
logo $ax+by\in S^\perp.$
\item[2)] É fácil ver que $\langle S\rangle^\perp\subseteq S^\perp.$ Agora, para $x\in S^\perp$, então para $t\in \langle S\rangle$, existe um $k\in\mathbb{N}$ e existem $a_1,\dots,a_k\in K$ e $s_1,\dots,s_k\in S$ tais que:
\[
x=\sum_{i=1}^ka_is_i,
\]
assim:
\[
\langle x,t\rangle=\left\langle x,\sum_{i=1}^ka_is_i\right\rangle=\sum_{i=1}^k\overline{a_i}\left\langle x,s_i\right\rangle=0;
\]
logo $x\in\langle S\rangle^\perp.$
\end{itemize}
\end{proof}

\begin{proposicao}\label{proj1}
Para subespaço $W$ de $V$, se $\dim(W)<\infty$, então:
\begin{itemize}
\item[1)] $V=W\oplus W^\perp$.
\item[2)] $W^{\perp\perp}=W$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[1)] Escolhemos em $W$ uma base ortonormal $(v_1,\dots,v_k)$. Seja $u\in V$ arbitrário. Consideremos:
\[
\tilde{u}=\langle u,v_1\rangle v_1+\dots+\langle u,v_k\rangle v_k\in W.
\]
Provemos que $u-\tilde{u}\in W^\perp$. Notemos que $W^\perp=\{v_1,\dots,v_k\}^\perp$. Para $i=1,\dots,k$, então temos:
\[
\langle \tilde{u},v_i\rangle=\left\langle\sum_{j=1}^k\langle u,v_j\rangle v_j,v_i\right\rangle
=\sum_{j=1}^k\langle u,v_j\rangle\langle v_j,v_i\rangle
=\langle u,v_i\rangle\langle v_i,v_i\rangle
=\langle u,v_i\rangle,
\]
aí temos:
\[
\langle u-\tilde{u},v_i\rangle=\langle u,v_i\rangle-\langle \tilde{u},v_i\rangle=0;
\]
logo $u-\tilde{u}\in W^\perp$, aí $u=\tilde{u}+(u-\tilde{u})\in W+W^\perp$. Para $v\in W\cap W^\perp$, então $\langle u,u\rangle=0$, aí $u=0$.
\item[2)] É fácil ver que $W\subseteq W^{\perp\perp}$. Além disso, para $x\in W^{\perp\perp}$, então existem $w\in W$ e $w^\perp\in W^\perp$ tais que $x=w+w^\perp$, aí $w^\perp=x-w\in W^\perp\cap W^{\perp\perp}=0$, aí $w^\perp=0$, aí $x=w\in W;$ logo $W=W^{\perp\perp}$.
\end{itemize}
\end{proof}

\begin{definicao}
Para subespaço $W$ de $V$ tal que $\dim(W)<\infty$, definimos a \textbf{projeção ortogonal} como a função $E_W:V\rightarrow W$ tal que, para $v\in V$ tenhamos $v-E_W(v)\in W^\perp$.
\end{definicao}

\begin{proposicao}\label{proj2}
Para subespaço $W$ tal que $\dim(W)<\infty$ e para $v\in V$, temos:
\[
\norm{v-E_W(v)}=\min\left\{\norm{v-w}\mid w\in W\right\}.
\]
\end{proposicao}
\begin{proof}
Seja $v^\perp=v-E_W(v)$. Então para $w\in W$ temos:
\[
v-w=v^\perp+E_W(v)-w,
\]
mas $v^\perp\in W^\perp$ e $E_W(v)-w\in W$, aí:
\[
\norm{v-w}^2=\norm{v^\perp}^2+\norm{E_W(v)-w}^2\geq\norm{v-E_W(v)}^2.
\]
\end{proof}

\begin{teorema}[Desigualdade de Bessel]
Sejam $v_1,\dots,v_k\in V$ vetores não nulos mutuamente ortogonais. Então para todo $v\in V$ temos:
\[
\sum_{i=1}^k\frac{\abs{\langle v,v_i\rangle}^2}{\norm{v_i}^2}\leq\norm{v}^2.
\]
\end{teorema}
\begin{proof}
Seja $e_i=\frac{v_i}{\norm{v_i}}.$ Então $(e_1,\dots,e_k)$ é ortonormal, aí pelo teorema de Pitágoras temos:
\[
0\leq\norm{v-\sum_{i=1}^k\langle v,e_i\rangle e_i}^2=\norm{v}^2-2\sum_{i=1}^k\abs{\langle v,e_i\rangle}^2+\sum_{i=1}^k\abs{\langle v,e_i\rangle}^2=\norm{v}^2-\sum_{i=1}^k\abs{\langle v,e_i\rangle}^2,
\]
aí temos:
\[
\norm{v}^2\geq\sum_{i=1}^k\abs{\langle v,e_i\rangle}^2=\sum_{i=1}^k\frac{\abs{\langle v,v_i\rangle}^2}{\norm{v_i}^2}.
\]
\end{proof}

\begin{observacao}
Pela demonstração, podemos ver que a igualdade ocorre se e só se o vetor $v$ é combinação linear de $v_1,\dots,v_k.$
\end{observacao}

\newpage

\section{Transformação Adjunta}

Antes de começarmos os estudos, apresentaremos uma proposição útil:

\begin{proposicao}\label{util}
Seja $u,v\in V$ vetores tais que para todo $x\in V$ tenhamos $\langle x,u\rangle=\langle x,v\rangle.$ Então $u=v.$
\end{proposicao}
\begin{proof}
Fazendo $x=u-v$, então:
\[
\langle u-v,u-v\rangle=\langle u-v,u\rangle-\langle u-v,v\rangle=0,
\]
aí $u-v=0$, aí $u=v.$
\end{proof}

\subsection{Funcionais Lineares}

\begin{proposicao}
Seja $V$ um espaço com produto interno. Para $u\in V$, definimos $\varphi_u:V\rightarrow K$ assim:
\[
\varphi_u(v)=\langle v,u\rangle.
\]
Então $\varphi_u$ é um funcional linear.
\end{proposicao}
\begin{proof}
Pela definição de produto interno, então temos:
\[
\varphi_v(ax+by)=\langle ax+by,v\rangle=a\langle x,v\rangle+b\langle y,v\rangle=a\varphi_v(x)+b\varphi_v(y).
\]
\end{proof}

\begin{teorema}[Teorema de Riesz]
Se $\dim(V)<\infty$, então para todo $f\in V^*$ existe um único $u\in V$ tal que $f=\varphi_u$.
\end{teorema}
\begin{proof}
Seja $f\in V^*$. Escolhemos uma base ortonormal $B=(e_1,\dots,e_n)$ em $V$ e seja $\alpha_i=f(e_i)\in K$ para $i=1,\dots,n$. Consideremos $u=\overline{\alpha_1}e_1+\dots+\overline{\alpha_n}e_n.$ Então para $k=1,\dots,n$ temos:
\[
\begin{array}{rcl}
\varphi_u(e_k)=\langle e_k,u\rangle&=&\langle e_k,\overline{\alpha_1}e_1+\dots+\overline{\alpha_n}e_n\rangle\\
&=&\alpha_k\langle e_k,e_k\rangle=\alpha_k=f(e_k),
\end{array}
\]
ou seja, $\varphi_u(e_k)=f(e_k)$; logo $\varphi_u=f$. Para $v\in V$, se $\varphi_v=f$, então para $x\in V$ temos:
\[
\langle x,v\rangle=\varphi_v(x)=f(x)=\varphi_u(x)=\langle x,u\rangle;
\]
logo pela Proposição \ref{util} temos $v=u.$
\end{proof}

\begin{observacao}
O teorema de Riesz não é válido para espaços com produto interno de dimensão finita. De fato, se $V=\mathcal{C}[a,b]$, então seja $x_0\in[a,b]$ e seja $\varphi\in V^*$ dada por:
\[
\varphi(f)=f(x_0).
\]
\end{observacao}

\subsection{Transformação Adjunta}

\begin{definicao}
Sejam $U$ e $V$ espaços com produto interno e $T\in\mathcal{L}(U,V)$. Uma \textbf{transformação adjunta} para $T$ é uma função $T^*\in\mathcal{L}(V,U)$ tal que para quaisquer $u\in U$ e $v\in V$ tenhamos:
\[
\langle T(u),v\rangle_V=\langle u,T^*(v)\rangle_U.
\]
\end{definicao}

\begin{teorema}
Se $T\in\mathcal{L}(U,V)$ e $\dim(U)<\infty$, então existe uma única adjunta $T^*$.
\end{teorema}
\begin{proof} Seja $v\in V$. Consideremos a função:
\[
\psi_v:u\mapsto\langle T(u),v\rangle\in K.
\]
Então $\psi=\varphi_v\circ T\in U^*$. Como $\dim(U)<\infty,$ então, pelo teorema de Riesz, existe um único $T^*(v)\in U$ tal que:
\[
\forall u\in U:\psi_v(u)=\langle u,T^*(v)\rangle.
\]
Consideremos a função:
\[
T^*:v\mapsto T^*(v).
\]
Para $a,b\in K$ e $u,v\in V$, então para todo $x\in U$ temos o seguinte:
\[
\begin{array}{rcl}
\langle x,T^*(au+bv)\rangle&=&\langle T(x),au+bv\rangle\\
&=&\overline{a}\langle T(x),u\rangle+\overline{b}\langle T(x),v\rangle\\
&=&\overline{a}\langle x,T^*(u)\rangle+\overline{b}\langle x,T^*(v)\rangle\\
&=&\langle x,aT^*(u)+bT^*(v)\rangle;
\end{array}
\]
logo pela Proposição \ref{util} temos $T^*(au+bv)=aT^*(u)+bT^*(v).$ Portanto $T^*$ é adjunta de $T$. Se $T'$ é outra adjunta de $T$, então para $v\in V$, então para todo $u\in U$ temos:
\[
\langle u,T'(v)\rangle=\langle T(u),v\rangle=\langle u,T^*(v)\rangle;
\] 
logo pela Proposição \ref{util} temos $T'(v)=T^*(v);$ portanto $T'=T^*.$
\end{proof}

\begin{observacao}
Por outro lado, consideremos $V=\mathbb{R}[t]$, e consideremos o produto interno:
\[
\langle f,g\rangle=\int_a^bf(t)g(t)\dif t.
\]
Consideremos o operador derivação $D=\mathcal{L}(V)$ dado por:
\[
\forall f\in\mathbb{R}:D(f)=f'.
\]
Então $D$ não tem um adjunto.
\end{observacao}

\subsection{Propriedades da Adjunção}

\begin{proposicao}
Sejam $U$ e $V$ e $W$ espaços com produto interno de \emph{dimensão finita} e sejam $T,S\in\mathcal{L}(U,V)$ e $P\in\mathcal{L}(V,W)$ e $\alpha\in K$. Então:
\begin{itemize}
\item $(T+S)^*=T^*+S^*.$
\item $(\alpha T)^*=\overline{\alpha}T^*.$
\item $(PT)^*=T^*P^*.$
\item $T^{**}=T.$
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Para $u\in U$ então para $x$ temos:
\[
\begin{array}{rcl}
\langle x,(T+S)^*(u)\rangle&=&\langle (T+S)(x),u\rangle\\
&=&\langle T(x)+S(x),u\rangle\\
&=&\langle T(x),u\rangle+\langle S(x),u\rangle\\
&=&\langle x,T^*(u)\rangle+\langle x,S^*(u)\rangle\\
&=&\langle x,T^*(u)+S^*(u)\rangle\\
&=&\langle x,(T^*+S^*)(u)\rangle;
\end{array}
\]
logo pela Proposição \ref{util} temos $(T+S)^*(u)=(T^*+S^*)(u)$; portanto $(T+S)^*=T^*+S^*.$
\item Para $u\in U$ então para $x$ temos:
\[
\begin{array}{rcl}
\langle x,(\alpha T)^*(u)\rangle&=&\langle (\alpha T)(x),u\rangle\\
&=&\langle \alpha T(x),u\rangle\\
&=&\alpha\langle T(x),u\rangle\\
&=&\alpha\langle x,T^*(u)\rangle\\
&=&\langle x,\overline{\alpha}T^*(u)\rangle\\
&=&\langle x,(\overline{\alpha}T^*)(u)\rangle;
\end{array}
\]
logo pela Proposição \ref{util} temos $(\alpha T)^*(u)=(\overline{\alpha}T^*)(u)$; portanto $(\alpha T)^*=\overline{\alpha}T^*.$
\item Para $u\in U$ então para $x$ temos:
\[
\begin{array}{rcl}
\langle x,(PT)^*(u)\rangle&=&\langle (PT)(x),u\rangle\\
&=&\langle P(T(x)),u\rangle\\
&=&\langle T(x),P^*(u)\rangle\\
&=&\langle x,T^*(P^*(u))\rangle\\
&=&\langle x,(T^*P^*)(u)\rangle;
\end{array}
\]
logo pela Proposição \ref{util} temos $(PT)^*(u)=(T^*P^*)(u)$; portanto $(PT)^*=T^*P^*.$
\item Para $u\in U$ então para $x$ temos:
\[
\begin{array}{rcl}
\langle x,T^{**}(u)\rangle&=&\langle T^*(x),u\rangle\\
&=&\overline{\langle u,T^*(x)\rangle}\\
&=&\overline{\langle T(u),x\rangle}\\
&=&\langle x,T(u)\rangle;
\end{array}
\]
logo pela Proposição \ref{util} temos $T^{**}(u)=T(u)$; portanto $T^{**}=T.$
\end{itemize}
\end{proof}

\begin{proposicao}\label{keradjunto}
Se $T\in\mathcal{L}(U,V)$ e $T^*$ é uma adjunta de $T,$ então:
\[
\mathrm{Ker}(T^*)=(\mathrm{Im}(T))^\perp.
\]
\end{proposicao}
\begin{proof}
Pela Proposição \ref{util}, temos as seguintes equivalências:
\[
\begin{array}{rcl}
v\in\mathrm{Ker}(T^*)&\Leftrightarrow& T^*(v)=0\\
&\Leftrightarrow&\forall x\in U:\langle x,T^*(v)\rangle=0\\
&\Leftrightarrow&\forall x\in U:\langle T(x),v\rangle=0\\
&\Leftrightarrow&v\in(\mathrm{Im}(T))^\perp.
\end{array}
\]
\end{proof}

\subsection{Matriz Adjunta}

\begin{definicao}
Seja $A=(a_{i,j})$ uma matriz sobre $K$. Definimos a \textbf{adjunta} de $A$ como a matriz $A^*$ dada por:
\[
A^*=(\overline{a_{j,i}}).
\]
\end{definicao}

\begin{teorema}
Sejam $B=(e_1,\dots,e_n)$ e $C=(f_1,\dots,f_m)$ bases ortonormais de $U$ e $V$, e seja $T\in\mathcal{L}(U,V)$. Se $A=[T]_{B,C}$, então $A^*=[T^*]_{C,B}$.
\end{teorema}
\begin{proof}
Pela Proposição \ref{matriz}, temos o seguinte:
\[
[T]_{B,C}=\begin{pmatrix}
\langle T(e_1),f_1\rangle&&\langle T(e_n),f_1\rangle\\&\ddots&\\\langle T(e_1),f_n\rangle&&\langle T(e_n),f_n\rangle
\end{pmatrix}=\begin{pmatrix}
\langle e_1,T^*(f_1)\rangle&&\langle e_n,T^*(f_1)\rangle\\&\ddots&\\\langle e_1,T^*(f_n)\rangle&&\langle e_n,T^*(f_n)\rangle
\end{pmatrix},
\]
assim temos:
\[
[T]_{B,C}^*=\begin{pmatrix}
\overline{\langle e_1,T^*(f_1)\rangle}&&\overline{\langle e_1,T^*(f_n)\rangle}\\&\ddots&\\\overline{\langle e_n,T^*(f_1)\rangle}&&\overline{\langle e_n,T^*(f_n)\rangle}
\end{pmatrix}=\begin{pmatrix}
\langle T^*(f_1),e_1\rangle&&\langle T^*(f_n),e_1\rangle\\&\ddots&\\\langle T^*(f_1),e_n\rangle&&\langle T^*(f_n),e_n\rangle
\end{pmatrix}=[T^*]_{C,B},
\]
\end{proof}

\subsection{Métodos de Aproximação}

Agora seja $T\in\mathcal{L}(U,V)$ e $b\in V$, e suponhamos que $\dim(U)<\infty.$ Queremos encontrar $x\in U$ que dê a melhor aproximação da equação:
\[
T(x)=b,
\]
ou seja, queremos encontrar $x\in U$ tal que a distância:
\[
\norm{T(x)-b}
\]
seja \emph{mínima}. Para isso, pela Proposição \ref{proj2}, podemos considerar $x\in U$ tal que $T(x)$ seja a projeção ortogonal de $b$ em $\mathrm{Im}(T).$ Pela Proposição \ref{keradjunto}, temos $(\mathrm{Im}(T))^\perp=\mathrm{Ker}(T^*),$ de modo que basta encontrarmos um $x\in U$ tal que $T(x)-b\in\mathrm{Ker}(T^*),$ ou seja, $T^*T(x)=T^*(b).$

\bigskip
\noindent
\textcolor{red}{EXEMPLO NUMÉRICO QUE ESTOU COM PREGUIÇA DE ESCREVER PORQUE É CHATO DEMAIS}

\newpage

\section{Operadores Unitários}

\subsection{Operadores Unitários}

\begin{definicao}
Sejam $U$ e $V$ espaços com produto interno. Um \textbf{isomorfismo de espaços com produto interno} é uma transformação linear bijetora $T$ tal que para $u,v\in U$ tenhamos:
\[
\langle T(u),T(v)\rangle=\langle u,v\rangle.
\]
\end{definicao}

\begin{teorema}\label{unitario}
Seja $U$ e $V$ espaços com produto interno de \emph{uma mesma dimensão finita} e seja $T\in\mathcal{L}(U,V)$. São equivalentes:
\begin{itemize}
\item[i)] Para $u,v\in U$ temos $\langle u,v\rangle=\langle T(u),T(v)\rangle$.
\item[ii)] $T$ é um isomorfismo de espaços com produto interno.
\item[iii)] Para toda base ortonormal $B$ de $U$, a sua imagem $T[B]$ é uma base ortonormal de $V$. 
\item[iv)] Existe uma base ortonormal $B$ de $U$ tal que $T[B]$ seja uma base ortonormal de $V$.
\end{itemize}
\end{teorema}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Se (i), então primeiro provemos que $\Ker(T)=0$. Para $v\in \Ker(T)$, então $T(v)=0$, aí:
\[
\langle v,v\rangle=\langle T(v),T(v)\rangle=0,
\]
aí $v=0$. Como $\dim(V)=\dim(U)=\dim(\mathrm{Im}(T))+\dim(\Ker(T))$, então $\dim(\mathrm{Im}(T))=\dim(V)$, aí $T$ é sobrejetora, aí (ii).
\item Se (ii), então, para base ortonormal $B=(e_1,\dots,e_n)$ de $U$, a sequência $(T(e_1),\dots,T(e_n))$ é ortonormal em $V$, aí é uma base ortonormal de $V$; logo (iii).
\item Se (iii), como sempre existe uma base ortonormal de $U$, então (iv) é evidente.
\item Se (iv), então seja $B=(e_1,\dots,e_n)$ uma base ortonormal de $U$ tal que $(T(e_1),\dots,T(e_n))$ seja base ortonormal de $V$. Para $u,v\in U$, então existem $a_1,\dots,a_n\in K$ e $b_1,\dots,b_n$ tais que:
\[
u=a_1e_1+\dots+a_ne_n,\quad\quad v=b_1e_1+\dots+b_ne_n,
\]
então:
\[
\langle u,v\rangle=a_1\overline{b_1}+\dots+a_n\overline{b_n},
\]
aí:
\[
T(u)=a_1T(e_1)+\dots+a_nT(e_n),\quad\quad T(v)=b_1T(e_1)+\dots+b_nT(e_n),
\]
aí:
\[
\begin{array}{rcl}
\langle T(u),T(v)\rangle&=&\sum_{i,j}\langle a_iT(e_i),b_jT(e_j)\rangle\\
&=&\sum_{i,j}a_i\overline{b_j}\langle T(e_i),T(e_j)\rangle\\
&=&\sum_{k=1}a_k\overline{b_k}\\
&=&\langle u,v\rangle.
\end{array}
\]
\end{itemize}
\end{proof}

\begin{corolario}
Dois espaços vetoriais $U$ e $V$ com produto interno de dimensão finita são isomorfos se e só se $\dim \ U=\dim \ V$.
\end{corolario}

\begin{proof}
Se $\dim(U)=\dim(V)$, então escolhamos bases ortonormais $B=(e_1,\dots,e_n)$ de $U$ e $C=(f_1,\dots,f_n)$ de $V$, aí seja $T:U\rightarrow V$ dada por:
\[
T(a_1e_1+\dots+a_ne_n)=a_1f_1+\dots+a_nf_n,
\]
então $T\in\mathcal{L}(U,V)$, e para $u,v\in U$, sendo:
\[
u=a_1e_1+\dots+a_ne_n,\quad\quad v=b_1e_1+\dots+b_ne_n,
\]
então:
\[
\langle u,v\rangle=a_1\overline{b_1}+\dots+a_n\overline{b_n}
\]
e também:
\[
\langle T(u),T(v)\rangle=a_1\overline{b_1}+\dots+a_n\overline{b_n},
\]
assim:
\[
\langle u,v\rangle=\langle T(u),T(v)\rangle.
\]
\end{proof}

\begin{definicao}
Se $V$ é um espaço com produto interno e $T\in\mathcal{L}(V)$, dizemos que $T$ é \textbf{operador unitário} se e só se $\forall u,v\in V:\langle T(u),T(v)\rangle=\langle u,v\rangle$.
\end{definicao}

\begin{teorema}
Seja $T\in\mathcal{L}(U,V)$. Então as seguintes propriedades são equivalentes:
\begin{itemize}
\item[i)] $\langle T(u),T(v)\rangle=
\langle u,v\rangle.$
\item[ii)] $\forall u\in U:\norm{T(u)}=\norm{u}.$
\end{itemize}
\end{teorema}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[a)] Suponhamos (i). Então, para todo $u\in U$ temos:
\[
\norm{T(u)}^2=\langle T(u),T(u)\rangle=\langle u,u\rangle=\norm{u}^2,
\]
aí $\norm{T(u)}=\norm{u};$ logo (ii).
\item[b)] Suponhamos (ii). Então, para quaisquer $u,v\in U$ temos:
\[
\norm{T(u+v)}^2=\norm{u+v}^2,
\]
aí:
\[
\norm{T(u)}^2+\langle T(u),T(v)\rangle+\langle T(v),T(u)\rangle+\norm{T(v)}^2=
\norm{u}^2+\langle u,v\rangle+\langle v,u\rangle+\norm{v}^2,
\]
aí:
\[
\langle T(u),T(v)\rangle+\langle T(v),T(u)\rangle=
\langle u,v\rangle+\langle v,u\rangle.
\]
Agora temos dois casos:
\begin{itemize}
\item[$\bullet$] Se $K=\mathbb{R}$, então para $u,v\in U$ temos:
\[
\langle T(u),T(v)\rangle+\langle T(v),T(u)\rangle=
\langle u,v\rangle+\langle v,u\rangle,
\]
aí temos:
\[
\langle T(u),T(v)\rangle+\langle T(u),T(v)\rangle=
\langle u,v\rangle+\langle u,v\rangle,
\]
aí temos:
\[
2\langle T(u),T(v)\rangle=
2\langle u,v\rangle,
\]
aí concluímos que:
\[
\langle T(u),T(v)\rangle=
\langle u,v\rangle.
\]
\item[$\bullet$] Se $K=\mathbb{C}$, então para $u,v\in U$ temos:
\[
\langle T(u),T(v)\rangle+\langle T(v),T(u)\rangle=
\langle u,v\rangle+\langle v,u\rangle,
\]
mas também:
\[
\langle T(iu),T(v)\rangle+\langle T(v),T(iu)\rangle=
\langle iu,v\rangle+\langle v,iu\rangle,
\]
assim temos:
\[
i\langle T(u),T(v)\rangle-i\langle T(v),T(u)\rangle=
i\langle u,v\rangle-i\langle v,u\rangle,
\]
aí temos:
\[
\langle T(u),T(v)\rangle-\langle T(v),T(u)\rangle=
\langle u,v\rangle-\langle v,u\rangle,
\]
assim temos:
\[
\begin{array}{rcl}
2\langle T(u),T(v)\rangle&=&\left(\langle T(u),T(v)\rangle+\langle T(v),T(u)\rangle\right)+\left(\langle T(u),T(v)\rangle-\langle T(v),T(u)\rangle\right)\\
&=&\left(\langle u,v\rangle+\langle v,u\rangle\right)+\left(\langle u,v\rangle-\langle v,u\rangle\right)\\
&=& 2\langle u,v\rangle,
\end{array}
\]
assim concluímos que:
\[
\langle T(u),T(v)\rangle=
\langle u,v\rangle.
\]
\end{itemize}
De qualquer modo concluímos que (i) ocorre.
\end{itemize}
\end{proof}

\begin{proposicao}
Seja $V$ um espaço com produto interno.
\begin{itemize}
\item[a)] Sejam $T,S\in\mathcal{L}(V)$ unitários. Então $T\circ S$ é operador unitário.
\item[b)] Se $\dim(V)<\infty,$ então o conjunto dos operadores unitários formam um grupo em relação à composição.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item[a)] Para operadores unitários $T$ e $S$, para todo $u,v\in V$ temos o seguinte:
\[
\langle TS(u),TS(v)\rangle=\langle S(u),S(v)\rangle=\langle u,v\rangle;
\]
logo $TS$ é unitário. Além disso é fácil ver que o operador identidade $I$ é unitário.
\item[b)] Se $\dim(V)<\infty,$ então, pelo Teorema \ref{unitario}, todo operador unitário $T$ é inversível, e para $u,v\in V$ temos:
\[
\langle u,v\rangle=\langle TT^{-1}(u),TT^{-1}(v)\rangle=\langle T^{-1}(u),T^{-1}(v)\rangle;
\]
logo $T^{-1}$ é unitário; portanto o conjunto dos operadores unitários é subgrupo do conjunto dos operadores inversíveis.
\end{itemize}
\end{proof}

\begin{definicao}
O conjunto dos operadores unitários no espaço $K^n$ se denota por $U_n(K)$.
\end{definicao}

\begin{teorema}
Para $T\in\mathcal{L}(V)$ \emph{inversível}, então $T$ é unitário se e somente se $T^*=T^{-1}$.
\end{teorema}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Se $T$ é unitário, então para todo $v\in V$ temos:
\[
\forall u\in V: \langle u,v\rangle=\langle T(u),T(v)\rangle=\langle u,T^*T(v)\rangle,
\]
aí $T^*T(v)=v$; logo $T^*T=I,$ aí $T^*=T^{-1}.$

\item Se $T^*=T^{-1}$, então para $u,v\in V$ temos:
\[
\langle T(u),T(v)\rangle=\langle u,T^*T(v)\rangle=\langle u,T^{-1}T(v)\rangle=\langle u,v\rangle;
\]
logo $T$ é unitário.
\end{itemize}
\end{proof}

\subsection{Matrizes Unitárias}

\begin{definicao}
Uma matriz $A\in\m{n}{K}$ se chama \textbf{unitária} se e só se $A^*$ é inversa de $A$. 
\end{definicao}

\begin{corolario}
Num espaço com produto interno de \emph{dimensão finita}, para $T\in\mathcal{L}(V)$, então $T$ é unitário se e só se para toda base $B$ de $V$ a matriz $[T]_B$ é unitária.
\end{corolario}

\begin{teorema}
Para matriz $A\in\m{n}{K}$, então $A$ é unitária se e só se as linhas de $A$ formam conjunto ortonormal em $K^n$, e isso equivale a dizer que as colunas de $A$ formam um conjunto ortonormal em $K^n$.
\end{teorema}
\begin{proof}
Seja $A\in\m{n}{K}$ uma matriz unitária, então $AA^*=I$, aí, sendo:
\[
A=\begin{pmatrix}
a_{1,1}&\dots&a_{1,n}\\\dots&\dots&\dots\\a_{n,1}&\dots&a_{n,n}
\end{pmatrix},
\]
então:
\[
A^*=\begin{pmatrix}
\overline{a_{1,1}}&\dots&\overline{a_{n,1}}\\\dots&\dots&\dots\\\overline{a_{1,n}}&\dots&\overline{a_{n,n}}
\end{pmatrix},
\]
então temos:
\[
A_{i,?}=(a_{i,1},\dots,a_{i,n}),\quad\quad A^*_{?,j}=\begin{pmatrix}
\overline{a_{j,1}}\\\vdots\\\overline{a_{j,n}}
\end{pmatrix},
\]
assim:
\[
a_{i,1}\overline{a_{j,1}}+\dots+a_{i,n}\overline{a_{j,n}}=\delta_{i,j},
\]
assim:
\[
\langle(a_{i,1},\dots,a_{i,n}),(a_{j,1},\dots,a_{j,n})\rangle=\delta_{i,j}.
\]
\end{proof}

\subsection{Matrizes Ortogonais}

\begin{definicao}
Para $A\in\m{n}{K}$, dizemos que $A$ se chama \textbf{ortogonal} se e só se $A^{-1}=A^t$.
\end{definicao}

\begin{observacao}
Toda matriz real ortogonal é unitária. Toda matriz unitária ortogonal é real.
\end{observacao}

\begin{definicao}
Definimos o \textbf{grupo ortogonal} $O(n,\mathbb{R})$ como o conjunto das matrizes reais ortogonais.
\end{definicao}

\subsection{Equivalência Unitária e Equivalência Ortogonal}

\begin{definicao}
Para matrizes $A,B\in\m{n}{K}$, dizemos que $A$ é \textbf{unitariamente equivalente} a $B$ se e só se existe uma matriz $U\in\m{n}{K}$ \emph{unitária} tal que $U^{-1}AU=B$.
\end{definicao}

\begin{definicao}
Para matrizes $A,B\in\m{n}{R}$, dizemos que $A$ é \textbf{ortogonalmente equivalente} a $B$ se e só se existe uma matriz $U\in\m{n}{\mathbb{R}}$ \emph{ortogonal} tal que $U^{-1}AU=B$.
\end{definicao}

\begin{proposicao}
Seja $T\in\mathcal{L}(V)$. Então, para quaisquer bases ortonormais $B$ e $C$ de $V$, as matrizes $[T]_B$ e $[T]_C$ são unitariamente equivalentes.
\end{proposicao}
\begin{proof}
Seja $P_{B,C}$ a matriz de mudança de $B$ para $C$. Então $P_{B,C}$ é unitária. Mas também:
\[
[T]_C=P^{-1}_{B,C}[T]_BP_{B,C}.
\]
\end{proof}

\newpage

\section{Operadores Normais}

Assim como estudamos os operadores que possuíssem uma base de autovetores em espaços vetoriais, agora queremos estudar os operadores que possuam uma base \emph{ortonormal} de autovetores em espaços com \emph{produto interno}.

\subsection{Operadores Normais}

\begin{proposicao}
Se $V$ tem dimensão finita e $T\in\mathcal{V}$ tem uma base ortonormal de autovetores, então $TT^*=T^*T.$
\end{proposicao}
\begin{proof}
Seja $T\in\mathcal{L}(V)$ que possua uma base ortonormal $B=(e_1,\dots,e_n)$ de autovetores. Então:
\[
[T]_B=\begin{pmatrix}
\lambda_1&&0\\&\dots&\\0&&\lambda_n
\end{pmatrix},
\]
e também:
\[
[T^*]_B=\begin{pmatrix}
\overline{\lambda_1}&&0\\&\dots&\\0&&\overline{\lambda_n}
\end{pmatrix},
\]
aí:
\[
[TT^*]_B=[T]_B[T^*]_B=\begin{pmatrix}
\lambda_1\overline{\lambda_1}&&0\\&\dots&\\0&&\lambda_n\overline{\lambda_n}
\end{pmatrix}=\begin{pmatrix}
\overline{\lambda_1}\lambda_1&&0\\&\dots&\\0&&\overline{\lambda_n}\lambda_n
\end{pmatrix}=[T^*]_B[T]_B=[T^*T]_B,
\]
assim $TT^*=T^*T$.
\end{proof}

\begin{definicao}
Para $T\in\mathcal{L}(V)$, dizemos que $T$ é \textbf{normal} se e só se $TT^*=T^*T$.
\end{definicao}

\begin{exemplo}
Temos alguns exemplos:
\begin{itemize}
\item Operadores autoadjuntos (isto é, que satisfazem $T^*=T$) são normais.
\item Operadores unitários (isto é, que satisfazem $T^*=T^{-1}$) são normais.
\item Operadores antiadjuntos (isto é, que satisfazem $T^*=-T$) são normais.
\end{itemize}
\end{exemplo}

\begin{teorema}\label{normal}
Seja $V$ um espaço com produto interno de \emph{dimensão finita}. Seja $T\in\mathcal{L}(V)$ um operador normal. Então temos as seguinte propriedades:
\begin{enumerate}
\item Para quaisquer $u,v\in V$ temos:
\[
\langle T(u),T(v)\rangle=\langle T^*(u),T^*(v)\rangle.
\]
Em particular, para $v\in V$ temos:
\[
\norm{T(v)}=\norm{T^*(v)}.
\]
Portanto temos:
\[
\mathrm{Ker}(T)=\mathrm{Ker}(T^*).
\]
\item Temos:
\[
\mathrm{Ker}(T^*T)=\mathrm{Ker}{T}.
\]
\item Para $k\geq 1$ temos:
\[
\mathrm{Ker}(T^k)=\mathrm{Ker}(T).
\]
\item O polinômio minimal $m_T$ é um produto de fatores irredutíveis \emph{distintos}.
\item Para $\lambda\in K$ e $v\in V$ temos:
\[
T(v)=\lambda v\Leftrightarrow T^*(v)=\overline{\lambda}v.
\]
\item Para polinômios $f$ e $g$ primos entre si, então para $x,y\in V$, se $f(T)(x)=0$ e $g(T)(y)=0$, então $x$ e $y$ são ortogonais.
\item Para $\lambda,\mu\in K$, se $\lambda\neq\mu$, então todo autovetor associado a $\lambda$ é ortogonal a todo autovetor associado a $\mu.$
\end{enumerate}
\end{teorema}
\begin{proof}
Temos o seguinte:
\begin{enumerate}
\item Para quaisquer $u,v\in V$ temos:
\[
\langle T(u),T(v)\rangle=\langle u,T^*T(v)\rangle=\langle u,TT^*(v)\rangle=\langle T^*(u),T^*(v)\rangle.
\]
Em particular, para $v\in V$ temos:
\[
\norm{T(v)}=\sqrt{\langle T(v),T(v)\rangle}=\sqrt{\langle T^*(v),T^*(v)\rangle}=\norm{T^*(v)}.
\]
Portanto temos:
\[
x\in\mathrm{Ker}(T)\Leftrightarrow\norm{T(x)}=0\Leftrightarrow\norm{T^*(x)}=0\Leftrightarrow x\in\mathrm{Ker}(T^*).
\]
\item Para $v\in V$, se $T^*T(v)=0$, então:
\[
\langle T(v),T(v)\rangle=\langle T^*T(v),v\rangle=0,
\]
aí $T(v)=0$.
\item Para $v\in V,$ se $T^2(v)=0$, então:
\[
\langle T^*T(v),T^*T(v)\rangle=\langle TT(v),TT(v)\rangle=0,
\]
aí $T^*T(v)=0$, aí $T(v)=0$. Para $k>1$, para $v\in V$, se $T^k(v)=0$, então $T^2T^{k-2}(v)=T^k(v)=0$, aí $T^{k-1}(v)=TT^{k-2}(v)=0.$ O resto procede por indução.
\item Seja $m_T=p_1^{e_1}\dots p_r^{e_r}.$ Para cada $i$ seja $q_i=\frac{m_t}{p_i^{e_i}}$. Então para $v\in V$ temos $p_i^{e_i}(T)q_i(T)(v)=m_T(T)(v)=0$, mas $p_i(T)$ é normal, aí $p_i(T)q_i(T)(v)=0;$ logo $m_t\mid p_iq_i$, aí $e_i=1.$
\item É fácil ver que $T-\lambda I$ é normal, então:
\[
\mathrm{Ker}(T-\lambda I)=\mathrm{Ker}(T-\lambda I)^*=\mathrm{Ker}(T^*-\overline{\lambda}I).
\]
\item Para polinômios $f$ e $g$ primos entre si, então existem polinômios $a$ e $b$ tais que $af+bg=1$, aí para $x,y\in V$, se $f(T)(x)=0$ e $g(T)(y)=0$, então $b(T)g(T)(y)=0$, mas $b(T)g(T)$ é normal, aí $(b(T)g(T))^*(y)=0$, assim:
\[
\begin{array}{rcl}
\langle x,y\rangle&=&\langle (a(T)f(T)+b(T)g(T))(x),y\rangle\\
&=&\langle a(T)f(T)(x)+b(T)g(T)(x),y\rangle\\
&=&\langle b(T)g(T)(x),y\rangle\\
&=&\langle x,(b(T)g(T))^*(y)\rangle\\
&=&\langle x,0\rangle\\
&=&0.
\end{array}
\]
\item Para $\lambda,\mu\in K$, se $\lambda\neq\mu$, então os polinômios $x-\lambda$ e $x-\mu$ são primos entre si, aí segue a propriedade.
\end{enumerate}
\end{proof}

\newpage

\subsection{Formas Canônicas no Caso Complexo}

\begin{proposicao}
Seja $A$ uma matriz sobre $K$ tal que $\mathrm{tr}(AA^*)=0,$ então $A=0$.
\end{proposicao}
\begin{proof}
Seja $A=(a_{i,j})$. Então temos:
\[
\mathrm{tr}(AA^*)=\sum_i(AA^*)_{i,i}=\sum_i\sum_jA_{i,j}A^*_{j,i}=\sum_i\sum_ja_{i,j}\overline{a_{i,j}}=\sum_i\sum_j\abs{a_{i,j}}^2,
\]
aí $\mathrm{tr}(AA^*)=0$ implica que para todo $(i,j)$ tenhamos $a_{i,j}=0;$ logo $A=0.$
\end{proof}

\begin{proposicao}\label{invariante}
Seja $V$ um espaço com produto interno de dimensão finita e seja $T$ um operador normal. Então, para todo subespaço $W$, se $W$ é $T$-invariante, então $W^\perp$ é $T$-invariante.
\end{proposicao}
\begin{proof}
Seja $(e_1,\dots,e_m)$ base ortonormal de $W$ e seja $(e_{m+1},\dots,e_n)$ base ortonormal de $W^\perp$, e consideremos $B=(e_1,\dots,e_n),$ que será uma base ortonormal de $V$. Então temos algo assim:
\[
[T]_B=\begin{pmatrix}
X&Z\\0&Y
\end{pmatrix},
\]
assim:
\[
[TT^*]_B=\begin{pmatrix}
X&Z\\0&Y
\end{pmatrix}\begin{pmatrix}
X^*&0\\Z^*&Y^*
\end{pmatrix}=\begin{pmatrix}
XX^*+ZZ^*&ZY^*\\YZ^*&YY^*
\end{pmatrix}
\]
e também:
\[
[T^*T]_B=\begin{pmatrix}
X^*&0\\Z^*&Y^*
\end{pmatrix}\begin{pmatrix}
X&Z\\0&Y
\end{pmatrix}=\begin{pmatrix}
X^*X&X^*Z\\Z^*X&Z^*Z+Y^*Y
\end{pmatrix},
\]
mas $TT^*=T^*T$, assim $XX^*+ZZ^*=X^*X$, assim $\mathrm{tr}(XX^*)+\mathrm{tr}(ZZ^*)=\mathrm{tr}(X^*X),$ mas $\mathrm{tr}(XX^*)=\mathrm{tr}(X^*X),$ aí $\mathrm{tr}(ZZ^*)=0,$ aí $Z=0,$ assim:
\[
[T]_B=\begin{pmatrix}
X&0\\0&Y
\end{pmatrix},
\]
o que implica que $W^\perp$ seja $T$ invariante.
\end{proof}

\begin{teorema}
Seja $V$ um espaço com produto interno sobre $\mathbb{C}$ de dimensão finita e seja $T\in\mathcal{L}(V)$ um operador normal. Então existe uma base ortonormal $B$ de $V$ tal que $[T]_B$ seja diagonal.
\end{teorema}
\begin{proof}
Indução na dimensão de $V.$ Se $\dim(V)=0$ é fácil. Se $\dim(V)>0$, como $\mathbb{C}$ é algebricamente fechado, então existem um $\lambda\in \mathbb{C}$ e um $v\neq 0$ tais que $T(v)=\lambda v$, aí seja $W=Kv$, então $W$ é $T$-invariante, aí pela Proposição \ref{invariante} o subespaço $W^\perp$ é $T$-invariante e a restrição de $T$ a $W^\perp$ é normal, aí por hipótese de indução existe uma base ortonormal $C$ de autovetores de $W^\perp$, assim $B=\{v\}\cup C$ é uma base ortonormal de autovetores de $V.$
\end{proof}

\noindent
\textcolor{red}{Outro exemplo numérico enfadonho!}

\subsection{Formas Canônicas no Caso Real}

\begin{proposicao}
Sejam $a,b\in\mathbb{R}$. Então a matriz:
\[
A=\begin{pmatrix}
a&-b\\b&a
\end{pmatrix}
\]
é normal.
\end{proposicao}
\begin{proof}
É fácil ver que $AA^*$ e $A^*A$ valem:
\[
\begin{pmatrix}
a^2+b^2&0\\0&a^2+b^2
\end{pmatrix}.
\]
\end{proof}

\begin{proposicao}\label{cicloreal}
Sejam $\alpha,\beta\in\mathbb{R}$ com $\beta\neq 0$, seja $p(x)=(x-\alpha)^2+\beta^2$, e seja $V$ um espaço com produto interno sobre $\mathbb{R}$ de dimensão finita e seja $T\in\mathcal{L}(V)$ um operador normal tal que $p(T)=0.$ Então existe uma base ortonormal $B$ tal que:
\[
[T]_B=\begin{pmatrix}
A&&0\\&\ddots&\\0&&A
\end{pmatrix},
\]
em que:
\[
A=\begin{pmatrix}
\alpha&-\beta\\\beta&\alpha
\end{pmatrix}.
\]
\end{proposicao}
\begin{proof}
Indução na dimensão de $V$. Se $\dim(V)=0$ é fácil. Se $\dim(V)>0$ então pegue um $e$ tal que $m_{T,e}=p$, aí seja $W=Z(e,T)$, então $W$ é $T$-invariante, aí pela Proposição \ref{invariante}, o subespaço $W^\perp$ é $T$-invariante e $T\upharpoonright_W$ e $T\upharpoonright_{W^\perp}$ são normais, assim podemos pegar uma base ortonormal $B$ de $W$, aí a matriz $[T\upharpoonright_W]_B$ tem a seguinte forma:
\[
[T\upharpoonright_W]_B=\begin{pmatrix}
a&c\\b&d
\end{pmatrix},
\]
aí, como $T\upharpoonright_W$ é normal e $B$ é ortonormal, temos o seguinte:
\[
\begin{pmatrix}
a&b\\c&d
\end{pmatrix}\begin{pmatrix}
a&c\\b&d
\end{pmatrix}=\begin{pmatrix}
a&c\\b&d
\end{pmatrix}\begin{pmatrix}
a&b\\c&d
\end{pmatrix},
\]
assim dividiremos em dois casos:
\begin{itemize}
\item Se $b=c$ então:
\[
[T\upharpoonright_W]_B=\begin{pmatrix}
a&b\\b&d
\end{pmatrix},
\]
mas o polinômio característico desta matriz é:
\[
q(x)=(x-a)(x-d)-b^2=x^2-(a+d)x+ad-b^2,
\]
de modo que:
\[
\Delta=(a+d)^2-4\cdot1\cdot(ad-b^2)=(a-d)^2+b^2\geq 0,
\]
aí $q$ tem raiz real, porém $q\mid p,$ aí $p$ tem raiz real, contradição.
\item Se $b\neq c$, então a igualdade das matrizes nos permite inferir que $b=-c$ e $a=d$, aí:
\[
[T\upharpoonright_W]_B=\begin{pmatrix}
a&-b\\b&a
\end{pmatrix},
\]
e além disso devemos ter $b\neq 0$, assim o polinômio característico da matriz é:
\[
q(x)=(x-a)^2+b^2=x^2-2ax+a^2+b^2,
\]
que é irredutível em $\mathbb{R}$, mas $q\mid p$, aí $q=p$, e isso implica que $a=\alpha$ e também $b=\beta$ ou $b=-\beta$, aí, sendo $B=(e_1,e_2)$ e $B'=(e_2,e_1)$, temos $[T\upharpoonright_W]_{B}=A$ ou $[T\upharpoonright_W]_{B'}=A$.
\end{itemize}
Com isso existe uma base ortonormal $\tilde{B}$ tal que $[T\upharpoonright_W]_{\tilde{B}}=A$. Além disso, por hipótese de indução, podemos pegar uma base ortonormal $C$ de $W^\perp$ tal que $[T\upharpoonright_{W^\perp}]_C$ tenha a seguinte forma:
\[
[T\upharpoonright_{W^\perp}]_C=\begin{pmatrix}
A&&0\\&\ddots&\\0&&A
\end{pmatrix}.
\]
Portanto concluímos que:
\[
[T]_{B\cup C}=\begin{pmatrix}
[T\upharpoonright_W]_{\tilde{B}}&0\\0&[T\upharpoonright_{W^\perp}]_C
\end{pmatrix}=\begin{pmatrix}
A&&&\\&A&&\\&&\ddots&\\&&&A
\end{pmatrix}.
\]
\end{proof}

\begin{teorema}
Seja $V$ um espaço com produto interno sobre $\mathbb{R}$ de dimensão finita e seja $T\in\mathcal{L}(V)$ um operador normal. Então existe uma base ortonormal $B$ de $V$ tal que:
\[
[T]_B=\begin{pmatrix}
A_1&&0\\&\ddots&\\0&&A_n
\end{pmatrix},
\]
em que $A_i$ esteja em uma das seguintes formas:
\[
\begin{array}{ccccl}
A_i&=&\begin{pmatrix}
a_i
\end{pmatrix}&,&\text{com }a_i\in\mathbb{R}.\\
A_i&=&\begin{pmatrix}
a_i&-b_i\\b_i&a_i
\end{pmatrix}&,&\text{com }a_i,b_i\in\mathbb{R}\text{ tais que }b_i\neq 0.
\end{array}
\]
\end{teorema}
\begin{proof}
Pelo Teorema \ref{normal}, podemos inferir que o polinômio minimal de $T$ deve ser da forma:
\[
m_t(x)=\left(x-\lambda_1\right)\dots\left(x-\lambda_r\right)\left(\left(x-a_1\right)^2+b_1^2\right)\dots\left(\left(x-a_s\right)^2+b_s^2\right)
\]
com $\lambda_1,\dots,\lambda_r$ distintos e com os pares $(a_1,b_1),\dots,(a_s,b_s)$ distintos e $b_1,\dots,b_s\neq 0$, assim, sendo:
\[
V_i=\mathrm{Ker}\left(T-\lambda_iI\right),\quad\quad W_j=\mathrm{Ker}\left(\left(T-a_sI\right)^2+b_s^2I\right),
\]
a decomposição primária nos permite inferir que:
\[
V=V_1\oplus\dots\oplus V_r\oplus W_1\oplus\dots\oplus W_s,
\]
e pelo Teorema \ref{normal} podemos ver que dois vetores de somandos diferentes são ortogonais. Para cada $i$, pegue uma base ortonormal $B_i$ de $V_i$. Para cada $j$, como $T$ é normal e $W_j$ é $T$-invariante, então pelo Teorema \ref{invariante}, o subespaço $W_j$ também é $T^*$-invariante, aí seja $T_j=T\upharpoonright_{W_j}$, então $T_j$ é normal e também $m_{T_j}=(x-a_j)^2+b_j^2,$ assim pela Proposição \ref{cicloreal} podemos pegar uma base ortonormal $C_j$ tal que:
\[
[T_j]_{C_j}=\begin{pmatrix}
A_j&&0\\&\ddots&\\0&&A_j
\end{pmatrix},
\]
em que:
\[
A_j=\begin{pmatrix}
a_j&-b_j\\b_j&a_j
\end{pmatrix}.
\]
Agora junte os $B_1,\dots,B_r$ e $C_1,\dots,C_s$ para obter uma base ortonormal de $V$ que satisfaça o objetivo do enunciado em questão.
\end{proof}

\newpage

\section{Complexificação}

\begin{definicao}
Seja $V$ um espaço vetorial sobre $\mathbb{R}$. Definimos a \textbf{complexificação} de $V$ como:
\[
V_\mathbb{C}=\{(x,y)\mid x,y\in V\},
\]
e munimo-lo do seguinte:
\[
\begin{array}{rcl}
(x,y)+(x',y')&=&(x+x',y+y').\\
(a+bi)(x,y)&=&(ax-by,bx+ay).
\end{array}
\]
\end{definicao}

\begin{proposicao}
Nas notações acima, verifica-se que $V_\mathbb{C}$ é um espaço vetorial sobre $\mathbb{C}$.
\end{proposicao}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{definicao}
Para espaço vetorial $V$ sobre $\mathbb{R}$, para $T\in\mathcal{L}(V)$ definimos:
\[
T_\mathbb{C}(x,y)=(T(x),T(y)).
\]
\end{definicao}

\begin{proposicao}
Nas notações acima, verifica-se que $T_\mathbb{C}\in\mathcal{L}(V_\mathbb{C})$.
\end{proposicao}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{proposicao}
Nas notações acima, verificam-se:
\begin{itemize}
\item Para toda base $B$ base de $V$, então $B_\mathbb{C}=\{(x,0)\mid x\in B\}$ é uma base de $V_\mathbb{C}$.
\item Para todo $T\in\mathcal{L}(V)$, então $p_T=p_{T_\mathbb{C}}$.
\item Para subespaço $W$ de $V_\mathbb{C}$, então $W$ é da forma $W=U_\mathbb{C}$ para algum subespaço $U$ de $V$ se e só se $W^*\subseteq W$, em que definimos $W^*=\{(x,-y)\mid(x,y)\in W\}$.
\end{itemize}
\end{proposicao}
\begin{proof}
Temos o seguinte:
\begin{itemize}
\item Seja $B=(e_1,\dots,e_n)$ uma base ortonormal de $V$. Para $(x,y)\in V_\mathbb{C}$, existem números reais:
\[
a_1,\dots,a_n,b_1,\dots,b_n\in\mathbb{R}
\]
tais que:
\[
x=a_1e_1+\dots+a_ne_n,\quad\quad y=b_1e_1+\dots+b_ne_n,
\]
assim:
\[
(x,y)=(a_1+b_1i)(e_1,0)+\dots+(a_n+b_ni)(e_n,0).
\]
Logo $B_\mathbb{C}$ gera $V_\mathbb{C}$. Para:
\[
a_1,\dots,a_n,b_1,\dots,b_n\in\mathbb{R},
\]
se:
\[
(a_1+b_1i)(e_1,0)+\dots+(a_n+b_ni)(e_n,0)=0,
\]
então:
\[
(a_1e_1+\dots+a_ne_n,b_1e_1+\dots+b_ne_n)=0,
\]
aí:
\[
a_1e_1+\dots+a_ne_n=0,\quad\quad b_1e_1+\dots+b_ne_n=0,
\]
assim:
\[
a_1=\dots=a_n=b_1=\dots=b_n=0.
\]
Logo $B_\mathbb{C}$ é linearmente independente.
\item \textcolor{red}{É só aplicar o item anterior e fazer umas continhas.}
\item $(\Rightarrow)$ Se $W$ é da forma $U_\mathbb{C}$ para algum subespaço $U$ de $V$, então para todo $(x,y)\in W$ temos $x\in U$ e $y\in U$, aí $x\in U$ e $-y\in U$, aí $(x,-y)\in W$; logo $W^*\subseteq W$.

\medskip
\noindent
$(\Leftarrow)$ Se $W^*\subseteq W$, então seja $(v_1,\dots,v_k)$ uma base de $W$, e para todo $i=1,\dots,k$ seja $v_i=(e_i,f_i)$ com $e_i,f_i\in V$, então temos $v_i^*=(e_i,-f_i)\in W$, assim:
\[
\begin{array}{rcl}
(e_i,0)&=&\frac{1}{2}(v_i+v_i^*)\in W\\
(f_i,0)&=&\frac{1}{2i}(v_i-v_i^*)\in W,
\end{array}
\]
assim, sendo $U$ o subespaço gerado por $e_1,\dots,e_n,f_1,\dots,f_n$, é fácil ver que $W=U_\mathbb{C}$.
\end{itemize}
\end{proof}

\begin{definicao}
Seja $V$ um espaço com produto interno sobre os reais. Então definimos:
\[
\langle(x,y),(x',y')\rangle=\left(\langle x,x'\rangle+\langle y,y'\rangle,\langle y,x'\rangle-\langle x,y'\rangle\right).
\]
\end{definicao}

\begin{proposicao}
Nas notações acima, verificam-se:
\begin{itemize}
\item $V_\mathbb{C}$ é um espaço com produto interno sobre os complexos.
\item Para $T\in\mathcal{L}(V)$ então temos $(T_\mathbb{C})^*=(T^*)_\mathbb{C}.$
\end{itemize}
\end{proposicao}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\begin{corolario}
$T\in\mathcal{L}(V)$ é normal (resp. unitário; autoadjunto) se e só se $T_\mathbb{C}$ é normal (resp. unitário; autoadjunto).
\end{corolario}
\begin{proof}
\textcolor{red}{É só fazer umas continhas.}
\end{proof}

\newpage

\section{Operadores Positivos}

\subsection{Operadores Autoadjuntos}

\begin{definicao}
Um operador $L\in\mathcal{L}(V)$ se chama \textbf{autoadjunto}, ou também \textbf{Hermitiano}, se e só se $T^*=T.$
\end{definicao}

\begin{teorema}\label{hermitiano}
Seja $V$ um espaço com produto interno sobre $\mathbb{R}$ de dimensão finita e seja $T\in\mathcal{L}(V)$ um operador autoadjunto. Então existe uma base ortonormal $B$ de $V$ tal que $[T]_B$ seja diagonal.
\end{teorema}
\begin{proof}
Seja $B$ uma base ortonormal de $V$ tal que:
\[
[T]_B=\begin{pmatrix}
A_1&&0\\&\ddots&\\0&&A_n
\end{pmatrix},
\]
em que $A_i$ esteja em uma das seguintes formas:
\[
\begin{array}{ccccl}
A_i&=&\begin{pmatrix}
a_i
\end{pmatrix}&,&\text{com }a_i\in\mathbb{R}.\\
A_i&=&\begin{pmatrix}
a_i&-b_i\\b_i&a_i
\end{pmatrix}&,&\text{com }a_i,b_i\in\mathbb{R}\text{ tais que }b_i\neq 0.
\end{array}
\]
Entretanto, temos:
\[
[T^*]_B=\begin{pmatrix}
A_1^*&&0\\&\ddots&\\0&&A_n^*
\end{pmatrix},
\]
assim para todo $i$ devemos ter $A^*_i=A_i,$ aí $A_i$ deve ser da forma $(a_i);$ portanto $[T]_B$ é diagonal.
\end{proof}

\subsection{Matrizes Autoadjuntas}

\begin{definicao}
Uma matriz $A$ se chama \textbf{autoadjunta}, ou também \textbf{Hermitiana}, se e só se $A^*=A.$
\end{definicao}

\begin{proposicao}
Uma matriz autoadjunta sempre tem um autovalor real.
\end{proposicao}
\begin{proof}
Seja $A$ uma matriz autoadjunta. Então $\det(xI-A)$ tem uma raiz $\lambda$, aí existe um $v\in\mathcal{C}^n$ não nulo tal que $Av=\lambda v$, mas $A$ é normal, aí:
\[
\overline{\lambda}v=A^*v=Av=\lambda v,
\]
aí $\overline{\lambda}=\lambda,$ aí $\lambda\in\mathbb{R}.$
\end{proof}

\noindent
Com isso podemos obter uma demonstração alternativa do Teorema \ref{hermitiano}.

\begin{proof}
Indução na dimensão de $V$. Se $\dim(V)=0$, então é fácil. Se $\dim(V)>0$, então existem $\lambda\in\mathbb{R}$ e $x\neq 0$ tais que $T(x)=\lambda x$, aí seja $W=\mathbb{R}x$, então $W$ é $T$-invariante, aí pela Proposição \ref{invariante} o subespaço $W^\perp$ é $T$-invariante, aí existe uma base ortonormal de autovetores $C$ de $W^\perp$, aí $B=\{x\}\cup C$ é uma base de autovetores de $V$.
\end{proof}

\subsection{Operadores Positivos}

\begin{definicao}
Seja $T$ um operador autoadjunto.
\begin{itemize}
\item $T$ é dito \textbf{positivo semidefinido} se e só se para todo $x\in V$ temos $\langle T(x),x\rangle\geq 0.$ Nesse caso, denotamos a propriedade por $T\geq 0.$
\item $T$ é dito \textbf{positivo definido} se e só se para todo $x\in V$ tal que $x\neq 0$ temos $\langle T(x),x\rangle>0.$ Nesse caso, denotamos a propriedade por $T\rhd 0.$
\end{itemize} 
\end{definicao}

\begin{teorema}
Seja $T\in\mathcal{L}(V)$ um operador autoadjunto e suponhamos que $\dim(V)<\infty$. Então:
\begin{itemize}
\item $T\geq 0$ se e só se para todo autovalor $\alpha$ temos $\alpha\geq 0.$
\item $T\rhd 0$ se e só se para todo autovalor $\alpha$ temos $\alpha> 0.$
\end{itemize} 
\end{teorema}

\begin{corolario}
Se $T\in\mathcal{L}(V)$ é um operador autoadjunto e $\dim(V)<\infty$, então $T\rhd0$ se e só se $T\geq 0$ e $T$ é inversível.
\end{corolario}

\newpage

\section{Matriz de Gram}

\begin{definicao}
Seja $V$ um espaço com produto interno. A \textbf{matriz de Gram} de uma sequência $(v_1,\dots,v_k)$ é a matriz:
\[
G(v_1,\dots,v_k)=\begin{pmatrix}
\langle v_1,v_1\rangle&\dots&\langle v_1,v_k\rangle\\\dots&\dots&\dots\\\langle v_k,v_1\rangle&\dots&\langle v_k,v_k\rangle
\end{pmatrix}.
\]
\end{definicao}

\begin{proposicao}
Seja $B=(e_1,\dots,e_n)$ uma base de $V$. Então $G(e_1,\dots,e_n)$ é inversível.
\end{proposicao}
\begin{proof}
Primeiro observemos que para quaisquer $u,v\in V$, com:
\[
[u]_B=(\alpha_1,\dots,\alpha_n),\quad [v]_B^*=(\beta_1,\dots,\beta_n)
\]
então:
\[
\langle u,v\rangle=[u]_BA[v]_B^*.
\]
De fato:
\[
\langle u,v\rangle=\sum_{i,j}\alpha_i\overline{\beta_j}\langle e_i,e_j\rangle,
\]
aí:
\[
\begin{array}{rcl}
[u]_BA[v]_B^*&=&(\alpha_1,\dots,\alpha_n)\begin{pmatrix}
\langle e_1,e_1\rangle&\dots&\langle e_1,e_n\rangle\\\dots&\dots&\dots\\\langle e_n,e_1\rangle&\dots&\langle e_n,e_n\rangle
\end{pmatrix}\begin{pmatrix}
\overline{\beta_1}\\\vdots\\\overline{\beta_n}
\end{pmatrix}\\
&=&\begin{pmatrix}
\alpha_1\langle e_1,e_1\rangle+\dots+\alpha_n\langle e_n,e_1\rangle\\\vdots\\\alpha_1\langle e_1,e_n\rangle+\dots+\alpha_n\langle e_n,e_n\rangle
\end{pmatrix}\begin{pmatrix}
\overline{\beta_1}\\\vdots\\\overline{\beta_n}
\end{pmatrix}
\end{array}.
\]
Se $A$ não for inversível, então existe $u\neq 0$ tal que $[u]_BA=0$. Neste caso, $\langle u,u\rangle=[u]_BA[u]_B^*=0$, aí $u=0$, contradição.
\end{proof}

\noindent
Se $B=(e_1,\dots,e_n)$ é uma base de $V$ e $A=G(e_1,\dots,e_n)$ então para todo $v\in V$ temos:
\[
[v]_BA[v]_B^*\geq 0
\]
e a igualdade ocorre se e somente se $v=0$.

\begin{definicao}
Uma matriz Hermitiana $A$ é dita:
\begin{itemize}
\item \textbf{Positiva semidefinitiva} se e só se para todo $x\in K^n$ temos $xAx^*\geq 0$.
\item \textbf{Positiva definitiva} se e só se para todo $x\in K^n$ tal que $x\neq 0$ temos $xAx^*>0$.
\end{itemize}
\end{definicao}

\begin{proposicao}
Seja $A\in\m{n}{K}$ uma matriz Hermitiana positiva definitiva. Seja $V$ um espaço vetorial com base $B=(e_1,\dots,e_n)$. Definamos, para $u,v\in V$, o seguinte:
\[
\langle u,v\rangle=[u]_BA[v]_B^*.
\]
Então $\langle u,v\rangle$ é um produto interno.
\end{proposicao}

\newpage

\section{Matriz de Gram}

\begin{definicao}
Seja $V$ um espaço sobre $K$ com produto interno. Sejam $v_1,\dots,v_m\in V$. A matriz:
\[
G(v_1,\dots,v_n)=\left(\langle v_i,v_j\rangle\right)\in\m{n}{K}
\]
chama-se a \textbf{matriz de Gram} do sistema $\{v_1,\dots,v_n\}$.
\end{definicao}

\noindent
A matriz $G(v_1,\dots,v_n)$ é Hermitiana, ou seja:
\[
G(v_1,\dots,v_n)^*=G(v_1,\dots,v_n),
\]
pois para todo $i,j$ temos:
\[
\overline{\langle v_i,v_j\rangle}=\langle v_j,v_i\rangle.
\]

\begin{proposicao}
Seja $\dim(V)=n$, $B=(e_1,\dots,e_n)$ uma base de $V$. Se $C=(f_1,\dots,f_n)$ é outra base de $V$, então:
\[
G(f_1,\dots,f_n)=P^t_{B,C}G(e_1,\dots,e_n)\overline{P_{B,C}}.
\]
\end{proposicao}
\begin{proof}
Observemos que para todo $x,y\in V$ temos:
\[
\langle x,y\rangle=[x]^t_BG_B\overline{[y]_B}=(x_1,\dots,x_n)\left(\langle e_i,e_j\rangle\right)\begin{pmatrix}
\overline{y_1}\\\vdots\\\overline{y_n},
\end{pmatrix}
\]
em que:
\[
x=\sum_i x_ie_i,\quad\quad y=\sum_i y_ie_i,
\]
pois:
\[
\langle x,y\rangle=\sum_{i,j}x_i\overline{y_j}\langle e_i,e_j\rangle.
\]
Agora temos:
\[
[x]_B=[x]_CP_{B,C}^t,
\]
aí sendo:
\[
P=(a_{ij}),
\]
temos:
\[
f_i=\sum_{j=1}^na_{ji}e_j,
\]
aí:
\[
x=x_1e_1+\dots+x_ne_n=\sum_{i=1}^nx'_if_i=\sum_{i,j}x'_ia_{ji}e_j=
\]
\end{proof}

\begin{corolario}
Se $B=(e_1,\dots,e_n)$ é uma base de $V$ então $\det G(e_1,\dots,e_n)>0$.
\end{corolario}
\begin{proof}
Seja $C$ uma base de $V$. Então:
\[
G_C=P^t_{B,C}G_B\overline{P_{B,C}},
\]
então, sendo $\det(P_{B,C})=\alpha$, temos:
\[
1=\det(G_C)=\det(P^t_{B,C})\det(G_B)\det(\overline{P_{B,C}})=\alpha\det(G_B)\overline{\alpha}=\abs{\alpha}^2\det(G_B).
\]
\end{proof}

\begin{corolario}
Seja $V$ um espaço com produto interno, $v_1,\dots,v_n\in V$ que são linearmente independentes. Então:
\[
\det G(v_1,\dots,v_n)>0
\]
\end{corolario}
\begin{proof}
Seja $V_0=\langle v_1,\dots,v_n\rangle$, então $V_0$ é um espaço com produto interno e $\dim(V_0)=n$ e $(v_1,\dots,v_n)$ é uma base de $V_0$. Assim, pelo corolário anterior, temos $G(v_1,\dots,v_n)>0$.
\end{proof}

\begin{proposicao}
Se $v_1,\dots,v_n$ são linearmente dependentes, então $G(v_1,\dots,v_n)$ é degenerada; em particular $\det G(v_1,\dots,v_n)=0$.
\end{proposicao}
\begin{proof}
Seja $B=\{e_1,\dots,e_n\}$ uma base e $C=\{f_1,\dots,f_n\}$ um conjunto arbitrário. Seja:
\[
P_{B,C}=\left([f_1]_B,\dots,[f_n]_B\right),
\]
a matriz de coordenadas de $\{f_1,\dots,f_n\}$ na base $B$. Então para $x$, sendo:
\[
x=x'_1f_1+\dots+x'_nf_n,
\]
então:
\[
[x]_B=\left(x'_1,\dots,x'_n\right)P^t_{B,C}.
\]
\end{proof}

\begin{proposicao}
Seja $U\subseteq V$ um subespaço, e seja $U=\langle u_1,\dots,u_m\rangle$ com $u_i$ linearmente independentes, e seja $v\in V$. Então:
\[
d(v,U)=\sqrt{\frac{\det G(u_1,\dots,u_m,v)}{\det G(u_1,\dots,u_m)}}.
\]
\end{proposicao}
\begin{proof}
Seja $v=u+w$ com $u\in U$ e $w\perp U$. Então para $i=1,\dots,m$ temos:
\[
\langle u_i,v\rangle=\langle u_i,u\rangle;
\]
assim temos:
\[
G(u_1,\dots,u_m,v)=\begin{pmatrix}
G(u_1,\dots,u_m)&\begin{matrix}\langle u_1,u\rangle\\\vdots\\\langle u_m,u\rangle\end{matrix}\\\begin{matrix}\langle u,u_1\rangle&\ldots&\langle u,u_m\rangle\end{matrix}&\langle u,u\rangle+\langle w,w\rangle
\end{pmatrix},
\]
assim:
\[
\det G(u_1,\dots,u_m,v)=\det G(u_1,\dots,u_m,u)+\det\begin{pmatrix}
G(u_1,\dots,u_m)&\begin{matrix}0\\\vdots\\0\end{matrix}\\\begin{matrix}\langle u,u_1\rangle&\ldots&\langle u,u_m\rangle\end{matrix}&\norm{w}^2
\end{pmatrix},
\]
aí:
\[
\det G(u_1,\dots,u_n,v)=\det G(u_1,\dots,u_n)\cdot\norm{w}^2.
\]
\end{proof}

\begin{proposicao}
Seja $V$ um espaço sobre $K$ com uma base $B=(v_1,\dots,v_n)$. Se $V$ possui um produto interno então $G(v_1,\dots,v_n)>0$. Reciprocamente, seja $A=(a_{ij})\in\m{n}{K}$ uma matriz positiva definida, aí definamos em $V$ o produto escalar $\langle v_i,v_j\rangle=a_{ij}$, então $V$ é um espaço com produto interno.
\end{proposicao}

\begin{teorema}
Seja $A=(a_{ij})\in\m{n}{K}$ uma matriz hermitiana. Então são equivalentes:
\begin{itemize}
\item $A$ é positiva definida.
\item $\mathrm{Spec}(A)=\{\lambda_1,\dots,\lambda_n\}$ com $\lambda_i>0$ para todo $i=1,\dots,n$
\item $A=B^2$ para alguma matriz $B$ tal que $B=B^*$ e $\det B\neq 0$. De modo mais geral $A=TT^*$ com $\det T\neq 0$.
\item Todos os menores principais de $A$ são positivos.
\end{itemize}
\end{teorema}
\begin{proof}
$(1)\Rightarrow(4)$ Se $A$ é positiva definida, então existe um produto interno tal que $G(e_1,\dots,e_n)=A$. Em particular, sendo:
\[
A_i=\begin{pmatrix}
a_{11}&&a_{1i}\\&\ddots&\\a_{i1}&&a_{ii}
\end{pmatrix},
\]
então $A_i=G(e_1,\dots,e_i)$, aí $\det(A_i)>0$.

\medskip
\noindent
$(4)\Rightarrow(1)$ Indução sobre $n$. Seja $\mathrm{Spec}(A)=\{\lambda_1,\dots,\lambda_n\}$ ($\lambda_i\in\mathbb{R}$ com repetição). Sabemos que $\det(A)=\lambda_1\dots>0$. Se $A$ não é positiva definida, então existem $i$ e $j$ tais que $\lambda_i\lambda_j<0$, aí sejam $u,v$ autovetores não nulos associados aos autovalores $\lambda_i,\lambda_j$. Escolhamos $\alpha,\beta\in K$ tais que $x=\alpha u+\beta v\in\langle e_1,\dots,e_{n-1}\rangle$, então:
\[
\begin{array}{rcl}
\langle Ax,x\rangle&=&\langle \alpha Au+\beta Av,\alpha u+\beta v\rangle\\
&=&\langle \lambda_i\alpha u+\lambda)j\beta v,\alpha u+\beta v\rangle\\
&=&\lambda_i\norm{\alpha u}^2+\lambda_j\norm{\beta v}^2+\lambda_i\alpha\overline{\beta}\langle u,v\rangle+\lambda_j\beta\overline{\alpha}\langle v,u\rangle\\
&=&\lambda_i\norm{\alpha u}^2+\lambda_j\norm{\beta v}^2,
\end{array}{rcl}
\]
assim:
\[
\lambda_i\norm{\alpha u}^2+\lambda_j\norm{\beta v}^2<0,
\]
contradizendo a hipótese de indução.
\end{proof}

\chapter{Formas Bilineares}

\section{Definições e Exemplos}

\begin{definicao}
Sejam $U,V,W$ espaços vetoriais sobre $K$. Uma \textbf{função bilinear} de $U$ e $V$ em $W$ é uma função $f:U\times V\rightarrow W$ que satisfaz o seguinte:
\begin{itemize}
\item $f(\alpha_1u_1+\alpha_2u_2,v)=\alpha_1f(u_1,v)+\alpha_2f(u_2,v)$.
\item $f(u,\beta_1v_1+\beta_2v_2)=\beta_1f(u,v_1)+\beta_2f(u,v_2)$.
\end{itemize}
Denotamos o conjunto das funções bilineares de $U$ e $V$ em $W$ por $B(U,V;W)$.
\end{definicao}

\begin{definicao}
Se $U=V=W$, cada $f\in B(U,V;W)$ determina em $U$ um produto:
\[
u\cdot v=f(u,v).
\]
Neste caso, $A=(U,f)$ chama-se uma \textbf{álgebra} sobre o corpo $K$.
\end{definicao}

\begin{exemplo}
Seja $A=(\mathbb{R}^3,\times)$, em que $\times$ é o produto vetorial usual, então $A$ é uma $\mathbb{R}$-álgebra.
\end{exemplo}

\begin{exemplo}
A estrutura dos complexos $\mathbb{C}$ é uma $\mathbb{R}$-álgebra de dimensão $2$.
\end{exemplo}

\begin{exemplo}
$K$ é uma $K$-álgebra de dimensão $1$.
\end{exemplo}

\begin{exemplo}
O conjunto das matrizes $\m{n}{K}$ é uma $K$-álgebra com:
\[
f(A,B)=AB.
\]
\end{exemplo}

\begin{exemplo}
Seja:
\[
H_n(\mathbb{R})=\{A\in\m{n}{K}\mid A^t=A\},
\]
e definamos:
\[
f(A,B)=AB+BA,
\]
então $H_n(\mathbb{R})$ é uma $\mathbb{R}$-álgebra, chamada \textbf{álgebra de Jordan}.
\end{exemplo}

\begin{definicao}
Uma \textbf{forma bilinear} entre dois espaços vetoriais $U$ e $V$ sobre $K$ é uma função bilinear de $U$ e $V$ em $K$. Denotamos o conjunto das formas bilineares entre $U$ e $V$ por $B(U,V)$.
\end{definicao}

\begin{exemplo}
Alguns exemplos são:
\begin{itemize}
\item Um produto interno sobre um espaço vetorial \textit{real}.
\item $(A,B)\mapsto\mathrm{tr}(AB)$.
\item Sejam $\varphi\in U^*$ e $\psi\in V^*$, então definamos $(\varphi\otimes\psi)(u,v)=\varphi(u)\psi(v)$, o \textbf{produto tensorial} de $\varphi$ e $\psi$.
\end{itemize}
\end{exemplo}

\section{Formas Não Degeneradas}

\begin{definicao}
Seja $(V,f)$ um espaço com uma forma bilinear $f$. Um subespaço $U\subseteq V$ chama-se \textbf{não-degenerado} se e só se a restrição $f\upharpoonright_U$ é não degenerada, e chama-se \textbf{isotrópico} se e só se $f\upharpoonright_U=0$.
\end{definicao}

\noindent
Por exemplo, num espaço simplético todo subespaço de dimensão $1$ é simplético.

\begin{proposicao}
Seja $(V,f)$ de dimensão finita.
\begin{itemize}
\item Se $U\subseteq V$ é não degenerado, então $V=U\oplus U^\perp$.
\item Se os dois $U$ e $U^\perp$ são não degenerados, então $U^{\perp\perp}=U$.
\end{itemize}
\end{proposicao}
\begin{proof}
\begin{itemize}
\item Seja $B=(u_1,\dots,u_m)$ uma base de $U$ e completemos a uma base $C=(v_1,\dots,v_m)$ de $V$. Consideremos $f'\in B(U,V)$ e $f''\in B(U,U)$ que são restrições de $f$ sobre $U\times V$ e $U\times U$. Sejam $A=[f']_{B,C}\in\m{m,n}{K}$ e $B=[f'']_{B,B}\in\m{n}{K}$. É claro que a matriz $B$ é formada pelas $m$ primeiras colunas da matriz $A$. Como $f\upharpoonright_U$ é não degenerada, então a matriz $B$ é não degenerada. Assim $\mathrm{posto}(B)=\mathrm{posto}(A)=m$. Agora consideremos:
\[
U^\perp=\{v\in V\mid[f']_{B,C}[v]^t_C=A[v]^t_C=0\},
\]
então:
\[
\begin{array}{rcl}
v\in U^\perp&\Leftrightarrow&\forall u\in U:f(u,v)=0\\
&\Leftrightarrow&\forall u\in U:[u]_B[f]_{B,C}[v]^t_C=0\\
&\Leftrightarrow&[f]_{B,C}[v]^t_C=0\\
&\Leftrightarrow&A[v]^t_C=0.
\end{array}
\]
Assim, como $A\in\m{m,n}{K}$, então $\dim(U^\perp)=n-m.$ Como $U$ é não degenerado, então $U\cap U^\perp=0$, portanto $V=U\oplus U^\perp.$
\item É claro que $U^{\perp\perp}.$ Se $U^\perp$ é não degenerado então $\dim(U^{\perp\perp})=n-\dim(U^\perp)=n-(n-m)=m=\dim(U),$ portanto $\dim(U^{\perp\perp})=U$.
\end{itemize}
\end{proof}

\begin{teorema}
Seja $(V,f)$ um espaço com uma forma bilinear $f$ simétrica ou antissimétrica. Então $V$ se decompõe numa soma ortogonal de subespaços:
\[
V=V_1\oplus\dots\oplus V_m,
\]
em que $\dim(V_i)=1$ no caso simétrico e $\dim(V_i)=1,2$ no caso antissimétrico, sendo $\dim(V_i)=1$ com $V_i$ isotrópico e $\dim(V_i)=2$ com $V_i$ não degenerado.
\end{teorema}
\begin{proof}
Indução sobre $\dim(V).$
\begin{itemize}
\item[1)] Caso simétrico. Se $f=0$, então $V=Ke_1\oplus\dots\oplus Ke_n$ para qualquer base $B=(e_1,\dots,e_n)$ é a soma desejada. Agora suponhamos que $f\neq 0$. Se $\forall v:f(v,v)=0$, então já vimos que para $u$ e $v$ temos $f(u,v)=-f(v,u)=f(v,u)$, aí $f(u,v)=0$, contradição. Assim existe $v$ tal que $f(v,v)\neq0.$ Agora tomemos $V_1=\langle v\rangle,$ então $V_1$ é não degenerado, portanto $V=V_1\oplus V_1^\perp$. Por indução, $V_1^\perp=V_2\oplus\dots\oplus V_m$, aí $V=V_1\oplus\dots\oplus V_m.$
\item[2)] Caso antissimétrico. Se $f=0$, então tudo é evidente. Se $f\neq 0$, então existem $u$ e $v$ tais que $f(u,v)=\alpha\neq0.$ Seja $V_1=\langle u,v\rangle,$ então $B=(u,v)$ é uma base de $V_1.$ Temos:
\[
[f\upharpoonright_{V_1}]_B=\begin{pmatrix}
0&\alpha\\-\alpha&0
\end{pmatrix},
\]
então $f\upharpoonright_{V_1}$ é não degenerada, aí $V_1$ é não degenerado e $V=V_1\oplus V_1^\perp.$
\end{itemize}
\end{proof}

\begin{corolario}
Seja $A\in\m{n}{K}$ uma matriz simétrica ou antissimétrica. Então existe $P\in\m{n}{K}$ não degenerada tal que:
\begin{itemize}
\item[1)] No caso simétrico:
\[
P^tAP=\begin{pmatrix}
\alpha_1&&0\\&\ddots&\\0&&\alpha_n
\end{pmatrix}
\]
em que $\alpha_i\in K.$
\item[2)] No caso antissimétrico:
\[
P^tAP=\begin{pmatrix}
A_1&&&&&\\&\ddots&&&&\\&&A_m&&&\\&&&0&&\\&&&&\ddots&\\&&&&&0
\end{pmatrix}
\]
em que:
\[
A_i=\begin{pmatrix}
0&1\\-1&0
\end{pmatrix}.
\]
\end{itemize}
\end{corolario}

\begin{corolario}[Teorema da Inversão]
Se $K=\mathbb{R}$ e $f\in B(V,\mathbb{R})$, então existe uma base $B$ de $V$ tal que:
\[
[f]_B=\begin{pmatrix}
I_r&0&0\\0&-I_s&0\\0&0&0_t
\end{pmatrix},
\]
em que $I_m$ é a matriz identidade de tamanho $m$ e $0_m$ é a matriz quadrada nula de tamanho $m.$ O trio $(r,s,t)$ se chama \textbf{assinatura} da forma $f$ e está unicamente determinada.
\end{corolario}
\begin{proof}
Temos $V=V_+\oplus V_-\oplus V_0$, em que $\dim(V_+)=r$ e $\dim(V_-)=s$ e $\dim(V_0)=t$, e também $r+s=\mathrm{posto}[f]_B$, assim $t=n-\mathrm{posto}[f]_B$ está determinado unicamente.

\medskip
\noindent
Agora seja $V=V'_+\oplus V'_-\oplus V'_0,$ e seja $\dim(V'_+)=r$ e $\dim(V'_-)=s'$. Basta provar que $r=r'.$ Suponhamos que $r>r'.$ Consideremos a projeção:
\[
\begin{array}{rcl}
\pi:V&\rightarrow&V'_+\\
v=v'_++v'_-+v'_0\mapsto v'_+
\end{array}
\]
Seja $\pi_+=\pi\upharpoonright_{V_+},$ então $\pi_+:V_+\rightarrow V'_+.$ Como $r>r'$ então $\dim(V_+)>\dim(V'_+),$ portanto existe $v\in V_+$ tal que $v\neq 0$ e $\pi_+(v)=0,$ aí $v\in V_+$ e $v\in V'_-\oplus V'_0$, pois $v'_+=0,$ aí:
\[
0<f(v,v)=f(v'_-+v'_0,v'_-+v'_0)=f(v'_-,v'_-)+f(v'_0,v'_0)\leq 0,
\]
chegando a contradição.
\end{proof}

\begin{observacao}
A forma bilinear de assinatura $(3,1,0)$ no espaço $\mathbb{R}^4$ determina a \textbf{geometria do espaço de Minkowski}, associada com a teoria da relatividade.
\end{observacao}

\section{Formas Quadráticas}

\begin{definicao}
Um polinômio homogêneo nas variáveis $x_1,\dots,x_n$ de grau $2$ com coeficientes no corpo $K$ chama-se uma \textbf{forma quadrática} nas variáveis $x_1,\dots,x_n,$ e sua aparência genérica é assim:
\[
q(x_1,\dots,x_n)=\sum_{i,j=1}^na_{ij}x_ix_j.
\]
A matriz $A=(a_{ij})$ chama-se matriz da forma quadrática $q.$
\end{definicao}

\noindent
Então a matriz $A^t=A$ e para $x\in K^n$ temos $a(x)=[x]A[x]^t.$

\medskip
\noindent
Associemos com $q$ a forma bilinear simétrica:
\[
f(x,y)=[x]A[y]^t,
\]
de modo que $q(x)=f(x,x).$

\medskip
\noindent
Se $\mathrm{Char}(K)\neq 2,$ então:
\[
f(x,y)=\frac{1}{2}\left(q(x+y)-q(x)-q(y)\right).
\]

\begin{definicao}
Uma aplicação $q:V\rightarrow K$ é chamada uma \textbf{forma quadrática} sobre $V$ se e só se existe uma forma bilinear $f$ simétrica tal que:
\[
\forall x\in V:q(x)=f(x,x).
\]
A forma $f$ chama-se a forma bilinear associada com a forma quadrática $q.$ A forma $q$ é dita \textbf{não degenerada} se e só se a $f$ é uma forma não degenerada.
\end{definicao}

\begin{teorema}
Temos o seguinte:
\begin{itemize}
\item Seja $V$ um espaço vetorial sobre $K$ de dimensão $n$ e $q:V\rightarrow K$ uma forma quadrática. Então exsite uma base $B$ de $V$ e $\lambda_1,\dots,\lambda_n\in K$ tais que:
\[
q(x)=\lambda_1x_1^2+\dots+\lambda_nx_n^2,
\]
em que $(x_1,\dots,x_n)=[x]_B.$
\item Se $K=\mathbb{R}$ então no caso anterior existe uma base $B$ de $V$ tal que nesta base:
\[
q(x)=x_1^2+\dots+x_r^2-x_{r+1}^2-\dots-x_{r+s}^2,
\]
em que os números $r$ e $s$ estão unicamente determinados.
\end{itemize}
\end{teorema}

\section{Produto Tensorial}

\begin{definicao}
Sejam $U$ e $V$ espaços vetoriais sobre $K.$ Um espaço $W$ junto com uma função bilinear $\iota$ de $U$ e $V$ a $W$ formam um \textbf{produto tensorial} de $U$ e $V$ se e só se para qualquer espaço $L$ e função bilinear $\varphi$ de $U$ e $V$ a $L$ existe uma única aplicação linear $\overline{\varphi}:W\rightarrow L$ tal que $\varphi=\overline{\varphi}\circ\iota.$
\end{definicao}

\begin{proposicao}
O par $(W,\iota)$ é únicamente determinado sob isomorfismo.
\end{proposicao}
\begin{proof}
Seja $(W',\iota')$ um outro produto tensorial. Então:
\begin{itemize}
\item Existe uma função linear $\varphi:W\rightarrow W'$ tal que $\iota'=\varphi\circ\iota.$
\item Existe uma função linear $\varphi':W'\rightarrow W$ tal que $\iota=\varphi'\circ\iota'.$
\end{itemize}
Assim:
\begin{itemize}
\item $(\varphi'\circ\varphi)\circ\iota=\iota,$ mas $I_W\circ\iota=\iota,$ aí $\varphi'\circ\varphi=I_W.$
\item $(\varphi\circ\varphi')\circ\iota'=\iota',$ mas $I_{W'}\circ\iota'=\iota',$ aí $\varphi\circ\varphi'=I_{W'}.$
\end{itemize}
\end{proof}

\begin{proposicao}
Para $U$ e $V$ existe um produto tensorial $(W,\iota).$
\end{proposicao}
\begin{proof}
Consideremos o espaço vetorial $X$ com base $U\times V.$ Seja $E$ a reunião do conjunto dos elementos de $X$ da forma:
\[
(a_1u_1+a_2u_2,v)-a_1(u_1,v)-a_2(u_2,v),\quad\quad a_1,a_2\in K,\quad u_1,u_2,v\in V
\]
com o conjunto dos elementos de $X$ da forma:
\[
(u,a_1v_1+a_2v_2)-a_1(u,v_1)-a_2(u,v_2),\quad\quad a_1,a_2\in K,\quad u,v_1,v_2\in V
\]
e seja $Y$ o subespaço gerado por $E$. Agora consideremos $W=X/Y$ juntamente com a projeção canônica $\iota:X\rightarrow W.$ Então podemos mostrar que $(W,\iota)$ é um produto tensorial \textcolor{red}{(Completar detalhes)}.
\end{proof}

\begin{definicao}
Denotamos o par $(W,\iota)$ definido na demonstração por $(U\otimes V,\otimes).$
\end{definicao}

\begin{proposicao}
Sejam $U$ e $V$ espaços. Então a imagem de $\langle\mathrm{Im}(\otimes)\rangle=U\otimes V$.
\end{proposicao}
\begin{proof}
Seja $L=(U\otimes V)/\langle\mathrm{Im}(\otimes)\rangle,$ e consideremos a projeção canônica $\pi:U\times V\rightarrow L.$ Então $\pi\circ\otimes=0$ e $0\circ\otimes=0$, assim $\pi=0$, aí $\langle\mathrm{Im}(\otimes)\rangle=U\otimes V.$
\end{proof}

\begin{teorema}
Sejam $U$ e $V$ espaços, $(e_i)_{i\in I}$ uma base de $U$ e $(f_j)_{j\in J}$ uma base de $V.$ Então a família $(e_i\otimes f_j)_{(i,j)\in I\times J}$ é uma base de $U\otimes V.$
\end{teorema}

\begin{proposicao}
Sejam $U$ e $V$ espaços vetoriais sobre $K.$ Seja $w=\sum_{i=1}^n u_i\otimes v_i\in U\otimes V$, em que $(v_1,\dots,v_n)$ é linearmente independente. Então os elementos $(u_1,\dots,u_n)$ estão determinados unicamente, isto é, se $w=\sum_{i=1}^nu'_i\otimes v_i,$ então $u'_i=u_i$ para todo $i=1,\dots,n.$ O mesmo é válido quando o $(u_1,\dots,u_n)$ é linearmente independente.
\end{proposicao}
\begin{proof}
Suponhamos que $\sum_{i=1}^nu_i\otimes v_i=\sum_{i=1}^nu'_i\otimes v_i,$ então:
\[
0=\sum_{i=1}^n\left(u_i\otimes v_i-u'_i\otimes v_i\right)=\sum_{i=1}^n(u_i-u'_i)\otimes v_i.
\]
Assim basta provar que, se:
\[
\sum_{i=1}^nu_i\otimes v_i=0
\]
então:
\[
u_1=\dots=u_n=0.
\]
\end{proof}

\section{Extensão Escalar}

\begin{definicao}
Para corpo $F$ e subcorpo $K$ e espaço $V$ sobre $K$, então uma \textbf{extensão escalar} de $V$ por $F$ é um espaço vetorial sobre $F$ munido de uma função $K$-linear $i:V\rightarrow W$ tal que para todo espaço $U$ sobre $F$ e para todo $\varphi\in\mathcal{L}_K(V,U)$ exista um único $\overline{\varphi}\in\mathcal{L}_F(W,U)$ tal que $\overline{\varphi}\circ i=\varphi.$
\end{definicao}

\begin{proposicao}
Para corpo $F$ e subcorpo $K$ e espaço $V$ sobre $K$, então uma extensão escalar de $V$ por $F$ é determinado unicamente sob isomorfismo.
\end{proposicao}

\begin{proposicao}
Se $V$ é um espaço vetorial sobre $\mathbb{R}$, então a complexificação $V_\mathbb{C}$ munida da função $i:V\rightarrow V_\mathbb{C}$ dada por $\forall x\in V:i(x)=(x,0)$ é uma extensão escalar de $V$ por $\mathbb{C}.$
\end{proposicao}

\begin{proposicao}
Para corpo $F$ e subcorpo $K$ e espaço $V$ sobre $K$, então $F\otimes V$ munido da função $i:V\rightarrow F\otimes V$ dada por $\forall x\in V:i(x)=1\otimes x$ é uma extensão escalar de $V$ por $F.$
\end{proposicao}

\begin{definicao}
Para corpo $F$ e subcorpo $K$ e espaço $V$ sobre $K$, definimos $V_F=F\otimes V$ e definimos $\iota_F:V\rightarrow F\otimes V$ por $\forall x\in V:\iota_F(x)=1\otimes x.$
\end{definicao}

\begin{proposicao}
Propriedades da extensão escalar.
\begin{itemize}
\item $\dim_F(V_F)=\dim_K(V).$
\item Para qualquer base $(e_i)_{i\in I}$ de $V$ sobre $K$, então $(\iota(e_i))_{i\in I}$ é uma base de $V_F$ sobre $F.$
\end{itemize}
\end{proposicao}

\begin{proposicao}[\textbf{Isomorfismos Canônicos}]
Temos os seguintes isomorfismos canônicos:
\begin{itemize}
\item Se $V$ é um espaço sobre $K$ então:
\[
\begin{array}{rcl}
K\otimes V&\cong&V\\
\alpha\otimes v&\mapsto&v.
\end{array}
\]
\item Se $U$ e $V$ são espaços sobre $K$, então:
\[
\begin{array}{rcl}
U\otimes V&\cong&V\otimes U\\
u\otimes v&\mapsto&v\otimes u.
\end{array}
\]
\item Se $U$ e $V$ e $W$ são espaços sobre $K$ então:
\[
\begin{array}{rcl}
(U\otimes V)\otimes W&\cong&U\otimes(V\otimes W)\\
(u\otimes v)\otimes w&\mapsto&u\otimes(v\otimes w).
\end{array}
\]
\end{itemize}
\end{proposicao}

\chapter{Mecânica Quântica}

\section{Postulados}

Apresentaremos alguns postulados.

\begin{postulado}
O estado de um sistema quântico num espaço $X$ (em geral $X=\mathbb{R}^n$ ou afins) está completamente determinado por uma função de onda $\psi:X\rightarrow\mathbb{C}$ tal que:
\[
\int_{X}\abs{\psi(x)}^2\dif x=1.
\]
Além disso, interpretamos $\abs{\psi(x)}^2$ como a distribuição de probabilidade de encontrarmos a partícula na posição $x.$
\end{postulado}

\begin{postulado}
Toda grandeza física $\alpha$ de um estado quântico está associado a um operador linear autoadjunto $\hat{\alpha}$ em $L^2.$ Para algumas grandezas podemos mostrar mais especificamente seus operadores correspondentes.
\begin{itemize}
\item Cada coordenada cartesiana de posição $x_i$ em $\mathbb{R}^n$ corresponde ao operador $\hat{x}_i$ dado pela multiplicação por $x_i$, ou seja:
\[
(\hat{x}_if)(x_1,\dots,x_n)=x_if(x_1,\dots,x_n).
\]
\item Cada coordenada cartesiana de momento linear $p_i$ corresponde ao operador $\hat{p}_i$ dado por:
\[
\hat{p}_i(f)=\frac{\hslash}{i}\pd{f}{x_i},
\]
em que $\hslash$ é a \textbf{constante reduzida de Planck}:
\[
\hslash=\frac{h}{2\pi},\quad\quad\quad h\cong 6,62607015\cdot10^{-34}J\cdot s,
\]
e $h$ é a \textbf{constante de Planck}.
\item Para a energia cinética $K_i$ na coordenada $i$, temos as seguintes contas:
\[
K_i=\frac{mv_i^2}{2}=\frac{m^2v_i^2}{2m}=\frac{p_i^2}{2m},
\]
pois o momento linear na coordenada $i$ é dado por:
\[
p_i=mv_i.
\]
Por enquanto desconsideramos efeitos relativísticos, assim a massa $m$ é uma constante, de modo que podemos associar $K_i$ ao operador:
\[
\hat{K}_i=\frac{\hat{p}_i^2}{2m}=-\frac{\hslash^2}{2m}\pd{}{x_i}.
\]
\item Para a energia cinética $K$, lembrando que:
\[
K=K_1+\dots+K_n,
\]
então associamos $K$ ao operador
\[
\hat{K}=\hat{K}_1+\dots+\hat{K}_n=-\frac{\hslash^2}{2m}\left(\pd[2]{}{x_1}+\dots+\pd[2]{}{x_n}\right)=-\frac{\hslash^2}{2m}\nabla^2.
\]
\item Para a energia potencial $V$, consideramos o operador $\hat{V}$ dado pela multiplicação por $V$, ou seja:
\[
(\hat{V}f)(x_1,\dots,x_n)=V(x_1,\dots,x_n)f(x_1,\dots,x_n).
\]
\item A energia total $E$ é dada por:
\[
E=K+V,
\]
assim associamos $E$ ao \textbf{operador Hamiltoniano}:
\[
\hat{H}=\hat{K}+\hat{V}=-\frac{\hslash^2}{2m}\nabla^2+V.
\]
\item Em $\mathbb{R}^3$, o momento angular $\bm{L}=(L_x,L_y,L_z)$ é dado pelo seguinte:
\[
\begin{array}{rcl}
L_x&=&yp_z-zp_y\\
L_y&=&zp_x-xp_z\\
L_z&=&xp_y-yp_x\\
\end{array}
\]
assim associamos os componentes de $\bm{L}$ aos seguintes operadores:
\[
\begin{array}{rclcl}
\hat{L}_x&=&\hat{y}\hat{p}_z-\hat{z}\hat{p}_y=\frac{\hslash}{i}\left(y\pd{}{z}-z\pd{}{y}\right)\\
\hat{L}_y&=&\hat{z}\hat{p}_x-\hat{x}\hat{p}_z=\frac{\hslash}{i}\left(z\pd{}{x}-x\pd{}{z}\right)\\
\hat{L}_z&=&\hat{x}\hat{p}_y-\hat{y}\hat{p}_x=\frac{\hslash}{i}\left(x\pd{}{y}-y\pd{}{x}\right)\\
\end{array}
\]
\item Também associamos o quadrado da magnitude de $\bm{L}$, que é:
\[
L^2=L_x^2+L_y^2+L_z^2
\]
ao seguinte operador:
\[
\hat{L}^2=\hat{L}_x^2+\hat{L}_y^2+\hat{L}_z^2.
\]
\end{itemize}
\end{postulado}

\begin{postulado}
Em qualquer medição de uma grandeza física $\alpha$ num sistema quântico, os únicos possíveis valores que serão observados são os autovalores $(\alpha_i)_{i\in I}$ do operador correspondente $\hat{\alpha}$, ou seja, que satisfazem a seguinte equação:
\[
\hat{\alpha}\psi_i=\alpha_i\psi_i.
\]
Além disso, se $\psi_i$ é um autovetor de $\hat{\alpha}$ com autovalor $\alpha_i$, então uma medição de $\alpha$ sobre $\psi_i$ sempre retornará o valor $\alpha_i$.
\end{postulado}

\begin{postulado}
Se um sistema quântico é descrito por uma função de onda $\psi$, então o valor esperado de uma grandeza física $\alpha$ é dado por:
\[
\langle\alpha\rangle=\int_{\mathbb{R}^n}\psi^*\hat{\alpha}\psi\dif x.
\]
\end{postulado}

\begin{postulado}
Um sistema quântico temporal:
\[
\begin{array}{rcl}
\Psi:\mathbb{R}^n\times\mathbb{R}&\rightarrow&\mathbb{C}\\
(x,t)&\mapsto&\Psi(x,t)
\end{array}
\]
evolui no tempo de acordo com a \textbf{equação de Schrödinger temporal}:
\[
\hat{H}\Psi(x,t)=-\frac{\hslash}{i}\pd{\Psi(x,t)}{t},
\]
em que $\hat{H}$ é o operador Hamiltoniano:
\[
\hat{H}=-\frac{\hslash^2}{2m}\nabla^2+V(x,t).
\]
\end{postulado}

\section{Princípio da Incerteza}

Postulamos que, se uma função de onda $\psi$ é um autovetor de um operador $\hat{\alpha}$ com autovalor $a$, então uma medição da grandeza física $\alpha$ sempre retornará o valor $a$. Assim, se $\psi$ é simultaneamente um autovetor de dois operadores $\hat{\alpha}$ e $\hat{\beta}$, ou seja, se $\hat{\alpha}(\psi)=a\psi$ e $\hat{\beta}\psi=b\psi$, então podemos associar simultaneamente as grandezas físicas $\alpha$ e $\beta$ aos valores definitivos $a$ e $b$. Portanto, seria interessante saber quando podemos encontrar um ``conjunto completo''\footnote{Se o espaço tem dimensão finita, podemos simplesmente definir um conjunto completo como uma base. Em alguns outros casos, podemos definir um conjunto completo como uma base de Schauder.} $(e_i)_{i\in I}$ de autovetores simultâneos de $\hat{\alpha}$ e $\hat{\beta}$. Sabemos que, num espaço dimensional de dimensão finita, uma família de operadores é simultaneamente diagonalizável se e somente se eles são diagonalizáveis e se comutam um com outro.

\printindex

\end{document}
